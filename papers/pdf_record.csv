pdf_index,ref,title,introduction
pdf0.pdf,https://lazarinastoy.com/topic-modelling-limitations-short-text/,#L5.1 Limitations of Topic Mining,No Preview Available
pdf1.pdf,https://www.cs.uic.edu/~liub/publications/WWW-2003.pdf,#L5.1 Paper about Mining topic specific concepts and definitions on th...,No Preview Available
pdf2.pdf,https://statweb.stanford.edu/~jtaylo/courses/stats306b/restricted/notebooks/EM_algorithm.pdf,An explanation of EM algorithm with exercises#L5.2,No Preview Available
pdf3.pdf,http://www.stat.columbia.edu/~bodhi/Talks/2ComponentMixModel.pdf,two component mixture models - explained,No Preview Available
pdf4.pdf,https://cyfs.unl.edu/cyfsprojects/videoPPT/84329ab252efffb78f4daf28ecbf6274/110121-Kupzyk.pdf,mixture models overview- explained better,No Preview Available
pdf5.pdf,https://monkeylearn.com/blog/introduction-to-topic-modeling/,An explanation of topic modeling #L5.1,No Preview Available
pdf6.pdf,https://www.youtube.com/watch?v=hN7kzmsSRAQ,#L5.2: EM algorithm for missing data,No Preview Available
pdf7.pdf,https://arxiv.org/abs/2203.10256,An interesting paper about dependency mixture LM #L5.1,"In this paper, we introduce the Dependency-based Mixture Language Models"
pdf8.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,Expectation Maximization Algorithm #L5.2,No Preview Available
pdf9.pdf,https://www.sciencedirect.com/science/article/pii/S0020025518302366,A Generative Model for Category Text Generation #L5.1,No Preview Available
pdf10.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,Wiki of EM algorithm #L5.2,No Preview Available
pdf11.pdf,https://text-machine-lab.github.io/blog/2020/generative-models/,Expansion on types of generative models #L5.1,No Preview Available
pdf12.pdf,https://link.springer.com/content/pdf/10.1023/A:1007692713085.pdf,#L5.2 Original paper of mixture of unigram model,Text Classification from Labeled and Unlabeled Documents using EM
pdf13.pdf,https://probmods.org/chapters/mixture-models.html,This explains about the mixture models described in lecture #L5.2,Mixture models
pdf14.pdf,https://www.analyticsvidhya.com/blog/2021/06/part-17-step-by-step-guide-to-master-nlp-topic-modelling-using-plsa/,#6.2 Part 17: Step by Step Guide to Master NLP – Topic Modelling using...,Step by Step Guide to Master NLP – Topic Modelling using pLSA
pdf15.pdf,http://www.hcbravo.org/dscert-algo/homeworks/topic_modeling.html,#6.1 Probabilistic Latent Semantic Analysis with the EM Algorithm,Probabilistic Latent Semantic Analysis with the EM Algorithm
pdf16.pdf,https://www.tidytextmining.com/topicmodeling.html,This describes about topic mining mentioned in the lecture #L5.1,Topic modeling
pdf17.pdf,https://amit02093.medium.com/understanding-generative-modelling-lda-and-bayesian-networks-8b3bc09d06a6,Some intro to generative modelling #L5.1,No Preview Available
pdf18.pdf,https://towardsdatascience.com/a-guide-to-collaborative-topic-modeling-recommender-systems-49fd576cc871,Just an extension to #L5.2,No Preview Available
pdf19.pdf,https://www.researchgate.net/figure/The-mixture-of-unigrams-model-shown-as-a-directed-graphical-model-using-plate-notation_fig3_242385158,#L5.2: Mixture of Unigram Models implemented as a graphical model,No Preview Available
pdf20.pdf,https://arxiv.org/pdf/1204.5488.pdf,a statistical method for estimating the mixing proportion and unknown...,Estimatiom of a two-component mixture model withapplications to multiple testing
pdf21.pdf,https://www.analyticsvidhya.com/blog/2021/06/part-17-step-by-step-guide-to-master-nlp-topic-modelling-using-plsa/,A tutorial of PLSA with advantages/disadvantages listed #5.2,No Preview Available
pdf22.pdf,https://github.com/yedivanseven/PLSA,Python library of implementation of PLSA #5.2,No Preview Available
pdf23.pdf,https://en.wikipedia.org/wiki/Generative_model,An introduction of Generative modebl By wikipedia #L5.1,Generative model
pdf24.pdf,https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05,Goes into detail about different ways of topic modeling #L5.1,No Preview Available
pdf25.pdf,https://monkeylearn.com/topic-analysis/,Application of Topic Mining: Topic Analysis #5.1,No Preview Available
pdf26.pdf,https://arxiv.org/pdf/1212.3900.pdf,Probabilistic Latent Semantic Analysis with the EM Algorithm,Probabilistic Latent Semantic Analysis with the EM Algorithm
pdf27.pdf,https://www.geeksforgeeks.org/ml-expectation-maximization-algorithm/,ML | Expectation-Maximization Algorithm,ML | Expectation-Maximization Algorithm
pdf28.pdf,https://www.dcs.bbk.ac.uk/~dell/teaching/cc/book/ditp/ditp_ch7.pdf,"EM Algorithms for Text
Processing",No Preview Available
pdf29.pdf,https://www.cs.cmu.edu/~nasmith/LS2/gimpel.06.pdf,#L5.1 A review of techniques in topic mining,Modeling Topics
pdf30.pdf,https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch20.pdf,#L5.2: A chapter from stat @ CMU for introduing the Mixture Model.,No Preview Available
pdf31.pdf,https://www.statcan.gc.ca/en/data-science/network/topic-modelling,#L5.1: A technical review of Topic Modeling.,Topic Modelling and Dynamic Topic Modelling : A technical review
pdf32.pdf,https://en.wikipedia.org/wiki/Mixture_model,"#L5.2: The Wikipedia page of the concept ""Mixture Model"" coverd in L5....",Mixture model
pdf33.pdf,https://en.wikipedia.org/wiki/Topic_model,"#L5.1: The Wikipedia page of the concept of ""topic model"" coverd in L5...",Topic model
pdf34.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,A great example to explain hidden Markov model in Lecture #L7.1,"In its discrete form, a hidden Markov process can be visualized as a generalization of the urn problem with replacement (where each item from the urn is returned to the original urn before the next..."
pdf35.pdf,http://www.stat.columbia.edu/~bodhi/Talks/2ComponentMixModel.pdf,These slides from Columbia give great mathematical insights to how the...,No Preview Available
pdf36.pdf,https://arxiv.org/pdf/2203.10256.pdf,This research paper gives a brief intro to Mixture Language Models des...,No Preview Available
pdf37.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,Introduction to Mixture Models #L5.2,"We often make simplifying modeling assumptions when analyzing a data set such as assuming each observation comes from one specific distribution (say, a Gaussian distribution). Then we proceed to es..."
pdf38.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,Introduction to Mixture Models,"We often make simplifying modeling assumptions when analyzing a data set such as assuming each observation comes from one specific distribution (say, a Gaussian distribution). Then we proceed to es..."
pdf39.pdf,https://www.ibm.com/topics/text-mining,Learn about text mining #L5.1,What is text mining?
pdf40.pdf,https://dl.acm.org/doi/abs/10.1145/2505515.2505578?casa_token=M1Dj1QeOpGYAAAAA:iZBTqESEpEOu9e-ZSvnY-rL7lQdnlK9h3K6D7yJTUyW_AkUkab6_StYcYsUPFj5LW1LA9PNg6DJiFA,#L5.2,No Preview Available
pdf41.pdf,https://people.eecs.berkeley.edu/~pabbeel/cs287-fa13/slides/Likelihood_EM_HMM_Kalman.pdf,#L5.1,EM Slides
pdf42.pdf,https://www.cs.toronto.edu/~rgrosse/csc321/mixture_models.pdf,#L5.2,No Preview Available
pdf43.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,#L5.1,No Preview Available
pdf44.pdf,http://www.hcbravo.org/dscert-algo/homeworks/topic_modeling.html,#L5.1,No Preview Available
pdf45.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A good article explaining EM algorithm and why is it used by comparing...,A Gentle Introduction to Expectation-Maximization (EM Algorithm)
pdf46.pdf,https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/topicbased-mixture-language-modelling/A26AD26F5DBE74575EB41483A9EABF5F,A good research paper describing an approach for constructing a mixtur...,Topic-based mixture language modelling
pdf47.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,A general overview of Topic Modeling and the various methods used for ...,Text Mining 101: Topic Modeling
pdf48.pdf,https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/plsa.pdf,An application of PLSA #L6.3,Hypertext Induced Topic Search (HITS)
pdf49.pdf,https://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf,A nice description of how the expectation maximization (EM) algorithm ...,No Preview Available
pdf50.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L5.2 EM,No Preview Available
pdf51.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L5.2 EM,No Preview Available
pdf52.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L5.2 EM,No Preview Available
pdf53.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L5.2 EM,No Preview Available
pdf54.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L5.2 EM,No Preview Available
pdf55.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L5.2 EM,No Preview Available
pdf56.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L5.2 EM,No Preview Available
pdf57.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,This explains about Topic Mining #L5.1,Topic Modeling
pdf58.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,This explains about EM algorithm #L5.2,Gentle Introduction to Expectation-Maximization
pdf59.pdf,https://monkeylearn.com/topic-analysis/,#L5.1 topic mining,No Preview Available
pdf60.pdf,https://text-machine-lab.github.io/blog/2020/generative-models/,#6.1 some general models,No Preview Available
pdf61.pdf,https://sambanova.ai/blog/Document-Classification-Demo/,#L5.2,No Preview Available
pdf62.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,#L5.1,No Preview Available
pdf63.pdf,https://d1wqtxts1xzle7.cloudfront.net/30739211/plsa-note-libre.pdf?1391833371=,#5.2 EM algorithm for PLSA - A note,No Preview Available
pdf64.pdf,https://cseweb.ucsd.edu/~elkan/250B/topicmodels.pdf,#L5.1 A paper on topic mining,No Preview Available
pdf65.pdf,https://dl.acm.org/doi/pdf/10.1145/1150402.1150482,#5.1 Mixture Models for topic mining,No Preview Available
pdf66.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,Wikipedia page of PLSA for Lecture #6.3,"Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis o..."
pdf67.pdf,https://stats.stackexchange.com/questions/83387/why-is-the-expectation-maximization-algorithm-guaranteed-to-converge-to-a-local/183410,"A discussion on the convergence guarantee of EM algorithm, related to ...","First of all, it is possible that EM converges to a local min, a local max, or a saddle point of the likelihood function. More precisely, as Tom Minka pointed out, EM is guaranteed to converge to a..."
pdf68.pdf,http://www.vldb.org/pvldb/vol8/p305-ElKishky.pdf,#L5.1,No Preview Available
pdf69.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,Wikipedia page for EM algorithm. Lecture #L6.1,No Preview Available
pdf70.pdf,https://www.geeksforgeeks.org/ml-expectation-maximization-algorithm/,#L5.2 - EM algorithm explanation,No Preview Available
pdf71.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,#L5.2 A blog explaining expectation maximization,No Preview Available
pdf72.pdf,https://blog.marketmuse.com/glossary/topic-modeling-definition/,#L5.1 Topic modelling,No Preview Available
pdf73.pdf,https://www.nature.com/articles/nbt1406,#L5.2 Expectation Maximisation algorithm,No Preview Available
pdf74.pdf,http://www-personal.umich.edu/~qmei/pub/kdd06-mix.pdf,A paper advised by Prof. Zhai. Proposed and analyzed the CPLSA model a...,#5.1 A Mixture Model for Contextual Text Mining
pdf75.pdf,https://sarit-maitra.medium.com/nlp-ner-topic-modeling-for-news-content-analysis-9cee7ec35e69,#L5.1,Unstructured Text Data Mining & Topic Modeling
pdf76.pdf,https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf,#L5.1 A paper explaining topic mining,No Preview Available
pdf77.pdf,https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd,Requirement for Mixture Models and Intuition behind using E-M for opti...,Gaussian Mixture Models and Expectation-Maximization (A full explanation)
pdf78.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,#6.3 plsa defination and concepts,No Preview Available
pdf79.pdf,https://www.cambridge.org/core/journals/political-analysis/article/abs/classification-by-opinionchanging-behavior-a-mixture-model-approach/CC64B1BD468B1B1D5790AAF1C9741456,#6.2 a paper about mixture model,No Preview Available
pdf80.pdf,https://monkeylearn.com/topic-analysis/,Intro to topic analysis with explanations and examples.,No Preview Available
pdf81.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,EM Algorithm in #L5.2,No Preview Available
pdf82.pdf,https://text-machine-lab.github.io/blog/2020/generative-models/,Generative Models in #L5.1,No Preview Available
pdf83.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,This gives a nice description of the EM model and how it works on a hi...,"The Expectation-Maximization Algorithm, or EM algorithm for short, is an approach for maximum likelihood estimation in the presence of latent variables.

A general technique for finding maximum l..."
pdf84.pdf,https://en.wikipedia.org/wiki/Mixture_model,This gives an example of the mixture model being implemented #L5.1,"Assume that a document is composed of N different words from a total vocabulary of size V, where each word corresponds to one of K possible topics. The distribution of such words could be modelled ..."
pdf85.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,"#L5.2
General setting for EM","Let’s move onto the general setting. Here is the setup:

We have some data X of whatever form.
We posit there is also unobserved (latent) data Δ, again of whatever form.
We have a model with pa..."
pdf86.pdf,https://text-machine-lab.github.io/blog/2020/generative-models/,"#L5.1
More information on discrete distributions and language modelin...","One classic example of generating discrete distributions is the task of language modeling. Specifically, the goal is to generate a natural language sequence of words which is realistic; it sounds l..."
pdf87.pdf,https://www.cs.toronto.edu/~rgrosse/csc321/mixture_models.pdf,A detailed set of notes on mixture models #L5.2,Mixture models
pdf88.pdf,https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf,A nice survey of topic modeling techniques #L5.1,A Survey of Topic Modeling
pdf89.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,#L5.1 - Topic modelling,No Preview Available
pdf90.pdf,https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43902.pdf,"A paper on n-gram mixture model, which use EM to separate between clus...",No Preview Available
pdf91.pdf,https://ieeexplore.ieee.org/document/6707701,N-gram mixture #5.2,Mixture of mixture n-gram language models
pdf92.pdf,https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43902.pdf,A paper introduces a mixture of n-gram language models. And use EM alg...,No Preview Available
pdf93.pdf,https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/lec15_16_handout.pdf,Explanation to EM steps and Gaussian Mixture Model,Slides for Gaussian Mixture Model and EM steps#L5.2
pdf94.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,Introduction to mixture models mentioned in lecture 5.2,No Preview Available
pdf95.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,Introduction to topic mining mentioned in lecture 5.1,No Preview Available
pdf96.pdf,https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95,#L5.2 This article provides a short introduction to Gaussian Mixture M...,"In the world of Machine Learning, we can distinguish two main areas: Supervised and unsupervised learning. The main difference between both lies in the nature of the data as well as the approaches ..."
pdf97.pdf,http://hanj.cs.illinois.edu/pdf/kdd20_ymeng.pdf,topic mining with category tree in spherical space,A paper about Hierarchical Topic Mining#L5.1
pdf98.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,#L5.2 This article provides an easy-to-understand introduction to the ...,"We often make simplifying modeling assumptions when analyzing a data set such as assuming each observation comes from one specific distribution (say, a Gaussian distribution). Then we proceed to es..."
pdf99.pdf,https://www.scientific.net/AMM.444-445.1713,#L5.1 This paper provides a survey of the current mainstream methods i...,"Text mining is the task of automatic discovery of new, previously unknown information from unstructured document collections. Vector space or bag of words representation is one of the mainstream de..."
pdf100.pdf,https://ieeexplore.ieee.org/document/6707701,A useful explanation of n-gram mixture covered in Lecture #L5.2,Mixture n-gram language models
pdf101.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,#L5.1 Definition of mixture model and two examples.,No Preview Available
pdf102.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,A useful explanation of topic mining covered in Lecture #L5.1,Text Mining 101: Topic Modeling
pdf103.pdf,https://brilliant.org/wiki/gaussian-mixture-model/,#L5.2 A detailed explanation of expectation maximization being used fo...,EM for Gaussian Mixture Models
pdf104.pdf,https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/topicbased-mixture-language-modelling/A26AD26F5DBE74575EB41483A9EABF5F,mixture language modeling #5.1,Topic-based mixture language modelling
pdf105.pdf,https://arxiv.org/pdf/2205.01845.pdf,A recent paper that discusses seed-guided topic mining using both disc...,"In this paper, we generalize the task of seed-guided topic discovery to allow out-of-vocabulary seeds. We propose a novel framework, named SEETOPIC, wherein the general knowledge of PLMs and the lo..."
pdf106.pdf,https://www.sciencedirect.com/science/article/abs/pii/S0925231222008748,This paper introduced a Neural Network to learn the distribution of th...,No Preview Available
pdf107.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,PLSA model in lecture #L5.2,"Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis o..."
pdf108.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,EM algorithm in lecture #L5.1,The EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables...
pdf109.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,Wiki on PLSA #L6.3,No Preview Available
pdf110.pdf,https://ieeexplore.ieee.org/document/6707701,Paper on mixture n-gram language models covered in #L5.2,Mixture n-gram language models
pdf111.pdf,https://monkeylearn.com/topic-analysis/,This page explains about topic analysis covered in #L5.1,Topic analysis uses natural language processing (NLP) to break down human language so that you can find patterns and unlock semantic structures within texts to extract insights and help make data-d...
pdf112.pdf,https://www1.icsi.berkeley.edu/ftp/global/global/pub/speech/papers/euro99-emlm.pdf,Another paper about topic-based language models using EM method #L5.2,"Topics are modeled in a latent variable framework in which we also derive an
EM algorithm to perform a topic factor decomposition based on
a segmented training corpus"
pdf113.pdf,https://books.google.com/books?hl=zh-CN,#6.1#,No Preview Available
pdf114.pdf,https://web.ics.purdue.edu/~jltobias/BayesClass/lecture_notes/mixtures_student.pdf,Mixture Models #L5.2,Mixture Models
pdf115.pdf,https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch20.pdf,Mixture Models #L5.2,Mixture Models
pdf116.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,Mixture Models #L5.2,Mixture Models
pdf117.pdf,https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf,Topic Mining  #L5.1,Topic Mining
pdf118.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,Topic Mining #L5.1,Topic Mining
pdf119.pdf,https://en.wikipedia.org/wiki/Mixture_model,Mixture model #L5.1,Mixture model
pdf120.pdf,http://www.iro.umontreal.ca/~nie/IFT6255/zhai-cimk-01.pdf,"A query language model based on feedback documents, on based on a gene...",#L4.2 Application of KL-divergence Minimization
pdf121.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,A detailed breakdown of the BM25 formula and explanations.,#L4.1 The Okapi BM25 Formula
pdf122.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,Discussions on the advantages of the EM algorithm and more detailed ma...,#L5.2 EM Algorithm Convergence Discussion and Proof
pdf123.pdf,https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/plsa.pdf,For those who needs more information on Probabilistic Latent Semantic ...,#L5.1 More on Probabilistic Latent Semantic Analysis/Indexing
pdf124.pdf,https://towardsdatascience.com/implement-expectation-maximization-em-algorithm-in-python-from-scratch-f1278d1b9137,EM algorithm in Python #L6.2,"def run_em(x, params):"
pdf125.pdf,https://www.colorado.edu/amath/sites/default/files/attached-files/em_algorithm.pdf,#L6.1 EM algorithm example,Expectation Maximization (EM) Algorithm
pdf126.pdf,https://cgi.luddy.indiana.edu/~yye/i529/lectures/EM.pdf,EM algorithm explanations with examples and applications #L6.2,No Preview Available
pdf127.pdf,https://www.statisticshowto.com/em-algorithm-expectation-maximization/,Induction to EM Algorithm with applications and limitations #L6.1,No Preview Available
pdf128.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,#L5.2 Expectation Maximization Algorithm - All about it!,No Preview Available
pdf129.pdf,https://monkeylearn.com/topic-analysis/,#L5.1 Topic Analysis - A Guide,No Preview Available
pdf130.pdf,https://monkeylearn.com/topic-analysis/,#L5.1 Introduction about Topic Analysis,Topic Analysis: The Ultimate Guide
pdf131.pdf,http://www.hcbravo.org/dscert-algo/homeworks/topic_modeling.html,PLSA with EM Algorithm #L6.3,PLSA with EM Algorithm
pdf132.pdf,https://www.geeksforgeeks.org/ml-expectation-maximization-algorithm/,EM Algorithm #L6.2,Expectation-Maximization Algorithm
pdf133.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,Expectation Maximization #L6.1,Expectation Maximization Explained
pdf134.pdf,https://medium.com/towards-data-science/gaussian-mixture-models-explained-6986aaf5a95,a useful explanation of mixture model covered in # L 5.2,"Which reads “given a data point x, what is the probability it came from Gaussian k"
pdf135.pdf,https://towardsdatascience.com/deep-generative-models-25ab2821afd3,A useful explanation of the generative model covered in # L5.1,The idea is to learn a low-dimensional latent representation of the training data called latent variables (variables which are not directly observed but are rather inferred through a mathematical ...
pdf136.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,PLSA for #L6.3,No Preview Available
pdf137.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,EM convergence for #L6.2,No Preview Available
pdf138.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,EM Algorithm for #L6.1,No Preview Available
pdf139.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,PLSA for #L6.1,No Preview Available
pdf140.pdf,https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html,A detailed page in teaching how to compute the EM and models such as G...,"Derivation of EM equations
Illustration of EM convergence
Derivation of update equations of coin tossing example"
pdf141.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A supplementary blog talking about the EM Algorithm. Easy to understan...,A Gentle Introduction to Expectation-Maximization
pdf142.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,EM algorithm note with excellent examples #L5.1,EM Algorithm Course Note
pdf143.pdf,https://arxiv.org/pdf/1212.3900.pdf,A tutorial going over the PLSA with abundant and detailed proof #L6.2,Tutorial on Probablistic Latent Semantic Analysis
pdf144.pdf,https://see.stanford.edu/materials/aimlcs229/cs229-notes8.pdf,EM Algorithm #L6.2,No Preview Available
pdf145.pdf,https://probabilistic-latent-semantic-analysis.readthedocs.io/en/latest/plsa.algorithms.plsa.html,PLSA #L6.3,"At its core lies the assumption that the normalized document-word (or term-frequency) matrix p(d, w), weighted with the inverse document frequency or not, can be factorized as:"
pdf146.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,EM Algorithm #L6.2,No Preview Available
pdf147.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,The Expectation-Maximization Algorithm #L6.1,"The Expectation-Maximization Algorithm, or EM algorithm for short, is an approach for maximum likelihood estimation in the presence of latent variables."
pdf148.pdf,https://www.kaggle.com/code/charel/learn-by-example-expectation-maximization/notebook,Detailed Python code implementing the EM algorithm #L6.1,No Preview Available
pdf149.pdf,https://en.wikipedia.org/wiki/Mixture_model,#L5.2 A wiki page of Mixture model,Mixture model
pdf150.pdf,https://arxiv.org/pdf/2203.10256.pdf,Dependency-based Mixture Language Models L5.1,No Preview Available
pdf151.pdf,https://pzuliani.github.io/papers/Rusakovica_etal_IB2014.pdf,Probabilistic Latent Semantic Analysis Applied to Whole Bacterial Geno...,"In this work we applied the PLSA approach to analyse the common genomic features in methicillin resistant Staphylococcus aureus, using tokens derived from amino acid sequences rather than DNA"
pdf152.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,Description of Probabilistic Latent Semantic Analysis (PLSA)  that cov...,Topic modelling with PLSA
pdf153.pdf,https://stat.uw.edu/seminars/statistical-and-computational-guarantees-em-algorithm,The Computational Guarantees for the EM Algorithm that covered in the ...,Statistical and Computational Guarantees for the EM Algorithm
pdf154.pdf,https://www.statisticshowto.com/em-algorithm-expectation-maximization/,The introduction of EM Algorithm that covered in the lecture #L6.1,EM Algorithm (Expectation-maximization): Simple Definition
pdf156.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,A great website that discusses the the EM algorithm in generality(cove...,Expectation Maximization (EM) is a classic algorithm developed in the 60s and 70s with diverse applications. It can be used as an unsupervised clustering algorithm and extends to NLP applications l...
pdf157.pdf,https://www.analyticsvidhya.com/blog/2021/05/complete-guide-to-expectation-maximization-algorithm/,A great website that explains how EM algorithm works(discussed in #L6....,"In most of the real-life problem statements of Machine learning, it is very common that we have many relevant features available to build our model but only a small portion of them are observable. ..."
pdf158.pdf,https://towardsdatascience.com/em-algorithm-aaaa181af127,EM Algorithm derivation and Python code #L6.2,No Preview Available
pdf159.pdf,http://cs229.stanford.edu/proj2009/JalilianLiuPu.pdf,An essay taking about the application in PLSA. It includes introductio...,"An Application of
Probabilistic Latent Semantic Analysis (PLSA)"
pdf160.pdf,https://www.mygreatlearning.com/blog/pos-tagging/,#L7.1 HMM application - Part of Speech Tagging,Part of Speech (POS) tagging with Hidden Markov Model
pdf161.pdf,https://arxiv.org/pdf/1301.6705.pdf,#L6.3 Original paper on PLSA,Probabilistic Latent Semantic Analysis
pdf162.pdf,https://towardsdatascience.com/implement-expectation-maximization-em-algorithm-in-python-from-scratch-f1278d1b9137,Implement Expectation-Maximization Algorithm(EM) in Python from Scratc...,Implement Expectation-Maximization Algorithm(EM) in Python from Scratch
pdf163.pdf,https://cdn.intechopen.com/pdfs/79594.pdf,Fast Computation of the EM Algorithm for Mixture Models. Related to Le...,Fast Computation of the EM Algorithm for Mixture Models
pdf164.pdf,https://github.com/hitalex/PLSA,A github repo that implements PLSA via the EM algorithm using Python #...,No Preview Available
pdf165.pdf,https://reader.elsevier.com/reader/sd/pii/S0885230898901188?token=754C232B97B13537BC680BCDBEAF7E2078A1A64BA75E084C4475C1DE8A94AFB4DECC0A292212CA0EF30E4F36A295687A,The paper describes using interpolated n-gram and trigger pair languag...,"Interpolation of n-gram and
mutual-information based trigger pair language
models for Mandarin speech recognition"
pdf166.pdf,https://en.wikipedia.org/wiki/N-gram,The introduction of n-gram by Wikipedia #L2.2,N-gram
pdf167.pdf,http://mlwiki.org/index.php/Statistical_Language_Models,The basic concepts of statistical Language Models #L2.1,Statistical Language Models
pdf168.pdf,https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1888739,Two-Component Mixture Model in the Presence of Covariates L5.2,No Preview Available
pdf169.pdf,http://www.stat.columbia.edu/~bodhi/Talks/2ComponentMixModel.pdf,Estimation of a Two-component Mixture Model,No Preview Available
pdf170.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A useful explanation of EM algorithm covered in #L6.2,"The EM algorithm is an iterative approach that cycles between two modes. The first mode attempts to estimate the missing or latent variables, called the estimation-step or E-step. The second mode a..."
pdf171.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,A useful explanation of EM algorithm covered in #L6.1,"(Expectation) Assign each data point to a cluster probabilistically. In this case, we compute the probability it came from the red cluster and the yellow cluster respectively.
(Maximization) Updat..."
pdf172.pdf,https://arxiv.org/pdf/1212.3900.pdf,A Modern View of PLSA and discussion on EM Algorithm #L6.3,A Modern View of PLSA
pdf173.pdf,https://www.javatpoint.com/em-algorithm-in-machine-learning,Explanation of the EM algorithm and Convergence in the EM algorithm #L...,"The EM algorithm is considered a latent variable model to find the local maximum likelihood parameters of a statistical model, proposed by Arthur Dempster, Nan Laird, and Donald Rubin in 1977."
pdf174.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,Detailed Explanation of the EM Algorithm #L6.1,"The EM algorithm is used for obtaining maximum likelihood estimates of parameters when some of the data is
missing."
pdf175.pdf,https://ieeexplore.ieee.org/document/5279464,Application of PLSA to human actions,No Preview Available
pdf176.pdf,https://www.geeksforgeeks.org/introduction-hill-climbing-artificial-intelligence/,General hill climbing heurisitic #L6.2,No Preview Available
pdf177.pdf,https://www.javatpoint.com/em-algorithm-in-machine-learning,Algorithmic representation of EM algorithm #L6.1,No Preview Available
pdf178.pdf,https://en.wikipedia.org/wiki/Viterbi_algorithm,#L7.3 Viterbi algorithm pseudocode,"function VITERBI
(
���
,
���
,
Π
,
���
,
���
,
���
)
:
���"
pdf179.pdf,https://www.stat.cmu.edu/~cshalizi/dst/18/lectures/24/lecture-24.html,#L7.2 HMM calculation,Definition of HMMs
pdf180.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A Gentle Introduction to Expectation-Maximization (EM Algorithm) #L6...,A Gentle Introduction to Expectation-Maximization (EM Algorithm)
pdf181.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,#L6.1 EM algorithm example,Expectation–maximization algorithm
pdf182.pdf,https://arxiv.org/abs/1204.5488,a statistical method for estimating the mixing proportion and unknow...,Estimation of a Two-component Mixture Model with Applications to Multiple Testing
pdf183.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,"Blog article on topic mining, related to Lecture #5.1",Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection. It can also be thought o...
pdf184.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,Viterbi Algorithm for #L7.3,No Preview Available
pdf185.pdf,https://u-next.com/blogs/data-science/hidden-markov-model/,The disadvantage of HMM for #L7.2,"Disadvantages

HMM cannot represent any dependency between the appliances. The conditional HMM can capture the dependencies, though.
HMM does not consider the state sequence dominating any given..."
pdf186.pdf,https://www.nature.com/articles/nbt1004-1315,HMM for #L7.1,No Preview Available
pdf187.pdf,https://www.cs.toronto.edu/~rgrosse/csc321/mixture_models.pdf,#L6.1,No Preview Available
pdf188.pdf,https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86,#Lec7 A brief introduction of Baum-Welch algorithm,No Preview Available
pdf189.pdf,http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/HMM_problems.htm,"#Lec7 The three problems associated with an HMM, and how to solve them...",No Preview Available
pdf190.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,Probabilistic latent semantic analysis wikipidia #L6.3,Probabilistic latent semantic analysis
pdf191.pdf,https://www.javatpoint.com/em-algorithm-in-machine-learning,General introduction of EM Algorithm #L6.1,EM Algorithm
pdf192.pdf,https://stats.stackexchange.com/questions/83387/why-is-the-expectation-maximization-algorithm-guaranteed-to-converge-to-a-local,Explaination of why EM alogorithm guaranteed to converge #L6.2,Why is the Expectation Maximization algorithm guaranteed to converge
pdf193.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,Detail description about EM algorithm #L6.1 #L6.2,EM algorithm
pdf194.pdf,https://www.analyticsvidhya.com/blog/2021/05/complete-guide-to-expectation-maximization-algorithm/,Guide to the EM algorithm. #L6.1,Expectation-Maximization Algorithm
pdf195.pdf,https://github.com/yedivanseven/PLSA,#L6.3,A python implementation of Probabilistic Latent Semantic Analysis
pdf196.pdf,https://stats.stackexchange.com/questions/45652/what-is-the-difference-between-em-and-gradient-ascent,#L6.2,What is the difference between EM and Gradient Ascent?
pdf197.pdf,https://www.jstor.org/stable/2240463?casa_token=nAHm_bGEFj8AAAAA:w9lb6tlt75CdD_Cdtw_IM0DcxCVakoHsQNVOycbs-W--NzI7LYEsc5F1VcFRqNCqzZU43eDG5zN9F9rCRJHmfTDKyDmaWuKg9Q3yG2sK3Hz5V0GjJp8,EM algorithm can converge to local maximum or stationary value of inco...,On the Convergence Properties of EM Algorithm.
pdf198.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,Concepts and a lot of information about EM-algorithm by WIKIPEDIA #L6....,Expectation–maximization algorithm
pdf199.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,Wikipedia page of PLSA for Lecture #6.3,Probabilistic latent semantic analysis
pdf200.pdf,http://www0.cs.ucl.ac.uk/staff/c.archambeau/publ/esann_ca03.pdf,#L6.2: Example of a Practical Issue with the Convergence of EM Algorit...,Convergence Problems of the EM Algorithm for Finite Gaussian Mixtures
pdf201.pdf,https://arxiv.org/pdf/2012.01031.pdf,#6.1 A interesting work implementing EM algorithm on knowledge graph r...,"We employ the variational EM algorithm
to optimize knowledge graph embedding and logic
rule inference alternately. In this way, our model
could combine efforts from both the knowledge
graph emb..."
pdf202.pdf,https://www.youtube.com/watch?v=AnbiNaVp3eQ,#L6.1: Some pros and cons of EM algorithm (from 10:45 in the video),#L6.1: Some pros and cons of EM algorithm
pdf203.pdf,https://medium.com/analytics-vidhya/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6,Introduction of Viterbi algorithm with HMM #L7.3,Viterbi algorithm for prediction with HMM
pdf204.pdf,https://en.wikipedia.org/wiki/Viterbi_algorithm,Viterbi algorithm wikipidia page #L7.3,Viterbi algorithm
pdf205.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,Detail explination about HMM #L7.2,Markov and Hidden Markov Model
pdf206.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,Hidden Markov model wikipidia page #L7.1,Hidden Markov model
pdf207.pdf,https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model,Background and basic introduction of HMM #L7.1,Hidden Markov Models
pdf208.pdf,https://www.cs.cmu.edu/~lemur/doxygen/lemur-3.1/html/PLSA.html,Implementation about PLSA #L6.3,Probabilistic Latent Semantic Analysis
pdf209.pdf,https://www.youtube.com/watch?v=vtadpVDr1hM,Useful youtube video about PLSA,Probabilistic Latent Semantic Analysis
pdf210.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,A detailed introduction to the em algorithm #L6.1,No Preview Available
pdf211.pdf,https://medium.com/technovators/topic-modeling-art-of-storytelling-in-nlp-4dc83e96a987,Covers PLSA (L#6.3) implementation in Python,Topic Modeling: Art of Storytelling in NLP
pdf212.pdf,https://handwiki.org/wiki/Probabilistic_latent_semantic_analysis,HandWiki entry for PLSA,#L6.3: Probabilistic latent semantic analysis
pdf213.pdf,http://csce.uark.edu/~lz006/course/2021fall/15-em.pdf,Slides from UArk for EM Algorithm,"L 6.1, 6.2: Expectation-Maximization (EM) Algorithm"
pdf214.pdf,https://engineering.purdue.edu/ChanGroup/ECE645Notes/StudentLecture10.pdf,Notes from Purdue University for the EM algorithm,"L6.1, 6.2: Expectation-Maximization Algorithm"
pdf215.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,Brief introduction of Probabilistic latent semantic analysis. Related ...,Probabilistic latent semantic analysis
pdf216.pdf,https://www.youtube.com/watch?v=xy96ArOpntA,#6.1 EM-algorithm clearly explained from its intuition to the derivati...,No Preview Available
pdf217.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,#L6.3 PLSA,No Preview Available
pdf218.pdf,https://www.analyticsvidhya.com/blog/2021/05/complete-guide-to-expectation-maximization-algorithm/,#L6.2 An explanation of EM algorithm,No Preview Available
pdf219.pdf,https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58,#L6.1 EM algorithm in N-gram language models,"Expectation-maximization (EM) algorithm
If you were to look at resources for EM algorithm on the Internet, you’d get the impression that the algorithm is extremely complex. However, at the core of..."
pdf220.pdf,http://romip.ru/russiras/doc/russir2012/pgmir2.pdf,These slides explain EM algorithm on mixture models #L6.1,Unigram mixtures and the EM algorithm
pdf221.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,#L6.3 - Good explanation of PLSA with example,No Preview Available
pdf222.pdf,https://arxiv.org/pdf/1611.00519.pdf,#L6.2 - In-depth analysis of EM algorithm convergence,No Preview Available
pdf223.pdf,https://medium.com/@chloebee/the-em-algorithm-explained-52182dbb19d9,Has an example to describe how the EM algorithm converges #L6.2,No Preview Available
pdf224.pdf,http://www.mit.edu/~6.454/www_fall_2002/shaas/summary.pdf,#L6.1 - A detailed research of EM algorithm as a hill climbing algorit...,No Preview Available
pdf225.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_em.html,Goes over the formal definition of the EM algorithm #L6.1,No Preview Available
pdf226.pdf,https://github.com/hitalex/PLSA,#L6.3 Python implementation of PLSA,PLSA implementation via EM
pdf227.pdf,https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1268,#L6.1 Further explanation about lower bound optimization in EM,"The EM Algorithm As a Lower Bound Optimization
Technique"
pdf228.pdf,https://arxiv.org/pdf/1212.3900.pdf,Probabilistic Latent Semantic Analysis #L6.3,No Preview Available
pdf229.pdf,https://towardsdatascience.com/implement-expectation-maximization-em-algorithm-in-python-from-scratch-f1278d1b9137,Implement Expectation-Maximization Algorithm(EM) in Python from Scratc...,Implement Expectation-Maximization Algorithm(EM) in Python from Scratch
pdf230.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,EM algorithm mentioned in Lecture 6.1 and 6.2,No Preview Available
pdf231.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,This explains about PLSA #L6.3,Topic modelling with PLSA
pdf232.pdf,https://www.intechopen.com/chapters/79594,Introduce a fast computation of the EM Algorithm #L6.2,Fast Computation of the EM Algorithm for Mixture Models
pdf233.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,Introduction to EM Algorithm #L6.1,A Gentle Introduction to Expectation-Maximization (EM Algorithm)
pdf234.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,"Introduction to EM includes E, M step #L6.2","The EM algorithm is an iterative approach that cycles between two modes. The first mode attempts to estimate the missing or latent variables, called the estimation-step or E-step. The second mode a..."
pdf235.pdf,https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV1011/oneata.pdf,Probabilistic Latent Semantic Analysis (PLSA) from a paper #L6.3,No Preview Available
pdf236.pdf,https://www.youtube.com/watch?v=i0yoNc6BLHM,Mixture of two Unigram Language Models from youtube video #L6.1,02 07 2 7 Probabilistic Topic Models Mixture of Unigram Language Models 00 12 39
pdf237.pdf,https://www.lri.fr/~sebag/COURS/EM_algorithm.pdf,This explains the EM algorithm along with derivation and explains abou...,The Expectation Maximization Algorithm
pdf238.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,The article provides a summary of how the EM algorithm is applied to P...,No Preview Available
pdf239.pdf,https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/EM_slides.pdf,This explains more about EM algorithm #L6.1,EM ALGORITHM
pdf240.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,#7.2 viterbi algorithm,No Preview Available
pdf241.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,def of hmm #7.1,No Preview Available
pdf242.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,This paper consists of examples and explanation of EM algorithm covere...,No Preview Available
pdf243.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,#L6.3 Topic modelling with PLSA,PLSA or Probabilistic Latent Semantic Analysis is a technique used to model information under a probabilistic framework. Latent because the topics are treated as latent or hidden variables.
pdf244.pdf,https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd,#L6.2 Guassian Mixture models and EM,"In this post, we will explain how latent variables can also be used to frame a classification problem, namely the Gaussian Mixture model (or GMM in short) that allows us to perform soft"
pdf245.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,#L6.1 EM algorithm expalined in a simle understandable way,Expectation Maximization (EM) is a classic algorithm developed in the 60s and 70s with diverse applications.
pdf246.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L6.1 An introduction of EM algorithm,A Gentle Introduction to Expectation-Maximization (EM Algorithm)
pdf247.pdf,https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV1011/oneata.pdf,This article gives a small intro to the mathematics behidn the Probabi...,No Preview Available
pdf248.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,This is a gentle intro to the EM alg which is discussed in lecture #L6...,Maximum likelihood estimation is an approach to density estimation for a dataset by searching across probability distributions and their parameters.
pdf249.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,PLSA model mentioned in Lecture 6.3,No Preview Available
pdf250.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,A simple explanation of EM algorithm #L6.2,No Preview Available
pdf251.pdf,https://statweb.stanford.edu/~jtaylo/courses/stats306b/restricted/notebooks/EM_algorithm.pdf,An explanation of EM algorithm #L6.1,No Preview Available
pdf252.pdf,https://ieeexplore.ieee.org/document/543975,An interesting intro paper about EM algorithm #L6.1,The EM algorithm is presented at a level suitable for signal processing practitioners who have had some exposure to estimation theory.
pdf253.pdf,https://dl.acm.org/doi/10.5555/2073796.2073829,#L6.3 Probabilistic latent semantic analysis,No Preview Available
pdf254.pdf,https://arxiv.org/abs/1301.6705,#L6.3 A paper on PLSA,No Preview Available
pdf255.pdf,https://arxiv.org/pdf/1611.00519.pdf,#L6.2 A pdf explaining convergence of EM algorithm in detail,No Preview Available
pdf256.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,#L6.1 A full-length explanation to Expectation Maximization Algorithm,No Preview Available
pdf257.pdf,https://projecteuclid.org/journals/annals-of-statistics/volume-11/issue-1/On-the-Convergence-Properties-of-the-EM-Algorithm/10.1214/aos/1176346060.full,#L6.2 Converge properties of EM algorithm,No Preview Available
pdf258.pdf,https://medium.com/analytics-vidhya/expectation-maximization-algorithm-step-by-step-30157192de9f,#L6.1 E step and M step in EM algorithm,No Preview Available
pdf259.pdf,https://www.alteryx.com/glossary/supervised-vs-unsupervised-learning,diff between supervised and unsupervised training,No Preview Available
pdf260.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,#L6.3 - PLSA introduction,No Preview Available
pdf261.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,The wikipedia of the E-M algorithm coverd in L6.1,Expectation–maximization algorithm
pdf262.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L6.1 Gentle Introduction to EM,No Preview Available
pdf263.pdf,https://yangxiaozhou.github.io/data/2020/10/20/EM-algorithm-explained.html,#L6.2 Deep introduction to EM algorithm,No Preview Available
pdf264.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L6.1,No Preview Available
pdf265.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,"#L6.3: The Wikipedia page of the topic ""PLSA"" covered in L6.3.",Probabilistic latent semantic analysis
pdf266.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,#L6.2: The handout of E-M algorithm of CS @ Columbia,No Preview Available
pdf267.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,"#L6.1: The Wikipedia page for the topic ""E-M algorithm"" covered in L6....",Expectation–maximization algorithm
pdf268.pdf,https://www.nature.com/articles/nbt1406,#L6.1 EM Algorithm,"What is the expectation maximization algorithm?
Chuong B Do & Serafim Batzoglou 
Nature Biotechnology volume 26, pages897–899 (2008)Cite this article

104k Accesses

315 Citations

59 Altme..."
pdf269.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,An simple explanation of PLSA #L6.3,No Preview Available
pdf270.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,#L6.2 A Good Article on Convergence Guarantee,No Preview Available
pdf271.pdf,https://www.mygreatlearning.com/blog/pos-tagging/,"An application of HMM, POS tagging #7.1","POS tagging with Hidden Markov Model
HMM (Hidden Markov Model) is a Stochastic technique for POS tagging. Hidden Markov models are known for their applications to reinforcement learning and tempor..."
pdf272.pdf,https://www.analyticssteps.com/blogs/expectation-maximization-em-algorithm-machine-learning,#6.1 All about Expectation-Maximization,No Preview Available
pdf273.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,#L6.2 EM algorithm | Explanation and proof of convergence - StatLect,EM algorithm
pdf274.pdf,https://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf,A Revealing Introduction to Hidden Markov Models #L7.1,No Preview Available
pdf275.pdf,https://arxiv.org/pdf/1301.6705.pdf,#L6.3,No Preview Available
pdf276.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,#L6.2,No Preview Available
pdf277.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L6.1,No Preview Available
pdf278.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,A introduction article of PLSA in L6.3,Topic modelling with PLSA
pdf279.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A introduction article of E-M algorithm in L6.2,A Gentle Introduction to Expectation-Maximization (EM Algorithm)
pdf280.pdf,https://medium.com/technovators/topic-modeling-art-of-storytelling-in-nlp-4dc83e96a987,#L6.3,"In simple context, we sample a document first then based on the document we sample a topic, and based on the topic we sample a word, which means d and w are conditionally independent given a hidden..."
pdf281.pdf,http://www.statslab.cam.ac.uk/~rrw1/markov/M.pdf,a detailed textbook introducing markov chain in mathematical view,Markov chains
pdf282.pdf,https://medium.com/b2w-engineering-en/the-reasoning-behind-the-expectation-maximization-em-algorithm-4a773428b3fc,#L6.2,No Preview Available
pdf283.pdf,https://medium.com/@chloebee/the-em-algorithm-explained-52182dbb19d9,#L6.1,No Preview Available
pdf284.pdf,https://github.com/laserwave/plsa,#L6.3 A python implementation of Probabilistic Latent Semantic Analysi...,No Preview Available
pdf285.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_em.html,#L6.1 Introduction to EM,No Preview Available
pdf286.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,#L6.1 #L6.2 Explanation and proof of convergence of EM model,No Preview Available
pdf287.pdf,https://www.upgrad.com/blog/em-algorithm-in-machine-learning/,"This blog provided a brief yet concrete example on EM algorithm, along...",#6.1 EM algorithm
pdf288.pdf,http://www.hcbravo.org/dscert-algo/homeworks/topic_modeling.html,#L6.3 A practice of PLSA with EM algorithm,Probabilistic Latent Semantic Analysis with the EM Algorithm
pdf289.pdf,http://rstudio-pubs-static.s3.amazonaws.com/184236_2e8be74535d94690b57ff363dc8e64a2.html,#L6.3 Understanding PLSA,No Preview Available
pdf290.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,An overview of PLSA #L6.3,"Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis o..."
pdf291.pdf,https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV1011/oneata.pdf,#L6.3: An article on PLSA categories and applications,No Preview Available
pdf292.pdf,https://people.cs.umass.edu/~mccallum/courses/inlp2004a/lect10-hmm2.pdf,some tips and tricks on Baum Welch algorithms #L7.3,Baum-Welch tips and tricks: normalization
pdf293.pdf,http://cs229.stanford.edu/proj2009/JalilianLiuPu.pdf,An application of Probabilistic Latent Semantic Analysis #L6.3,No Preview Available
pdf294.pdf,https://www.analyticsvidhya.com/blog/2021/06/part-17-step-by-step-guide-to-master-nlp-topic-modelling-using-plsa/,An overview of PLSA topic model #L6.3,Topic Modelling using PLSA
pdf295.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,A discussion of the EM algorithm convergence in a more mathematical wa...,Intuition about EM algorithm convergence
pdf296.pdf,https://www.dcs.bbk.ac.uk/~dell/teaching/cc/book/ditp/ditp_ch7.pdf,EM Algorithm for Text Processing #L6.2,No Preview Available
pdf297.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A good explanation of EM Algorithm #L6.1,A Gentle Introduction to Expectation-Maximization
pdf298.pdf,https://aclanthology.org/P09-3008.pdf,Optimizing Language Model Information System with Expectation Maximiza...,No Preview Available
pdf299.pdf,http://www.statslab.cam.ac.uk/~rrw1/markov/M.pdf,a textbook introducing Markov Chain in detail with mathematical view #...,Markov Chain
pdf300.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_em.html,"Introduction to EM: Gaussian Mixture Models
#L6.1",Introduction to EM: Gaussian Mixture Models
pdf301.pdf,https://dl.acm.org/doi/10.5555/2073796.2073829,Probabilistic latent semantic analysis,"Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural l..."
pdf302.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A Gentle Introduction to Expectation-Maximization (EM Algorithm),"Maximum likelihood estimation is an approach to density estimation for a dataset by searching across probability distributions and their parameters.

It is a general and effective approach that u..."
pdf303.pdf,https://www.analyticsvidhya.com/blog/2021/05/complete-guide-to-expectation-maximization-algorithm/,#L6.2 Good intro to EM algorithm and Gaussian Mixture models,No Preview Available
pdf304.pdf,https://medium.com/@chloebee/the-em-algorithm-explained-52182dbb19d9,The EM Algorithm Explained #L6.1,"The Expectation-Maximization algorithm (or EM, for short) is probably one of the most influential and widely used machine learning algorithms in the field."
pdf305.pdf,https://arxiv.org/pdf/1611.00519.pdf,This paper analyzes the convergence of the EM algorithm #L6.2,No Preview Available
pdf306.pdf,https://www.youtube.com/watch?v=DIADjJXrgps,EM algorithm for ML video explaination. #L6.2,No Preview Available
pdf307.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,An overview of how EM algorithm works #L6.1,The expectation-maximization algorithm is an approach for performing maximum likelihood estimation in the presence of latent variables. It does this by first estimating the values for the latent va...
pdf308.pdf,https://www.edureka.co/blog/em-algorithm-in-machine-learning/,EM algorithm with Machine Learning perspective. #L6.1,No Preview Available
pdf309.pdf,https://www.edureka.co/blog/em-algorithm-in-machine-learning/,EM algorithm with Machine Learning perspective,No Preview Available
pdf310.pdf,https://nowak.ece.wisc.edu/ece830/ece830_fall11_lecture16.pdf,#L6.1 #L6.2 Introduced gradient descent and EM algorithm as two approa...,No Preview Available
pdf311.pdf,http://www.hcbravo.org/dscert-algo/homeworks/topic_modeling.html,An exercise on PLSA with EM algorithm #6.2,No Preview Available
pdf312.pdf,https://aclanthology.org/E06-1014.pdf,"Improving Probabilistic Latent Semantic Analysis
with Principal Compo...",No Preview Available
pdf313.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,#L6.2,No Preview Available
pdf314.pdf,http://faculty.washington.edu/fxia/courses/LING572/EM_collins97.pdf,Math proof of EM algorithm effectiveness and its convergence #6.1,No Preview Available
pdf315.pdf,http://www.hcbravo.org/dscert-algo/homeworks/topic_modeling.html,PLSA algorithm covered in #L6.3,Probabilistic Latent Semantic Analysis with the EM Algorithm
pdf316.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,Topic modelling with PLSA #L6.3,Topic modelling with PLSA
pdf317.pdf,https://arxiv.org/pdf/1611.00519.pdf,#L6.2,No Preview Available
pdf318.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,Explanation and proof of convergence EM Algorithm covered in #L6.2,EM algorithm
pdf319.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,EM algorithm,The Expectation-Maximization (EM) algorithm is a recursive algorithm that can be used to search for the maximum likelihood estimators of model parameters when the model includes some unobservable v...
pdf320.pdf,https://stats.stackexchange.com/questions/83387/why-is-the-expectation-maximization-algorithm-guaranteed-to-converge-to-a-local,Explains EM algorithm convergence in #L6.2,Why is the Expectation Maximization algorithm guaranteed to converge to a local optimum?
pdf321.pdf,https://arxiv.org/ftp/arxiv/papers/1301/1301.6705.pdf,#L6.3 Original paper of PSLA,No Preview Available
pdf322.pdf,https://medium.com/analytics-vidhya/expectation-maximization-algorithm-step-by-step-30157192de9f,#L6.2 E step and M step in EM algorithm,No Preview Available
pdf323.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,#L6.1An introduction to the EM algorithm,No Preview Available
pdf324.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,#6.3Simple Introduction to PLSA,No Preview Available
pdf325.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,Explains EM algorithm in #L6.1,Expectation–maximization algorithm
pdf326.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,Concept of PLSA model in lecture #L6.3,"Probabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis o..."
pdf327.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,EM algorithm in lecture #L6.2,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, wher..."
pdf328.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,EM algorithm in lecture #L6.1,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, wher..."
pdf329.pdf,http://rstudio-pubs-static.s3.amazonaws.com/184236_2e8be74535d94690b57ff363dc8e64a2.html,Math and codes: PLSA with EM algorithm #6.2,No Preview Available
pdf330.pdf,https://github.com/hamzarawal/HMM-Baum-Welch-Algorithm,Numpy Implementation of Baum-Welch (Forward-Backward) algorithm in Pyt...,Numpy Implementation of Baum-Welch (Forward-Backward) algorithm in Python.
pdf331.pdf,http://www.adeveloperdiary.com/data-science/machine-learning/forward-and-backward-algorithm-in-hidden-markov-model/,Forward and Backward Algorithm in Hidden Markov Model #L7.2,Forward and Backward Algorithm in Hidden Markov Model
pdf332.pdf,https://www.geeksforgeeks.org/introduction-hill-climbing-artificial-intelligence/,"General definition of hill climbing, can be used to compare with EM",General introduction to hill climbing problem#L6.1
pdf333.pdf,https://towardsdatascience.com/hidden-markov-model-implemented-from-scratch-72865bda430e,Hidden Markov Model — Implemented from scratch #L7.2,Hidden Markov Model — Implemented from scratch
pdf334.pdf,https://medium.com/@natsunoyuki/hidden-markov-models-with-python-c026f778dfa7,Hidden Markov Models with Python #L7.1,Hidden Markov Models with Python
pdf335.pdf,https://arxiv.org/pdf/1301.6705.pdf,Original Paper for PLSA,Original Paper for PLSA #L6.3
pdf336.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,#6.3,Probabilistic latent semantic analysis
pdf337.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,#6.2,Expectation Maximization Explained
pdf338.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,#6.1 EM algorithm,Expectation–maximization algorithm
pdf339.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,Explains PLSA in #L6.3,Topic modelling with PLSA
pdf340.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,L6.2: Introduction to EM Algorithm,No Preview Available
pdf341.pdf,https://stats.stackexchange.com/questions/83387/why-is-the-expectation-maximization-algorithm-guaranteed-to-converge-to-a-local,L6.1: EM algorithm and local minimum,EM is not guaranteed to converge to a local minimum. It is only guaranteed to converge to a point with zero gradient with respect to the parameters. So it can indeed get stuck at saddle points.
pdf342.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_em.html,A comprehensive introduction to EM #L6.1,Introduction to EM: Gaussian Mixture Models
pdf343.pdf,https://arxiv.org/abs/1301.6705,#L6.3 The original PLSA paper that introduces the novel statistical te...,"Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural l..."
pdf344.pdf,https://engineering.purdue.edu/ChanGroup/ECE645Notes/StudentLecture10.pdf,#L6.1 Helpful notes on the EM algorithm with detailed proofs on derivi...,No Preview Available
pdf345.pdf,https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV1011/oneata.pdf,A nice summary of PLSA #L6.3,Probabilistic Latent Semantic Analysis
pdf346.pdf,https://people.eecs.berkeley.edu/~jordan/papers/jin-etal-nips16.pdf,#L6.2 A paper that discusses the behavior and consequences of local ma...,No Preview Available
pdf347.pdf,https://cs229.stanford.edu/notes2022fall/main_notes.pdf,Chapter 11 deals with EM algorithms #L6.2,EM algorithms
pdf348.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,A detailed set of notes on the EM algorithm #L6.1,The EM Algorithm
pdf349.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,Proof of convergence of EM algorithm#L6.2,Proof of convergence of EM algorithm#L6.2
pdf350.pdf,https://en.wikipedia.org/wiki/Baum–Welch_algorithm,Wiki page on Baum-Welch algorithm #L7.3,No Preview Available
pdf351.pdf,https://www.analyticsvidhya.com/blog/2021/06/part-17-step-by-step-guide-to-master-nlp-topic-modelling-using-plsa/,"#L6.3
Good graphic notation on PLSA","Recap the basic assumption of topic modelling algorithms:

Each document consists of a mixture of topics, and
Each topic consists of a collection of words.
pLSA stands for Probabilistic Latent ..."
pdf352.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,Wiki page on Hidden Markov model #L7.2,No Preview Available
pdf353.pdf,https://en.wikipedia.org/wiki/Markov_decision_process,wiki page on Markov decision process #L7.1,No Preview Available
pdf354.pdf,http://www.hcbravo.org/dscert-algo/homeworks/topic_modeling.html,An implementation of PLSA with EM algorithm#L6.3,Probabilistic Latent Semantic Analysis with the EM Algorithm
pdf355.pdf,https://www.dcs.bbk.ac.uk/~dell/teaching/cc/book/ditp/ditp_ch7.pdf,"#L6.2
An EM Example","Let’s look at how to estimate the parameters from our latent variable marble
game from Section 7.1 using EM. We assume training data x consisting of
N = |x| observations of X with Na, Nb, and Nc ..."
pdf356.pdf,https://arxiv.org/ftp/arxiv/papers/1112/1112.2028.pdf,"#L6.1
EM algorithm and document classification","Data mining [2][3] is the extraction of useful knowledge from large amount of data. Data mining
tools can provide solution to the business problems that were to too time consuming when done
manua..."
pdf357.pdf,https://www.lri.fr/~sebag/COURS/EM_algorithm.pdf,Convergence of EM algorithm #L6.2,"The Expectation Maximization Algorithm
A short tutorial"
pdf358.pdf,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=,An introduction about EM algorithm #L5.2,The Expectation-Maximization Algorithm
pdf359.pdf,http://rstudio-pubs-static.s3.amazonaws.com/184236_2e8be74535d94690b57ff363dc8e64a2.html,L6.3: More information about plsa,No Preview Available
pdf360.pdf,https://link.springer.com/chapter/10.1007/11744085_40,#6.3 PLSA Example - Scene Classification,No Preview Available
pdf361.pdf,https://arxiv.org/pdf/1301.6705.pdf,#6.2 PLSA reference paper,No Preview Available
pdf362.pdf,https://see.stanford.edu/materials/aimlcs229/cs229-notes8.pdf,#6.1 EM Algorithm,No Preview Available
pdf363.pdf,https://collected.jcu.edu/cgi/viewcontent.cgi?article=1031,#L6.1 A paper that talks about the application EM in data analysis and...,"With the growing prevalence of big data, it is interesting to see whether
the popular machine-learning tool known as the Expectation Maximization (EM)
algorithm (Dempster, Laird, & Rubin, 1977) i..."
pdf364.pdf,https://arxiv.org/pdf/1212.3900.pdf,Probabilistic Latent Semantic Analysis #L6.3,No Preview Available
pdf365.pdf,https://github.com/laserwave/plsa,"#L6.3 Python code of PLSA using EM, with English and Chinese support",No Preview Available
pdf366.pdf,https://www.nowpublishers.com/article/Details/SIG-034,Theory and Use of the EM Algorithm #L6.2,No Preview Available
pdf367.pdf,https://ieeexplore.ieee.org/abstract/document/410439?casa_token=eup4oBCaONwAAAAA:mffBFzvv1xA_wPdQJzFQ49cXabGdlYM3ePs_cvkR39XhU61q7m6ayu-2vnFRvq_vIfHzmoq7vTM,Implementing the Viterbi algorithm #L7.4,No Preview Available
pdf368.pdf,https://www.tandfonline.com/doi/abs/10.1198/jcgs.2011.09109,Online EM Algorithm for Hidden Markov Models #L7.2,No Preview Available
pdf369.pdf,https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1985.tb00439.x,A Probabilistic Distance Measure for Hidden Markov Models #L7.1,No Preview Available
pdf370.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,Wikipedia page of hidden Markov mode. Relevant to Lecture #L7.1,"A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process — call it {\displaystyle X} — with unobservable (""hidden"") states."
pdf371.pdf,https://stats.stackexchange.com/questions/177801/details-in-proof-for-convergence-of-expectation-maximization-algorithm,L6.2: Proof of EM algorithm convergence,No Preview Available
pdf373.pdf,https://link.springer.com/content/pdf/10.1023/A:1007617005950.pdf,L6.3 Unsupervised learning with PLSA,No Preview Available
pdf374.pdf,https://hmmlearn.readthedocs.io/en/latest/,#L.2 the mostly common used library for HMM in python. I use it for se...,No Preview Available
pdf375.pdf,https://ieeexplore.ieee.org/document/674360,#L7.1 a paper relating the error correction code with hidden markov mo...,No Preview Available
pdf376.pdf,https://towardsdatascience.com/topic-modelling-with-plsa-728b92043f41,PLSA#6.3,No Preview Available
pdf378.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,EM#6.1,No Preview Available
pdf379.pdf,https://docs.oracle.com/database/121/DMCON/GUID-F4D117F3-FA0C-4CA4-9034-67D12339AE90.htm,#L6.2 An Oracle documentation that illustrates the concepts in EM algo...,"About Expectation Maximization
Expectation Maximization (EM) estimation of mixture models is a popular probability density estimation technique that is used in a variety of applications. Oracle Da..."
pdf380.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,A good explanation for Hidden Markov Model about what it is with examp...,Markov and Hidden Markov Model
pdf381.pdf,https://nipunbatra.github.io/hmm/,Exploring Hidden Markov Models with interactive illustration #L7.2,Hidden Markov Models - An interactive illustration
pdf382.pdf,https://nanonets.com/blog/topic-modeling-with-lsa-plsa-lda-lda2vec/,"Different topic modeling techniques. LSA, PLSA, and LDA. What are thei...","#L6.3 LSA, PLSA, and LDA"
pdf383.pdf,https://stats.stackexchange.com/questions/83387/why-is-the-expectation-maximization-algorithm-guaranteed-to-converge-to-a-local,EM is not guaranteed to converged to a local minimum. It is only guara...,#L6.2 EM Algorithm Convergence Guarantee?
pdf384.pdf,https://www.dcs.bbk.ac.uk/~dell/teaching/cc/book/ditp/ditp_ch7.pdf,This text explain expectation maximization algorithm as generalization...,"#L6.1 EM algorithm, latent variables, and HMMs"
pdf385.pdf,https://towardsdatascience.com/training-hidden-markov-models-831c1bdec27d,Training Hidden Markov Models #L7.3,Training Hidden Markov Models
pdf386.pdf,https://medium.com/analytics-vidhya/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6,Viterbi algorithm for prediction with HMM #L7.2,Viterbi algorithm for prediction with HMM
pdf387.pdf,https://analyticsindiamag.com/a-guide-to-hidden-markov-model-and-its-applications-in-nlp/,A Guide to Hidden Markov Model and its Applications in NLP #L7.1,A Guide to Hidden Markov Model and its Applications in NLP
pdf388.pdf,https://en.wikipedia.org/wiki/Viterbi_algorithm,Wikipedia page of Viterbi algorithm. Relevant to Lecture #7.3,The Viterbi algorithm is a dynamic programming algorithm for obtaining the maximum a posteriori probability estimate of the most likely sequence of hidden states—called the Viterbi path—that result...
pdf389.pdf,https://analyticsindiamag.com/a-guide-to-hidden-markov-model-and-its-applications-in-nlp/,A blog post introducing applications of HMM. Lecture #L7.2,"An application, where HMM is used, aims to recover the data sequence where the next sequence of the data can not be observed immediately but the next data depends on the old sequences."
pdf390.pdf,https://faculty.sbs.arizona.edu/hammond/archive/ling178-sp06/mathCh8.pdf,#L7.2 A introduction about HMM and other probability models that are s...,"In this chapter, we consider probability models that are specifically linguistic: Hidden Markov Models (HMMs) and Probabilistic Context-free Grammars (PCFGs).
These models can be used to directly ..."
pdf391.pdf,https://www.researchgate.net/publication/4208250_Human-computer_dialogue_simulation_using_hidden_Markov_models,#L7.1 A research that used HMM language model,"This paper presents a probabilistic method to simulate task-oriented human-computer dialogues at the intention level, that may be used to improve or to evaluate the performance of spoken dialogue s..."
pdf392.pdf,http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/,Derivation and implementation of Baum Welch Algorithm for Hidden Marko...,Baum Welch Algorithm for Hidden Markov Model
pdf393.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,A useful page that goes into details about HMM #L7.1,Markov models are used to predict the future state based on the current hidden or observed states. Markov model is a finite-state machine where each state has an associated probability of being in ...
pdf394.pdf,https://towardsdatascience.com/training-hidden-markov-models-831c1bdec27d,The introduction of Baum-Welch Algorithms that covered in the lecture ...,Training Hidden Markov Models: The Baum-Welch and Forward-Backward Algorithms
pdf395.pdf,https://www.mathworks.com/help/stats/hidden-markov-models-hmm.html,The introduction of Hidden Markov Models that covered in the lecture #...,Hidden Markov Models (HMM)
pdf396.pdf,http://www.adeveloperdiary.com/data-science/machine-learning/forward-and-backward-algorithm-in-hidden-markov-model/,The description of Forward and Backward Algorithm that covered in the ...,Forward and Backward Algorithm in Hidden Markov Model
pdf397.pdf,https://towardsdatascience.com/cross-topic-argument-mining-learning-how-to-classify-texts-1d9e5c00c4cc,More introduction of Topic Mining and Analysis that covered in the lec...,Cross-Topic Argument Mining: Learning How to Classify Texts
pdf398.pdf,https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86,Good introduction for the Baum-Welch algorithm in part 2 and Viterbi a...,Baum-Welch algorithm for training a Hidden Markov Model
pdf399.pdf,http://jedlik.phy.bme.hu/~gerjanos/HMM/node6.html,A good supplementary talking about the three problems of HMMs and the ...,Three basic problems of HMMs
pdf400.pdf,http://courses.washington.edu/ling573/SPR2014/slides/ling573_class6_queryexp_pass_short.pdf,#L7.1: An overview of some tools for Passage Retrieval,No Preview Available
pdf401.pdf,http://www.super.tka4.org/materials/lib/Articles-Books/Speech Recognition/hmm-tutorial.pdf,A great introduction to Hidden Markov Models #L7.3,"The Hidden Markov Model (HMM) is a popular statistical tool for modelling a wide
range of time series data. In the context of natural language processing(NLP), HMMs have
been applied with great s..."
pdf402.pdf,https://d1wqtxts1xzle7.cloudfront.net/31037917/Rabiner1986_An_Introduction_to_Hidden_Markov_Models-libre.pdf?1392213829=,A great introduction to Hidden Markov Model  #L7.2,"The basic theory of Markov chains has been known to
mathematicians and engineers for close to 80 years, but it is
only in the past decade that it has been applied explicitly to
problems in speec..."
pdf403.pdf,https://courses.cs.duke.edu/cps160/compsci260/fall14/resources/HMM/eddy96.pdf,A great explanation of Hidden Markov Models #L7.1,"Computational analysis is increasingly important for inferring the functions and structures of proteins [1] because
the speed of DNA sequencing has long since surpassed
the rate at which the biol..."
pdf404.pdf,https://people.cs.umass.edu/~mccallum/courses/inlp2004a/lect10-hmm2.pdf,#L7.3 alternative Baum Welch Algorithm slides,"Hidden Markov Models
Baum Welch Algorithm"
pdf405.pdf,https://ieeexplore.ieee.org/document/1450960,The original paper for Viterbi algorithm(discussed in #L6.3).,The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems...
pdf406.pdf,https://igntu.ac.in/eContent/IGNTU-eContent-804059256131-MA-Linguistics-4-HarjitSingh-ComputationalLinguistics-3.pdf,A great textbook explaining three examples associated with HMM: Decodi...,"HMM is probabilistic model for machine learning. It is mostly used in
speech recognition, to some extent it is also applied for classification task. HMM
provides solution of three problems : eval..."
pdf407.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,A great tutorial that explains the intuition behind Markov assumptions...,Markov and Hidden Markov models are engineered to handle data which can be represented as ‘sequence’ of observations over time. Hidden Markov models are probabilistic frameworks where the observed ...
pdf408.pdf,https://www.cl.cam.ac.uk/teaching/1718/MLRD/slides/slides9.pdf,More slides on the Viterbi Algorithm for HMM decoding #L7.2,No Preview Available
pdf409.pdf,https://medium.com/analytics-vidhya/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6,#L7.3 An easy-to-understand introduction about viterbi algorithm,"The purpose of the Viterbi algorithm is to make an inference based on a trained model and some observed data. It works by asking a question: given the trained parameter matrices and data, what is t..."
pdf410.pdf,https://www.tidytextmining.com/topicmodeling.html,#L5.1: Implementation of topic modeling,"Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows docum..."
pdf411.pdf,https://pitt.libguides.com/textmining/topicmodeling,#L5.1: Tools for topic mining,"An introduction to text mining/analysis and resources for finding text data, preparing text data for analysis, methods and tools for analyzing text data, and further readings regarding text mining ..."
pdf412.pdf,https://pitt.libguides.com/textmining/topicmodeling,Tools for topic mining,"An introduction to text mining/analysis and resources for finding text data, preparing text data for analysis, methods and tools for analyzing text data, and further readings regarding text mining ..."
pdf413.pdf,https://www.tidytextmining.com/topicmodeling.html,Implementation of topic modeling,"Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows docum..."
pdf414.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,Topic mining,Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection. It can also be thought o...
pdf415.pdf,https://www.youtube.com/watch?v=xejm-z3sbWA,#L7.2: A worked out example of Viterbi algorithm for Decoding problem ...,No Preview Available
pdf416.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,A good explanation of Viterbi algorithm covered in #L7.3,The Viterbi algorithm is a dynamic programming algorithm similar to the forward procedure which is often used to find maximum likelihood. Instead of tracking the total probability of generating the...
pdf417.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,A good example for HMM covered in #L7.2,Learning in HMMs involves estimating the state transition probabilities A and the output emission probabilities B that make an observed sequence most likely. Expectation-Maximization algorithms are...
pdf418.pdf,https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model,A good explanation for HMM mentioned in #L7.1,"In Computational Biology, a hidden Markov model (HMM) is a statistical approach that is frequently used for modelling biological sequences. In applying it, a sequence is modelled as an output of a ..."
pdf419.pdf,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/hugoz_ecir01.pdf,A paper introducing HMM-based passage models for document classificati...,We present an application of Hidden Markov Models to supervised document classification and ranking. We consider a family of models that take into account the fact that relevant documents may conta...
pdf420.pdf,https://medium.com/@phylypo/nlp-text-segmentation-using-hidden-markov-model-f238743d87eb,#L7.3: HMM for text data,Text Segmentation Using Hidden Markov Model
pdf421.pdf,https://towardsdatascience.com/hidden-markov-models-an-overview-98926404da0e,#L7.2: HMM for sequential data,"There are many tools available for analyzing sequential data. One of the most simple, flexible and time-tested is Hidden Markov Models (HMMs). They were originally developed for signal processing, ..."
pdf422.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,#L7.1: Introduction to HMM,"A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process — call it 
���
 — with unobservable (""hidden"") states. As part of the..."
pdf423.pdf,https://www.youtube.com/watch?v=7zDARfKVm7s,#L7.3: Overview of Forward-Backward Algorithm,No Preview Available
pdf424.pdf,https://www.youtube.com/watch?v=jwYuki9GgJo,#L7.3: Backward algorithm - recursive relation,No Preview Available
pdf425.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,#L6.2: EM algorithm,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, wher..."
pdf426.pdf,https://www.hongliangjie.com/2010/01/04/notes-on-probabilistic-latent-semantic-analysis-plsa/,#L6.3: PLSA equation,Notes on Probabilistic Latent Semantic Analysis (PLSA)
pdf427.pdf,https://github.com/hitalex/PLSA,#L6.1: PLSA EM algorithm implementation,This is a PLSA (Probabilistic Latent Semantic Analysis) implementation via the EM (Expectation-Maximization) algorithm.
pdf428.pdf,https://en.wikipedia.org/wiki/Mixture_model,#L5.2: Mixture model,"In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the..."
pdf429.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,#L5.1:Intro to topic mining,Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection. It can also be thought o...
pdf430.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,Introduction to Markov and Hidden Markov Model with elaborated example...,Markov and Hidden Markov Model
pdf431.pdf,https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86,#L7.3 A medium article about Baum-Welch algorithm for training a Hidde...,Baum-Welch algorithm for training a Hidden Markov Model
pdf432.pdf,https://medium.com/analytics-vidhya/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6,#L7.2 A medium article about Viterbi algorithm for prediction with HMM,Viterbi algorithm for prediction with HMM
pdf433.pdf,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-6-231,A linear memory algorithm for baum-welch algo #L7.3,Baum-Welch training is an expectation-maximisation algorithm for training the emission and transition probabilities of hidden Markov models in a fully automated way. It can be employed as long as a...
pdf434.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,The slides from upenn about the viterbi algorithm #L7.2,No Preview Available
pdf435.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,# The hidden markov model with example #L7.1,"stochastic process is a collection of random variables that are indexed by some mathematical sets. That is, each random variable of the stochastic process is uniquely associated with an element in..."
pdf436.pdf,https://www.baeldung.com/cs/hidden-markov-model,#L7.1 An Introduction to the Hidden Markov Model,An Introduction to the Hidden Markov Model
pdf437.pdf,https://www.codingninjas.com/codestudio/library/baum-welch-algorithm-hmm,Explanation of the Baum Welch Algorithm #L7.3,"Baum Welch Algorithm, an optimization approach, uses both dynamic programming approach and the concept of Expectation-Maximisation (EM) algorithm."
pdf438.pdf,https://www.statisticshowto.com/hidden-markov-model/,Hidden Markov Model: Simple Definition & Overview #L7.2,The Hidden Markov Model (HMM) is a relatively simple way to model sequential data.
pdf439.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,"Hidden Markov Models: Concepts, Examples #L7.1",Hidden Markov models (HMMs) are a type of statistical modeling that has been used for several years.
pdf440.pdf,https://people.csail.mit.edu/rameshvs/content/hmms.pdf,#L7.3 F/B algorithm with HMM,Short review of Hidden Markov Models (HMMs) and the forward/backward algorithm
pdf441.pdf,https://medium.com/analytics-vidhya/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6,#L7.2 Viterbi algorithm explanation with figures,Viterbi algorithm for prediction with HMM
pdf442.pdf,https://web.stanford.edu/~jurafsky/slp3/A.pdf,#L7.1 Book chapter on HMM,Book chapter on HMM
pdf443.pdf,https://faculty.cc.gatech.edu/~zha/CS8803WST/lecture23.pdf,Hidden Markov Models introduced in the Web Search and Text Mining cour...,No Preview Available
pdf444.pdf,https://web.stanford.edu/~jurafsky/slp3/A.pdf,A comprehensive chapter about HMMs #L7.1,No Preview Available
pdf445.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,Markov process and Hidden Markov Model explained with examples #L7.1,No Preview Available
pdf446.pdf,https://courses.grainger.illinois.edu/ECE417/fa2021/lectures/lec15.pdf,A lecture slide on Baum-Welch Algorithm #L7.3,No Preview Available
pdf447.pdf,https://people.cs.umass.edu/~mccallum/courses/inlp2004a/lect10-hmm2.pdf,A clear explanation of Baum-Welch Algorithm #L7.3,No Preview Available
pdf448.pdf,https://www.youtube.com/watch?v=Xh3mf9O5SL0,Viterbi Algorithm pseudo code and explanation with examples #L7.2,No Preview Available
pdf449.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,An example of using Viterbi Algorithm on DNA #L7.2,No Preview Available
pdf450.pdf,https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab,#L7.1 Hidden Markov Models Simplified,Hidden Markov Models (HMMs) are a class of probabilistic graphical model that allow us to predict a sequence of unknown (hidden) variables from a set of observed variables
pdf451.pdf,https://ieeexplore.ieee.org/abstract/document/1165342,#L7.2,An introduction to hidden Markov models
pdf452.pdf,https://www.sciencedirect.com/science/article/pii/S0959440X9680056X,#L7.1,Example of HMM for protein-structure prediction and large-scale genome-sequence analysis
pdf453.pdf,https://en.wikipedia.org/wiki/Viterbi_algorithm,#L7.3 Viterbi algorithm wiki page,No Preview Available
pdf454.pdf,https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html,#L7.2 Markov Model for Text Generation,No Preview Available
pdf455.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,#L7.1 Introduction to Hidden Markov Model,No Preview Available
pdf456.pdf,https://towardsdatascience.com/introduction-to-the-structural-topic-model-stm-34ec4bd5383,Introduction to The Structural Topic Model (STM) #L7.3,No Preview Available
pdf457.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,A brief intro for concepts and examples for HMM #L7.2,No Preview Available
pdf458.pdf,https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf,"A Tutorial on Hidden Markov Models and
Selected Applications in Speec...",No Preview Available
pdf459.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,HMM : Viterbi algorithm - a toy example. Related to Lecture #L7.3,HMM : Viterbi algorithm - a toy example
pdf460.pdf,https://studylib.net/doc/5809964/a-hidden-markov-model-information-retrieval-system,#L7.1 A Hidden Markov Model Information Retrieval System,No Preview Available
pdf461.pdf,https://www.nzdl.org/cgi-bin/library.cgi?e=d-00000-00---off-0cstr--00-0----0-10-0---0---0direct-10---4-------0-1l--11-en-50---20-about---00-0-1-00-0-0-11-1-0utfZz-8-00,new approach to Information Retrieval developed on the basis of Hidden...,No Preview Available
pdf462.pdf,http://www.adeveloperdiary.com/data-science/machine-learning/implement-viterbi-algorithm-in-hidden-markov-model-using-python-and-r/,#L7.3 - Viterbi algorithm implementation in HMM,No Preview Available
pdf463.pdf,https://www.youtube.com/watch?v=hHHP89oyYkA,#L7.2 - Problems with HMM,No Preview Available
pdf464.pdf,https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model,Topics about hidden markov model #L7.1,Hidden Markov Models (HMMs) are generated for all FunFams in CATH superfamilies and are used for predicting functions for uncharacterized sequences.
pdf465.pdf,https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab,#L7.1 - Examples and explanation of HMM as a probabilistic model,No Preview Available
pdf466.pdf,https://machinelearninginterview.com/topics/natural-language-processing/latexpagehow-do-you-generate-text-using-a-hmm-model-like-given-below-pxypxypy/,How do you generate text using a Hidden Markov Model (HMM) ? #L7.2,How do you generate text using a Hidden Markov Model (HMM) ?
pdf467.pdf,https://www.sciencedirect.com/topics/mathematics/viterbi-algorithm,Topics about Viterbi algorithm #L7.3,Viterbi Algorithm
pdf468.pdf,https://medium.com/analytics-vidhya/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6,#L7.3 Viterbi algorithm explained,The purpose of the Viterbi algorithm is to make an inference based on a trained model and some observed data.
pdf469.pdf,https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html,#L7.2 Text generation using Markov chains,"Markov chains have been around for a while now, and they are here to stay. From predictive keyboards to applications in trading and biology, they’ve proven to be versatile tools."
pdf470.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,Explanation of HMM #L7.1,No Preview Available
pdf471.pdf,https://github.com/jason2506/PythonHMM,#L7.3 a python implementation of the Hidden Markov Model.,No Preview Available
pdf472.pdf,https://projecteuclid.org/journals/annals-of-statistics/volume-34/issue-3/Semiparametric-estimation-of-a-two-component-mixture-model/10.1214/009053606000000353.full,Two Component Mixture Language Model for #L2.2,No Preview Available
pdf473.pdf,https://ieeexplore.ieee.org/document/6707701,Mixture Language Model for #L5.1,No Preview Available
pdf474.pdf,https://www.mathworks.com/help/stats/hidden-markov-models-hmm.html,#L7.1 Hidden Markov Models,"A hidden Markov model (HMM) is one in which you observe a sequence of emissions, but do not know the sequence of states the model went through to generate the emissions. Analyses of hidden Markov m..."
pdf475.pdf,https://drive.google.com/file/d/14PHi7CyX21eip9jop_tMdr_511Viv2Uo/view,Viterbi Algorithm #L7.2,Viterbi Algorithm
pdf476.pdf,http://www.cim.mcgill.ca/~latorres/Viterbi/va_alg.htm,#L7.3: Derivation of the Viterbi Algorithm,No Preview Available
pdf477.pdf,https://groups.google.com/g/nltk-users/c/onsdw1zixUA?pli=1,L7.2: Generating Text using HMM,No Preview Available
pdf478.pdf,https://www.ncbi.nlm.nih.gov/CBBresearch/Przytycka/download/lectures/PCB_Lect06_HMM.pdf,L7.2 State and transition probabilities explained,No Preview Available
pdf479.pdf,https://dl.acm.org/doi/10.1145/584792.584855,"L7.1 Capturing Term Dependencies with Mixture Model
Unigram Model cap...",No Preview Available
pdf480.pdf,https://medium.com/analytics-vidhya/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6,#L7.3 Viterbi Algorithm introduction,No Preview Available
pdf481.pdf,https://medium.com/analytics-vidhya/hidden-markov-models-for-dummies-i-4e22df428759,"#L7.2 Hidden Markov Model, simple explanation",No Preview Available
pdf482.pdf,https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf,#L7.1 Hidden Markov Model introduction,No Preview Available
pdf483.pdf,https://people.csail.mit.edu/stephentu/writeups/hmm-baum-welch-derivation.pdf,A general derivation of the Baum-Welch Algorithm. #L7.3,No Preview Available
pdf484.pdf,https://medium.com/@Ayra_Lux/hidden-markov-models-part-2-the-decoding-problem-c628ba474e69,Goes over the Viterbi algorithm for HMM #L7.2,No Preview Available
pdf486.pdf,https://m-clark.github.io/models-by-example/hmm.html,"L#7.1, 7.2, 7.3: Supplemental for learning about HMMs, has code in pyt...",Hidden Markov Model
pdf487.pdf,https://web.stanford.edu/class/cs262/notes/lecture6.pdf,"L#7.1, 7.2, 7.3: Lecture notes on HMMs from Stanford University",Lecture notes on HMMs from Stanford University
pdf488.pdf,https://ai.stanford.edu/~pabbeel/depth_qual/Rabiner_Juang_hmms.pdf,Seminal research that introduces HMMs,"L#7.1, 7.2, 7.3: An Introduction To Hidden Markov Models"
pdf489.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,Example of Viterbi algorithm #L7.2,No Preview Available
pdf490.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,This explains about Hidden Markov Model and examples #L7.1,"Hidden Markov Models: Concepts, Examples"
pdf491.pdf,https://www.davidsbatista.net/assets/documents/posts/2017-11-11-hmm_viterbi_mini_example.pdf,Detailed exercise using Viterbi Algorithm #L7.3,HMM- Viterbi Algorithm with example
pdf492.pdf,https://www.cl.cam.ac.uk/teaching/1718/MLRD/slides/slides9.pdf,#L7.2 A detailed overview of the Viterbi Algorithm that includes a bre...,No Preview Available
pdf493.pdf,https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86,"#L7.3 A helpful overview of the Baum-Welch algorithm, including a brea...","Also known as the forward-backward algorithm, the Baum-Welch algorithm is a dynamic programming approach and a special case of the expectation-maximization algorithm (EM algorithm). Its purpose is ..."
pdf494.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,#L7.1 A helpful in-depth overview of Markov and Hidden Markov models i...,When we can not observe the state themselves but only the result of some probability function(observation) of the states we utilize HMM. HMM is a statistical Markov model in which the system being ...
pdf495.pdf,https://inst.eecs.berkeley.edu/~cs188/sp12/slides/cs188 lecture 17 -- HMMs and particle filters 6PP.pdf,#L7 HMM and Particle Filter (approximate solution for big probability ...,"|X| may be too big to even store B(X)
 E.g. X is continuous"
pdf496.pdf,https://link.springer.com/article/10.1023/A:1007469218079,a paper about multi-scale structure in sequence models #L7.2,"Our model is motivated by the complex multi-scale structure which appears in many natural sequences, particularly in language, handwriting and speech"
pdf497.pdf,https://medium.com/@phylypo/nlp-text-segmentation-using-hidden-markov-model-f238743d87eb,#L7.3,No Preview Available
pdf498.pdf,https://faculty.cc.gatech.edu/~zha/CS8803WST/lecture23.pdf,#L7.2,No Preview Available
pdf499.pdf,https://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf,#L7.1,No Preview Available
pdf500.pdf,http://www.cs.columbia.edu/~mcollins/fb.pdf,This explains about Forward Backward Algorithm #L7.3,The Forward-Backward Algorithm
pdf501.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,This blog gives concrete example with matrix representation and state ...,#7.1 Hidden Markov Model
pdf502.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,Hidden Markov Model explained with example,Hidden Markov Model#L7.1
pdf503.pdf,http://www.adeveloperdiary.com/data-science/machine-learning/derivation-and-implementation-of-baum-welch-algorithm-for-hidden-markov-model/,This explains the derivation and implementation of Baum Welch algorith...,Baum Welch Algorithm for Hidden Markov Model
pdf504.pdf,https://www.freecodecamp.org/news/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc/,Detailed explanation of viterbi algorithm #L7.3,Viterbi algorithm
pdf505.pdf,https://ieeexplore.ieee.org/abstract/document/1450960?casa_token=kOIiKYzHWCsAAAAA:dxlYfkKB6P_eUzq0VwWHerkVUg9uzcbOFss9nxgmhVQblmSVEyhs6H8jX-B7wBGsVeUqy7VdHx4,#L7.3,The viterbi algorithm
pdf506.pdf,https://www.youtube.com/watch?v=56mGTszb_iM,HMM for text generation #L7.1,Markov Chains and Text Generation
pdf507.pdf,https://machinelearninginterview.com/topics/natural-language-processing/latexpagehow-do-you-generate-text-using-a-hmm-model-like-given-below-pxypxypy/,Text generation using HMM #L7.2,generate text using a Hidden Markov Model (HMM)
pdf508.pdf,https://www.exploredatabase.com/2020/04/three-fundamental-problems-of-hmm.html,This explains in detail about three problems of HMM #L7.2,Three fundamental problems of HMM
pdf509.pdf,https://dl.acm.org/doi/10.5555/188490.188593,Passage Retrieval using HMM #L7.1,Document and passage retrieval based on hidden Markov models
pdf510.pdf,https://courses.cs.duke.edu/cps160/compsci260/fall14/resources/HMM/eddy96.pdf,The hidden Markov model was introduced as a new method to develop prot...,Hidden Markov models
pdf511.pdf,https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis,The introduction of Probabilistic latent semantic analysis by Wikipedi...,Probabilistic latent semantic analysis
pdf512.pdf,https://www.mecs-press.org/ijitcs/ijitcs-v14-n2/IJITCS-V14-N2-1.pdf,"#L7.3
A good paper giving more information about HMM","The Hidden Markov Model (HMM) is a mathematical technique representing a substantial and helpful collection
of stochastic processes. It can be identified through the mark of property, meaning that..."
pdf513.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,"#L7.2
Some more explanation of Markov Models","Markov models are named after Andrey Markov, who first developed them in the early 1900s. Markov models are a type of probabilistic model that is used to predict the future state of a system, based..."
pdf514.pdf,https://towardsdatascience.com/natural-language-processing-iii-b067c5d0e89c,"#L7.1
Latent Dirichlet Allocation and Hidden Markov Models","LDA- Latent Dirichlet Allocation
LDA is useful for finding a reasonably accurate variety of topics within a given document. It is handy when you are dealing with topic modeling. Consider a few exa..."
pdf515.pdf,https://ocw.mit.edu/courses/16-410-principles-of-autonomy-and-decision-making-fall-2010/2ebbc8cc4bc9adc3418a572a17331f63_MIT16_410F10_lec21.pdf,MIT Baum-Welch algorithm #L7.3,No Preview Available
pdf516.pdf,https://people.eecs.berkeley.edu/~russell/papers/uai11-viterbi.pdf,A temporally abstracted Viterbi algorithm #7.2,No Preview Available
pdf517.pdf,https://web.archive.org/web/20160722120155/https://ariddell.org/simple-topic-model.html,Simple Unigram Mixture Model #L7.1,No Preview Available
pdf518.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,An example of Viterbi algorithm,An example of Viterbi algorithm#L7.3
pdf519.pdf,http://proceedings.mlr.press/v2/gruber07a/gruber07a.pdf,Paper for Hidden Topic Markov Models,Hidden Topic Markov Models#L7.2
pdf520.pdf,https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model,HMM#L7.1,No Preview Available
pdf521.pdf,https://analyticsindiamag.com/a-guide-to-hidden-markov-model-and-its-applications-in-nlp/,#L7.2,No Preview Available
pdf522.pdf,https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86,This contains information about Baum-Welch algorithm discussed in lect...,Baum-Welch algorithm
pdf523.pdf,https://web.archive.org/web/20160722120155/https://ariddell.org/simple-topic-model.html,#L7.1,No Preview Available
pdf524.pdf,https://www.cl.cam.ac.uk/teaching/1718/MLRD/slides/slides9.pdf,This contains more information about the Viterbi algorithm discussed i...,No Preview Available
pdf525.pdf,http://www.super.tka4.org/materials/lib/Articles-Books/Speech Recognition/hmm-tutorial.pdf,"This paper describe the HMM and its evaluation,decoding,learning and i...",Hidden Markov Models
pdf526.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,This article contains details about HMM discussed in lecture #L7.1,Hidden Markov Models
pdf527.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,"Hidden Markov Model
Elaborated with examples","Hidden Markov Model
Elaborated with examples"
pdf528.pdf,https://www.youtube.com/watch?v=RWkHJnFj5rY,Video on HMM,No Preview Available
pdf529.pdf,https://dl.acm.org/doi/pdf/10.1145/312624.312680,"A Hidden Markov Model
Information Retrieval System","A Hidden Markov Model
Information Retrieval System"
pdf530.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,#L7.3 Viterbi Algorithm,No Preview Available
pdf531.pdf,https://stephentu.github.io/writeups/hmm-baum-welch-derivation.pdf,Baum-welch#L7.3,No Preview Available
pdf532.pdf,https://arxiv.org/abs/2102.07284,#L7.2 A paper on HMM for Classification,No Preview Available
pdf533.pdf,https://web.stanford.edu/~jurafsky/slp3/A.pdf,Chapter on HMMs which goes into HMMs and Viterbi algorithm in detail #...,Hidden Markov Models
pdf534.pdf,https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model,Intro to HMMs in #L7.1,Hidden Markov models
pdf535.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,#L7.1 Introduction to HMM,No Preview Available
pdf536.pdf,https://stephentu.github.io/writeups/hmm-baum-welch-derivation.pdf,Baum-welch#L7.3,No Preview Available
pdf537.pdf,https://journals.sagepub.com/doi/pdf/10.1177/1550147718772541,This paper proposed that a 2nd-order HMM based on probabilistic log-Vi...,"Log-Viterbi algorithm applied on
second-order hidden Markov model for
human activity recognition"
pdf538.pdf,http://www.cim.mcgill.ca/~latorres/Viterbi/va_alg.htm,Viterbi#L7.2,No Preview Available
pdf539.pdf,https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86,#L7.3,No Preview Available
pdf540.pdf,https://people.cs.umass.edu/~miyyer/cs585/lectures/11-hmms.pdf,Sequence modeling: part-of-speech tagging with hidden Markov models #L...,"sequence modeling:
part-of-speech tagging with
hidden Markov models"
pdf541.pdf,https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab,Simplified Hidden Markov Models #7.3,Hidden Markov Models
pdf542.pdf,https://en.wikipedia.org/wiki/Viterbi_algorithm,The Viterbi algorithm in #L7.3,The Viterbi algorithm
pdf543.pdf,https://en.wikipedia.org/wiki/Viterbi_algorithm,Viterbi algorithm covered in #L7.3,Viterbi algorithm
pdf544.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,Explanation #7.2,Markov and Hidden Markov Model
pdf545.pdf,https://www.nature.com/articles/nbt1004-1315,A paper that explains HMM covered in #L7.2,What is a hidden Markov model?
pdf546.pdf,https://aritter.github.io/CS-7650-sp23/slides/lec4-seq1.pdf,"A very detailed set of slides covering HMMs for POS tagging, parameter...",Lecture 4: Sequence Models I
pdf547.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,Hidden Markov model wikipedia #7.1,Hidden Markov model
pdf548.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,HMM covered in #L7.1,Hidden Markov model
pdf549.pdf,https://www.tu-chemnitz.de/physik/KSND/abb/node55.html,hmm as a probabilistic model in #L7.2,"HMM is a probabilistic pattern matching technique, in which the observations are considered to be the outputs of a stochastic process and consists of an underlying Markov chain."
pdf550.pdf,https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86,An explanation of Baum-Welch algorithm for training a Hidden Markov Mo...,Baum-Welch algorithm for training a Hidden Markov Model
pdf551.pdf,https://medium.com/analytics-vidhya/hidden-markov-models-for-dummies-i-4e22df428759,Hidden Markov Models explained in a very simple way for beginners. #L7...,No Preview Available
pdf552.pdf,https://www.cl.cam.ac.uk/teaching/1617/MLRD/slides/slides9.pdf,Describes how dynamic programming works in the viterbi algorithm,No Preview Available
pdf553.pdf,https://people.eecs.berkeley.edu/~russell/papers/uai11-viterbi.pdf,Viterbi Algorithm,No Preview Available
pdf554.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,Hidden Markov Model #7.1,No Preview Available
pdf555.pdf,https://www.sciencedirect.com/topics/mathematics/viterbi-algorithm,An explanation of Viterbi Algorithm covered in Lecture #L7.2,A maximum-likelihood detector searches over all possible input sequences using an efficient recursive algorithm known as the Viterbi algorithm
pdf556.pdf,https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model,An explanation of Hidden Markov Model covered in Lecture #L7.1,A hidden Markov model (HMM) is a probabilistic graphical model that is commonly used in statistical pattern recognition and classification.
pdf557.pdf,https://en.wikipedia.org/wiki/Baum–Welch_algorithm,"#L7.3: The Wikipedia page of the topic ""Baum-Welch Algorithm"" covered ...",Baum–Welch algorithm
pdf558.pdf,https://en.wikipedia.org/wiki/Viterbi_algorithm,"#L7.2: The Wikipedia page of the topic ""Viterbi Algorithm"" covered in ...",Viterbi algorithm
pdf559.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,"#L7.1: The Wikipedia page of the topic ""Hidden Markov Model"" covered i...",Hidden Markov model
pdf560.pdf,https://research.aimultiple.com/large-language-model-training/,"L8. This week is about NLP. Nowadays, LLM is the most popular approach...",No Preview Available
pdf561.pdf,https://ieeexplore.ieee.org/abstract/document/266451?casa_token=lvK8ruVt1mMAAAAA:xgMrfLxA25FPfupfFp7tluAq2JGqJofI_tatnQOIp5zbupJ0swW_mMyZLd4TlJBMwX6LROC5JQ,#7.3 HMM clustering for connected word recognition,No Preview Available
pdf562.pdf,https://www.researchgate.net/publication/327849802_The_Baum-Welch_algorithm_with_limiting_distribution_constraints,#L7.3 This paper talks about an extension of the Baulm Welch algorithm...,"We present a method of incorporating limiting distribution information in the Baum–Welch algorithm for estimating parameters of discrete-time, finite-state, Hidden Markov Models. We find that havin..."
pdf563.pdf,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=,#7.2 Rabiners HMM,No Preview Available
pdf564.pdf,https://dl.acm.org/doi/pdf/10.1145/312624.312680,#7.1 HMM for Information Retrieval,No Preview Available
pdf565.pdf,https://onlinelibrary.wiley.com/doi/10.1111/j.1756-8765.2009.01074.x,#L7.2 A paper that talks about the novel use of HMM in semantic repres...,"In this paper, we describe a model that learns semantic representations from the distributional statistics of language. This model, however, goes beyond the common bag-of-words paradigm, and infers..."
pdf566.pdf,https://medium.com/@rmwkwok/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86,Baum-Welch algorithm for training a Hidden Markov Model,Baum-Welch algorithm for training a Hidden Markov Model
pdf567.pdf,https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html,#L7.1 An article discussing the real-world applications of HMM for tex...,"Here are some Markov Chains industry applications:

Text Generation (you’re here for this).
Financial modelling and forecasting (including trading algorithms).
Logistics: modelling future deliv..."
pdf568.pdf,https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model,A great introduction and explanation to HMMs. Discussed in lectures #L...,"Hidden Markov models (HMMs), named after the Russian mathematician Andrey Andreyevich Markov, who developed much of relevant statistical theory, are introduced and studied in the early 1970s. They ..."
pdf569.pdf,https://medium.com/analytics-vidhya/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6,Viterbi algorithm for prediction with HMM,Viterbi algorithm for prediction with HMM
pdf570.pdf,https://towardsdatascience.com/text-generation-with-markov-chains-an-introduction-to-using-markovify-742e6680dc33,#L7.1 Text Generation Using HMM,No Preview Available
pdf571.pdf,https://www.hongliangjie.com/2010/01/04/notes-on-probabilistic-latent-semantic-analysis-plsa/,#L6.2,Probabilistic Latent Semantic Analysis (PLSA)
pdf572.pdf,http://www.hcbravo.org/dscert-algo/homeworks/topic_modeling.html,L6.1,Probabilistic Latent Semantic Analysis with the EM Algorithm
pdf573.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,#L6.1,Expectation–maximization algorithm
pdf574.pdf,https://timeseriesreasoning.com/contents/hidden-markov-models/,#L7.3,Hidden Markov Models for Time Series Analysis
pdf575.pdf,https://deepai.org/machine-learning-glossary-and-terms/hidden-markov-model,#L7.1,What is a Hidden Markov Model?
pdf576.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,Examples for HMM #L7.3,"Hidden Markov Models: Concepts, Examples"
pdf577.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,Introduction for Markov and Hidden Markov Model#L7.2,Markov and Hidden Markov Model
pdf578.pdf,https://medium.com/@Ayra_Lux/hidden-markov-models-part-1-the-likelihood-problem-8dd1066a784e,#L7.1,Hidden Markov Models
pdf579.pdf,https://analyticsindiamag.com/a-guide-to-hidden-markov-model-and-its-applications-in-nlp/,HMM and its applications in NLP #L7.3,"A Hidden Markov Model (HMM) is a statistical model which is also used in machine learning. It can be used to describe the evolution of observable events that depend on internal factors, which are n..."
pdf580.pdf,https://d1wqtxts1xzle7.cloudfront.net/31037917/Rabiner1986_An_Introduction_to_Hidden_Markov_Models-libre.pdf?1392213829=,L7.1 Paper about HMM with detailed explanations.,"It is the purpose of this tutorial paper to 
give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech 
recognition."
pdf581.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,HMM: Viterbi Algorithm with example #L7.3,No Preview Available
pdf582.pdf,https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab,Hidden Markov Models Simplified #L7.2,No Preview Available
pdf583.pdf,https://web.archive.org/web/20160722120155/https://ariddell.org/simple-topic-model.html,A Simple Topic Model (Mixture of Unigram Models) #L7.1,No Preview Available
pdf584.pdf,https://www.researchgate.net/publication/2316139_A_Hidden_Markov_Model_Information_Retrieval_System,HMM in information retrival,No Preview Available
pdf585.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,General Introduction to HMM,No Preview Available
pdf586.pdf,https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model,Lec 7. Introductio to Hidden Markov Model,No Preview Available
pdf587.pdf,https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model,Lec 7. Introduction yo Hidden Markov Model,Hidden Markov Model
pdf588.pdf,https://www.codingninjas.com/codestudio/library/baum-welch-algorithm-hmm,#L7.3 Baum Welch Algorithm,No Preview Available
pdf589.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,#L7.2 HMM Real World Examples and Detailed Description,No Preview Available
pdf590.pdf,https://aclanthology.org/2020.acl-main.454.pdf,it is a work taking advantage of doc-query techniques for faithfulness...,"FEQA: A Question Answering Evaluation Framework for Faithfulness
Assessment in Abstractive Summarization"
pdf591.pdf,https://arxiv.org/pdf/1910.13461.pdf,an impactful work based on BERT #L8.2,"BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension"
pdf592.pdf,https://arxiv.org/pdf/1910.13461.pdf,an impactful work based on BERT,"BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension"
pdf593.pdf,https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture01-wordvecs1.pdf,A very good introduction provide by stanford on NLP #L8.1,Representing words by their context
pdf594.pdf,https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture01-wordvecs1.pdf,a very good introduction course provided by stanford on NLP,Representing words by their context
pdf595.pdf,https://analyticsindiamag.com/a-guide-to-hidden-markov-model-and-its-applications-in-nlp/,#L7.2 HMM in NLP,No Preview Available
pdf596.pdf,https://pdxscholar.library.pdx.edu/cgi/viewcontent.cgi?article=2153,"Empirical Analysis of CBOW and Skip Gram NLP
Models #L8.1",No Preview Available
pdf597.pdf,http://www.adeveloperdiary.com/data-science/machine-learning/forward-and-backward-algorithm-in-hidden-markov-model/,#L7.3 Forward Backward algorithm,No Preview Available
pdf598.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,#L7.2 Examples for HMMs,No Preview Available
pdf599.pdf,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/hugoz_ecir01.pdf,#L7.1 Passage Modeling,No Preview Available
pdf600.pdf,https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-1/,A blog that introduced the pre-trained transformer language model for ...,"Since BERT was first applied to search and document ranking, we at the Vespa team have been busy making it easy to use BERT or Transformer models in general, for ranking and question answering with..."
pdf601.pdf,https://www.geeksforgeeks.org/overview-of-word-embedding-using-embeddings-from-language-models-elmo/,A supplementary for ELMo and implementations with python code. Easy to...,Overview of Word Embedding using Embeddings from Language Models (ELMo)
pdf602.pdf,https://towardsdatascience.com/neural-language-models-32bec14d01dc,A clear introduction to the Neural Language Models with basic RNN and ...,Neural Language Models
pdf603.pdf,https://iulg.sitehost.iu.edu/moss/hmmcalculations.pdf,If you are looking for examples with detailed steps.,#L7.3 Example of the Baum-Welch Algorithm
pdf604.pdf,https://people.csail.mit.edu/rameshvs/content/hmms.pdf,If you are more of a learning through examples person. This note goes ...,#L7.2 Understand Forward-Backward algorithm through an example.
pdf605.pdf,https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf,"This example model  is composed of 2 states, H (high GC content) and L...",#L7.1 An example of Viterbi algorithm to predict regions of coding DNA sequence
pdf606.pdf,https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/,Deep Contextualized Word Representations with ELMo #L8.2,Deep Contextualized Word Representations with ELMo
pdf607.pdf,https://sease.io/2021/12/using-bert-to-improve-search-relevance.html,From Training to Ranking: Using BERT to Improve Search Relevance #L8.3,From Training to Ranking: Using BERT to Improve Search Relevance
pdf608.pdf,https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/,Agglomerative Hierarchical Clustering #L8.1,The agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It’s also known as AGNES (Agglomerative Nesting). The al...
pdf609.pdf,https://www.freecodecamp.org/news/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc/,"#L7.3 Markov Chain Model, HMM and Viterbi algorithm",medium.com/free-code-camp/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24
pdf610.pdf,https://arxiv.org/pdf/2009.05451.pdf,A Comparison of LSTM and BERT for Small Corpus #L8.2,"Keywords: BERT, LSTM, intent classification, chatbot, dialogue systems, dialogue act classification"
pdf611.pdf,https://arxiv.org/abs/1910.14424,MonoBERT #L8.3,Multi-Stage Document Ranking with BERT
pdf612.pdf,https://huggingface.co/blog/bert-101,BERT explained #L8.2,BERT Model Explained
pdf614.pdf,https://aclanthology.org/2021.naacl-tutorials.1/,#L8.3,No Preview Available
pdf615.pdf,https://dl.acm.org/doi/full/10.1145/3560815,#L8.2,"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
pdf616.pdf,https://aclanthology.org/J92-4003.pdf,#L8.1,No Preview Available
pdf617.pdf,http://www.adeveloperdiary.com/data-science/machine-learning/forward-and-backward-algorithm-in-hidden-markov-model/,#L7.3 A detailed explanation of forward-backward algorithm in HMM,No Preview Available
pdf618.pdf,http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_6/HMM_problems.htm,#L7.2 Talks about the three fundamental problems of HMM,No Preview Available
pdf619.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,#L7.1 This page is about Hidden Markov Models,No Preview Available
pdf620.pdf,https://d1wqtxts1xzle7.cloudfront.net/31279335/___Recurrent_Neural_Networks_Design_And_Applicatio(BookFi.org)-libre.pdf?1390940527=,#L8.2 Book on RNNs from 2001,"RECURRENT
NEURAL
NETWORKS"
pdf621.pdf,https://en.wikipedia.org/wiki/Word_embedding,#L8.1 Wikipedia on word embedding,"Toggle the table of contents
Word embedding"
pdf622.pdf,https://github.com/jingtaozhan/RepCONC,"#L8.3 the official repo for our WSDM'22 paper, Learning Discrete Repre...",No Preview Available
pdf623.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,#L8.1 the word embedding approach for representing text data,No Preview Available
pdf624.pdf,https://medium.com/analytics-vidhya/rnns-lstms-cnns-transformers-and-bert-be003df3492b,"RNNs, LSTMs, CNNs, Transformers and BERT 
#L8.2",No Preview Available
pdf625.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,the word embedding approach for representing text data,No Preview Available
pdf626.pdf,https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c,BERT Explained: A Complete Guide with Theory and Tutorial #L8.3,No Preview Available
pdf627.pdf,https://medium.com/jumio/tutorial-unsupervised-ranking-using-machine-learning-b45d911b1af7,#L8.3 - Good tutorial on unsupervised ranking with ML with code and ex...,No Preview Available
pdf628.pdf,https://medium.com/@rachel_95942/language-models-and-rnn-c516fab9545b,#L8.2 - Language models and RNNs,No Preview Available
pdf629.pdf,https://www.youtube.com/watch?v=a0dWsRNE5mo,#L8.1 - Good explanation for Brown clustering,No Preview Available
pdf630.pdf,https://www.engr.uvic.ca/~seng474/svd.pdf,"A brief introduction to SVD and LSA, related to matrix factorization m...",No Preview Available
pdf631.pdf,https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c,A great tutorial that explain the architecture of skip-gram model(disc...,As the vocabulary of any language is large and cannot be labeled by human and hence we require unsupervised learning techniques that can learn the context of any word on its own. Skip-gram is one o...
pdf632.pdf,https://towardsdatascience.com/a-word2vec-implementation-using-numpy-and-python-d256cf0e5f28,Implementing word2vec using Python #L8.1,No Preview Available
pdf633.pdf,https://arxiv.org/abs/1910.14424,#L8.3 Multi-stage document ranking,No Preview Available
pdf634.pdf,https://arxiv.org/abs/1802.05365v2,#L8.2 ELMO,No Preview Available
pdf635.pdf,https://towardsdatascience.com/machine-learning-algorithms-part-12-hierarchical-agglomerative-clustering-example-in-python-1e18e0075019,#L8.1 Hierarchical Agglomerative Clustering,No Preview Available
pdf636.pdf,https://arxiv.org/abs/2010.06467,The paper introduced in Lecture #L8.3,Pretrained Transformers for Text Ranking: BERT and Beyond
pdf637.pdf,https://en.wikipedia.org/wiki/BERT_(language_model),Brief introduction of BERT. Related to Lecture #L8.2,BERT
pdf638.pdf,https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c,Skip-Gram: NLP context words prediction algorithm. Related to Lecture ...,Skip-Gram: NLP context words prediction algorithm
pdf639.pdf,https://arxiv.org/abs/1810.04805,#L8.3 BERT paper,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
pdf640.pdf,https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet-sentiment-analysis-ced697948c47,#L8.2 LSTM vs BERT,LSTM vs BERT
pdf642.pdf,https://aclanthology.org/2021.naacl-main.334.pdf,A paper introducing a language model (SapBert) pertained on both large...,"We propose SAPBERT, a pretraining scheme that selfaligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection..."
pdf643.pdf,https://arxiv.org/pdf/1901.04085.pdf,A great paper about how to train BERT(discussed in #L10.3) for passage...,"Recently, neural models pretrained on a language modeling task, such as
ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive resu..."
pdf644.pdf,https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577,A great tutorial explaining the vanishing problem in RNN model and how...,"The presence of the forget gate’s activations allows the LSTM to decide, at each time step, that certain information should not be forgotten and to update the model’s parameters accordingly."
pdf645.pdf,https://arxiv.org/pdf/1910.14424.pdf,"The paper on multi-stage re-ranking with monoBERT and duoBERT, mention...",No Preview Available
pdf646.pdf,https://aclanthology.org/N19-1423.pdf,"The original paper on BERT, related to #L8.2",No Preview Available
pdf647.pdf,https://aclanthology.org/D14-1162.pdf,"The original paper on GloVe embeddings, related to #L8.1",No Preview Available
pdf648.pdf,https://aclanthology.org/N18-1202.pdf,"The original paper on ELMo, related to #L8.2",No Preview Available
pdf649.pdf,https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf,The paper that explains how word embeddings are related to matrix fact...,No Preview Available
pdf650.pdf,https://towardsdatascience.com/word2vec-explained-49c52b4ccb71,#L8.1 word2vec introduction and implementation,No Preview Available
pdf651.pdf,https://arxiv.org/pdf/1904.07094.pdf,This is a paper which explains about Contextualized Embeddings for Doc...,Contextualized Embeddings for Document Ranking
pdf652.pdf,https://ciir-publications.cs.umass.edu/getpdf.php?id=1407,#L8.3 A deep look into NN for ranking,No Preview Available
pdf653.pdf,https://neerajku.medium.com/document-ranking-using-bert-a4b00eb258c4,Using BERT to rank documents #L8.3,No Preview Available
pdf654.pdf,https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,This gives a detailed explanation of BERT #L8.2,How BERT works
pdf655.pdf,https://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/,Goes over the ELMo model #L8.2,ELMo: Deep contextualized word representations
pdf656.pdf,https://medium.com/@sue_nlp/explaining-rnnlm-in-natural-language-processing-nlp-in-depth-and-in-an-easy-to-understand-way-91acb1df2148,This explains about RNN Language Model #L8.2,RNNLM explained
pdf657.pdf,https://medium.datadriveninvestor.com/skip-gram-model-broken-down-subsampling-n-grams-feab04a6f220?gi=9a7ac100ce1f,Skip-gram model explained #L8.1,No Preview Available
pdf658.pdf,https://web.stanford.edu/~jurafsky/slp3/7.pdf,This explains about Neural Language Models #L8.1,Neural Language Models
pdf659.pdf,https://www.datanovia.com/en/lessons/agglomerative-hierarchical-clustering/,This explains about Agglomerative Clustering #L8.1,Agglomerative Hierarchical Clustering
pdf660.pdf,https://arxiv.org/abs/2010.06467,#L8.3 Arxiv paper of Pretrained Transformers for Text Ranking: BERT an...,Pretrained Transformers for Text Ranking: BERT and Beyond
pdf661.pdf,https://medium.com/analytics-vidhya/rnns-lstms-cnns-transformers-and-bert-be003df3492b,#L8.2 A medium article that introduce common Neural Language Models,"RNNs, LSTMs, CNNs, Transformers and BERT"
pdf662.pdf,https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b,#L8.1 A medium article about skip-gram,Word2Vec (skip-gram model): PART 1 - Intuition.
pdf663.pdf,https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf,GPT is another model that uses pre-training and fine-tuning. It's base...,No Preview Available
pdf664.pdf,https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,BERT Explained: State of the art language model for NLP #L8.3,BERT Explained: State of the art language model for NLP
pdf665.pdf,https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/,A Step-by-Step NLP Guide to Learn ELMo for Extracting Features from Te...,A Step-by-Step NLP Guide to Learn ELMo for Extracting Features from Text
pdf666.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,What Are Word Embeddings for Text? #L8.1,What Are Word Embeddings for Text?
pdf667.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,What are word embeddings in text? #L10.1,What are word embeddings in text?
pdf668.pdf,https://neerajku.medium.com/document-ranking-using-bert-a4b00eb258c4,#L8.3 Document ranking using BERT,No Preview Available
pdf669.pdf,https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,#L8.2 BERT model detailed explanation,No Preview Available
pdf670.pdf,https://arxiv.org/abs/1908.10084,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks #L8.3,No Preview Available
pdf671.pdf,https://www.tensorflow.org/text/guide/word_embeddings,Word embeddings TF #L8.1,No Preview Available
pdf672.pdf,https://arxiv.org/abs/1809.09795,Deep contextualized word representations for detecting sarcasm and iro...,No Preview Available
pdf673.pdf,https://medium.com/@merfaruktuna/introduction-to-language-modelling-and-deep-neural-network-based-text-generation-2bdfb1ab5088,Good article about neural network language models - the explanation is...,No Preview Available
pdf674.pdf,https://www.youtube.com/watch?v=NL3U8QOGBaE,This video explains the implementation of brown clustering for unsuper...,No Preview Available
pdf675.pdf,https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,Explanation of BERT #L8.3,BERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language.
pdf676.pdf,https://builtin.com/data-science/recurrent-neural-networks-powerhouse-language-modeling,Basic explanation of RNN Language Model #L8.2,Recurrent Neural Networks: The Powerhouse of Language Modeling
pdf677.pdf,https://medium.com/@rachel_95942/language-models-and-rnn-c516fab9545b,Explanation about RNN Language Model #L8.2,"Just like an n-gram Language Model, you can use an RNN Language Model to generate text by repeated sampling."
pdf678.pdf,https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c,Explanation of Skip-gram #L8.1,Skip-gram is used to predict the context word for a given target word. It’s reverse of CBOW algorithm.
pdf679.pdf,https://www.baeldung.com/cs/word-embeddings-cbow-vs-skip-gram,Word Embeddings: CBOW vs Skip-Gram #L8.1,"Skip-Gram works well with small datasets, and can better represent less frequent words. CBOW is found to train faster than Skip-Gram, and can better represent more frequent words."
pdf680.pdf,https://www.quora.com/What-are-the-advantages-and-disadvantages-of-Word2vec-and-GloVe,The advantages/disadvantages of word2vec and GloVe #8.1,No Preview Available
pdf681.pdf,https://towardsdatascience.com/from-word-embeddings-to-pretrained-language-models-a-new-age-in-nlp-part-1-7ed0c7f3dfc5,This data science article gives a great intro to NLP word embeddings w...,"From Word Embeddings to Pretrained Language Models — A New Age in NLP — Part 1
For words to be processed by machine learning models, they need some form of numeric representation that models can u..."
pdf682.pdf,https://arxiv.org/abs/1903.06902,#L8.3 Neural Ranking Models,"A Deep Look into Neural Ranking Models for Information Retrieval
Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W. Bruce Croft, Xueqi Cheng
Ranking models lie a..."
pdf683.pdf,https://medium.com/analytics-vidhya/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6,A good article regarding the Viterbi algorithm #L7.3,Viterbi algorithm for prediction with HMM
pdf684.pdf,https://towardsdatascience.com/probability-learning-vi-hidden-markov-models-fab5c1f0a31d,This article covers about using the Hidden Markov Model as Probabilist...,Probability Learning VI: Hidden Markov Models
pdf685.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,An overview of Hidden Markov Model #L7.1,Markov and Hidden Markov Model
pdf686.pdf,https://analyticsindiamag.com/how-to-code-your-first-lstm-network-in-keras/,A cool tutorial to build a LSTM on Keras #L8.1,"The simplest application of RNN is in Natural Language Processing. In all natural languages, the order of the words is important to convey the meaning in the right context."
pdf687.pdf,https://link.springer.com/article/10.1007/s10791-021-09398-0,#L8.3 Neural ranking models for IR,Neural ranking models for document retrieval
pdf688.pdf,https://medium.com/analytics-vidhya/why-are-lstms-struggling-to-matchup-with-transformers-a1cc5b2557e3,#L8.2 Compare LSTM with the cutting-edge Transformer,Why are LSTMs struggling to matchup with Transformers?
pdf689.pdf,https://arxiv.org/pdf/1901.09069.pdf,#L8.1 A survey on word embeddings,Word Embeddings: A Survey
pdf690.pdf,https://medium.com/nwamaka-imasogie/neural-networks-word-embeddings-8ec8b3845b2e,#L8.1: Word embeddings,No Preview Available
pdf691.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,An article describing about the word embedding approach for representi...,What Are Word Embeddings for Text
pdf692.pdf,https://aclanthology.org/2021.naacl-tutorials.1.pdf,Pretrained Transformers for Text Ranking: BERT and Beyond #L8.3,No Preview Available
pdf693.pdf,https://towardsdatascience.com/glove-elmo-bert-9dbbc9226934,A guide to state-of-the-art text classification using Spark NLP #L8.2,No Preview Available
pdf694.pdf,https://www.mygreatlearning.com/blog/word-embedding/,What is Word Embedding | Word2Vec | GloVe #L8.1,No Preview Available
pdf695.pdf,https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c,#L8.1 Skip Gram,Skip-Gram: NLP context words prediction algorithm
pdf696.pdf,https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23,"Introduction to BERT variants: ALBERT, RoBERTa, ELECTRA #8.3",No Preview Available
pdf697.pdf,https://d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html,A tutorial of fine-tuning BERT for sequence-level or token-level appli...,No Preview Available
pdf698.pdf,https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-1/,This blog gives a great intro to text ranking using BERT described in ...,"Since BERT was first applied to search and document ranking, we at the Vespa team have been busy making it easy to use BERT or Transformer models in general, for ranking and question answering with..."
pdf699.pdf,https://jalammar.github.io/illustrated-bert/,This github article by Jay Alammar gives a very detailed explanation o...,"The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)"
pdf700.pdf,https://aclanthology.org/2021.naacl-tutorials.1.pdf,#L8.3,No Preview Available
pdf701.pdf,https://arindum-yaa-mondal.medium.com/elmo-can-do-better-information-retrieval-rather-than-traditional-static-word-embedding-df4a6482a816,#L8.2,No Preview Available
pdf702.pdf,http://cs.unibo.it/~danilo.montesi/CBD/Articoli/2015_A Word Embedding based Generalized Language Model for Information Retrieval.pdf,#L8.1,No Preview Available
pdf703.pdf,https://ieeexplore.ieee.org/abstract/document/1517930?casa_token=P-wHlOOfOwUAAAAA:Clhjyx0Zu7OfI-BhPCUN611LGzskBE1WLscbjHAiCG4sSGPJvHDZWhEpVtl2MhEYw9Cy_1GpHA,#8.3 Neural networks for ranking web pages,No Preview Available
pdf704.pdf,https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html,Research paper from Google Research about the Transformer Language Mod...,Transformer: A Novel Neural Network Architecture for Language Understanding
pdf705.pdf,https://arxiv.org/abs/1511.08630,#8.2 A C-LSTM Neural Network for Text Classification,No Preview Available
pdf706.pdf,https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,Bert Language Model #L8.2,BERT Explained: State of the art language model for NLP
pdf707.pdf,https://ieeexplore.ieee.org/abstract/document/8258123?casa_token=6fGQPbn52aYAAAAA:5JOuQ7o5fkdJwRIrzTAruOpZ_MahpGtlWIe22_gEmGX4js6i_Ww3j71tbbB7amIa4WodFAbSKg,#8.1 Improving text classification using word embedding,No Preview Available
pdf708.pdf,https://direct.mit.edu/coli/article/49/1/253/113643/Pretrained-Transformers-for-Text-Ranking-BERT-and?searchresult=1,#L8.3: Text ranking,No Preview Available
pdf709.pdf,https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/,#L8.2 LSTM,No Preview Available
pdf711.pdf,https://colah.github.io/posts/2015-08-Understanding-LSTMs/,"#L8.2
Problems about long-term dependencies","Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are tryin..."
pdf713.pdf,https://cran.r-project.org/web/packages/textrank/vignettes/textrank.html,"#L8.3
A text ranking method for summarizing text",Textrank for summarizing text
pdf714.pdf,http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/,#L8.1,No Preview Available
pdf715.pdf,https://huggingface.co/docs/transformers/model_doc/bert,#L8.2 An article from Hugging Face that presents various resources to ...,"The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It’s a bidirectio..."
pdf716.pdf,http://www.seas.ucla.edu/spapl/weichu/htkbook/node220_mn.html,#L8.1 A short article introducing the main concepts of class based lan...,"As described in section 14.1.3, a class-base language model consists of two separate components"
pdf717.pdf,https://cran.r-project.org/web/packages/textrank/vignettes/textrank.html,#L8.3,Textrank for summarizing text
pdf718.pdf,https://towardsdatascience.com/glove-elmo-bert-9dbbc9226934,#L8.2,ELMo & BERT
pdf719.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,#L8.1,Word Embedding Algorithms
pdf720.pdf,https://www.tensorflow.org/text/tutorials/classify_text_with_bert,A complete tutorial on BERT and using it to perform sentiment analysis...,No Preview Available
pdf721.pdf,https://ignite.apache.org/docs/latest/machine-learning/binary-classification/ann,ANN (Approximate Nearest Neighbor) in #L8.3,ANN (Approximate Nearest Neighbor)
pdf723.pdf,https://en.wikipedia.org/wiki/Brown_clustering,#L8.1 content brown clustering wiki,Brown clustering
pdf724.pdf,https://direct.mit.edu/coli/article/49/1/253/113643/Pretrained-Transformers-for-Text-Ranking-BERT-and?searchresult=1,Pretrained Transformers for Text Ranking: BERT and Beyond,Pretrained Transformers for Text Ranking: BERT and Beyond
pdf725.pdf,https://arxiv.org/abs/2301.11696,Neural Network for Text Classification,Neural Network for Text Classification
pdf726.pdf,https://arxiv.org/pdf/2110.01804.pdf,A Survey On Neural Word Embeddings,No Preview Available
pdf727.pdf,https://www.atlantis-press.com/journals/jsta/125941349/view,#L6.3 A paper that explores PLSA to be used as a technique in SVD,The Probabilistic Latent Semantic Analysis has been related with the Singular Value Decomposition. Several problems occur when this comparative is done. Data class restrictions and the existence of...
pdf728.pdf,https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa,"#L8.1
Word embedding and word2vec","This looks like multiple-context CBOW model just got flipped. To some extent that is true.

We input the target word into the network. The model outputs C probability distributions. What does thi..."
pdf729.pdf,https://github.com/AdeDZY/DeepCT,#L8.3 A Github repository regarding the implementation of DeepCT and H...,"arXiv paper ""Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval"" arXiv, 2019
The WebConf2020 paper ""Context-Aware Document Term Weighting for Ad-Hoc Search"" pdf, 2..."
pdf730.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html,Language models for information retrieval #L8.1,No Preview Available
pdf731.pdf,https://web.stanford.edu/~jurafsky/slp3/11.pdf,Covers BERT and related topics #L8.2,Fine-Tuning and Masked Language Models
pdf732.pdf,https://www.youtube.com/watch?v=M2T36PlRv1U&list=PLEAYkSg4uSQ1r-2XrJ_GBzzS6I-f8yfRU&index=75,Lectures 10.1 to 10.10 in the playlist provide a detailed explanation ...,Deep Learning(CS7015): Lec 10.1 One-hot representations of words
pdf733.pdf,https://medium.com/@kashyapkathrani/all-about-embeddings-829c8ff0bf5b,#L8.1 This article covers a variety of word embeddings with specific e...,"Both the techniques of word embedding have given a decent result, but the problem is the approach is not accurate enough. As, they don’t take into consideration the order of words in which they app..."
pdf734.pdf,https://arxiv.org/pdf/2010.06467.pdf,#L8.3 The paper discussed in the tutorial/lecture.,No Preview Available
pdf735.pdf,https://huggingface.co/bert-base-uncased,"#L8.2 The Hugging Face page for BERT (uncased text, base model), which...","BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (..."
pdf736.pdf,https://medium.com/jumio/tutorial-unsupervised-ranking-using-machine-learning-b45d911b1af7,"#L8.3: Short explanation and python code example of BM25, Bi-encoder, ...",No Preview Available
pdf737.pdf,https://www.projectpro.io/article/bert-nlp-model-explained/558,#L8.2: Detailed discussion on BERT,BERT NLP Model Explained for Complete Beginners
pdf738.pdf,https://www.youtube.com/watch?v=UqRCEmrv1gQ,"#L8.1: Short overview of Word2Vec, Skipgram, CBOW",Word2Vec - Skipgram and CBOW
pdf739.pdf,https://github.com/castorini/docTTTTTquery,"A repo about docTTTTTquery, a model that uses T5 as the expansion mode...",No Preview Available
pdf740.pdf,https://paperswithcode.com/method/elmo,The ELMo papers and codes #L8.2,"Embeddings from Language Models, or ELMo, is a type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how the..."
pdf741.pdf,https://people.cs.umass.edu/~miyyer/cs585/lectures/06-neural-lms.pdf,The slides about the nlm #L8.1,No Preview Available
pdf742.pdf,https://medium.com/syncedreview/google-introduces-rankt5-a-fine-tuned-t5-model-that-boosts-text-ranking-and-zero-shot-performance-4cfa8fd23cef,explanation of one text ranking algorithm in #L8.3,which employs pretrained T5 models for text ranking with various ranking losses to directly optimize ranking performance. RankT5 models more natively support text ranking by outputting real number...
pdf743.pdf,https://medium.com/towards-data-science/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,a good explanation of bert model covered in #L8.2,"As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is consid..."
pdf744.pdf,https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa,a good explanation of word2vec covered in #L8.1,Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW)
pdf745.pdf,https://medium.com/jumio/tutorial-unsupervised-ranking-using-machine-learning-b45d911b1af7,This article contains details about ML related Ranking models discusse...,Unsupervised Ranking Using Machine Learning
pdf746.pdf,https://www.geeksforgeeks.org/overview-of-word-embedding-using-embeddings-from-language-models-elmo/,This article contains about Embeddings from Language Models (ELMo) dis...,Overview of Word Embedding using Embeddings from Language Models (ELMo)
pdf747.pdf,https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c,This article contains about the Skip-Gram model discussed in Lecture #...,Skip-Gram
pdf748.pdf,https://medium.com/jumio/tutorial-unsupervised-ranking-using-machine-learning-b45d911b1af7,Unsupervised Ranking Using Machine Learning #L8.3,No Preview Available
pdf749.pdf,https://towardsdatascience.com/neural-language-models-32bec14d01dc,Neural Language Models #L8.2,No Preview Available
pdf750.pdf,https://www.linkedin.com/pulse/nlp-topic-the-language-modeling-nilesh-gode/,L10.2: Fixed window Neural Language Model,No Preview Available
pdf751.pdf,http://www.diva-portal.org/smash/get/diva2:1041938/FULLTEXT01.pdf,L10.1: Distributional hypothesis,No Preview Available
pdf752.pdf,https://aclweb.org/aclwiki/Distributional_Hypothesis,L10.1: Distributional Hypothesis,No Preview Available
pdf753.pdf,https://weaviate.io/blog/cross-encoders-as-reranker,Explains Bi-Encoder in #L8.3,Bi-Encoder models
pdf754.pdf,https://arxiv.org/pdf/1810.04805.pdf,A paper that explains BERT in #L8.2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
pdf755.pdf,https://nlp.stanford.edu/pubs/glove.pdf,Explains GloVe in #L8.1,GloVe: Global Vectors for Word Representation
pdf756.pdf,https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,An explanation of BERT #L8.3,No Preview Available
pdf757.pdf,https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/,Intro to LSTM #L8.2,No Preview Available
pdf758.pdf,https://www.turing.com/kb/guide-on-word-embeddings-in-nlp,An intro to word embedding #L8.1,No Preview Available
pdf759.pdf,https://medium.com/jumio/tutorial-unsupervised-ranking-using-machine-learning-b45d911b1af7,#Unsupervised ranking#L8.3,"Introduction to Ranking
Every day we encounter ranking, especially machine learning-aided ranking, without even realizing it. Whether you are shopping on Amazon, looking for your next flight, sear..."
pdf760.pdf,https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314,Description about Skip-gram and CBOW #L8.1,Word2Vec — Skip-gram and CBOW
pdf761.pdf,https://en.wikipedia.org/wiki/Recurrent_neural_network,Wikipidia page of Recurrent neural network #L8.2,Recurrent neural network
pdf762.pdf,https://aclanthology.org/2021.naacl-tutorials.1/,Useful paper for Text Ranking #L8.3,Pretrained Transformers for Text Ranking: BERT and Beyond
pdf763.pdf,https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314,Description about Skip-gram and CBOW,Word2Vec — Skip-gram and CBOW
pdf764.pdf,https://ankitnitjsr13.medium.com/text-rank-algorithm-a8c2cc58ea9c,More explanation of Text Ranking that covered in the lecture #L8.3,Math behind TextRank Algorithm
pdf765.pdf,https://www.tutorialspoint.com/time_series/time_series_lstm_model.htm,The introduction of LSTM model that covered in the lecture #L8.2,Time Series - LSTM Model
pdf766.pdf,https://www.geeksforgeeks.org/word-embeddings-in-nlp/,The introduction of Word Embedding that covered in the lecture #L8.1,Word Embeddings in NLP
pdf767.pdf,https://arxiv.org/pdf/2204.08669.pdf,L8.3: Monobert for hate speech detection,No Preview Available
pdf768.pdf,https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html,L8.3 Training BERT for NLP,No Preview Available
pdf769.pdf,https://www.linkedin.com/pulse/nlp-topic-the-language-modeling-nilesh-gode/,L8.2: Fixed window Neural Language model,No Preview Available
pdf770.pdf,https://towardsdatascience.com/from-word-embeddings-to-pretrained-language-models-a-new-age-in-nlp-part-1-7ed0c7f3dfc5,From Word Embeddings to Pretrained Language Models — A New Age in NLP ...,No Preview Available
pdf771.pdf,https://arxiv.org/pdf/1301.6705.pdf,Probabilistic Latent Semantic Analysis (PLSA) L6.3,No Preview Available
pdf772.pdf,https://medium.com/b2w-engineering-en/the-reasoning-behind-the-expectation-maximization-em-algorithm-4a773428b3fc,The Expectation-Maximization (EM) Algorithm L6.2,No Preview Available
pdf773.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,The EM Algorithm L6.1,No Preview Available
pdf774.pdf,https://www.mathworks.com/help/stats/hidden-markov-models-hmm.html,Hidden Markov Models (HMM) Matlab L7.3,No Preview Available
pdf775.pdf,https://web.stanford.edu/~jurafsky/slp3/A.pdf,Stanford Hidden Markov Models L7.2,No Preview Available
pdf776.pdf,https://towardsdatascience.com/markov-and-hidden-markov-model-3eec42298d75,Hidden Markov Models L7.1,No Preview Available
pdf777.pdf,https://link.springer.com/article/10.1007/s10791-021-09398-0,Neural ranking models for document retrieval mentioned in Lecture 8.3,No Preview Available
pdf778.pdf,https://towardsdatascience.com/glove-elmo-bert-9dbbc9226934,A guide to state-of-the-art text classification using Spark NLP mentio...,No Preview Available
pdf779.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,Overview of word embedding in neural language models mentioned in lect...,No Preview Available
pdf780.pdf,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00298/43535/What-BERT-Is-Not-Lessons-from-a-New-Suite-of,"#L8.3: BERT does not understand negation, which can be problematic. Ho...",BERT fails completely to show generalizable understanding of negation
pdf781.pdf,https://blog.invgate.com/gpt-3-vs-bert,"#L8.2: What makes GPT-3 and BERT different? Of course their framework,...","GPT-3 is an autoregressive model, while BERT is bidirectional. While GPT-3 only considers the left context when making predictions, BERT takes into account both left and right context. This makes B..."
pdf782.pdf,https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/,#L8.1: This document shows the power of word embeddings. If you calcul...,king – man + woman = queen
pdf783.pdf,https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af,#L8.3 An article based on transfer-based self-supervised architectures...,No Preview Available
pdf784.pdf,https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef,#L8.2 A detailed blog explaining BERT,No Preview Available
pdf785.pdf,https://pureai.com/articles/2020/01/06/neural-word-embeddings.aspx,#L8.1 An article on neural word embeddings,No Preview Available
pdf786.pdf,https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/,"L8.2, 8.3: Article explains how the BERT language model works, which I...",Explanation of BERT Model – NLP
pdf787.pdf,https://www.geeksforgeeks.org/understanding-of-lstm-networks/,"L8.2: Great article that explains what are LSTM, how do they work",Understanding of LSTM Networks
pdf788.pdf,https://aclanthology.org/2021.naacl-tutorials.1.pdf,Pretrained Transformers for Text Ranking: BERT and Beyond L8.3,No Preview Available
pdf789.pdf,https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9,Two minutes NLP — 11 word embeddings models you should know L8.2,No Preview Available
pdf790.pdf,https://www.topbots.com/leading-nlp-language-models-2020/,10 Leading Language Models For NLP In 2022 #L8.3,No Preview Available
pdf791.pdf,https://towardsdatascience.com/glove-elmo-bert-9dbbc9226934,"GloVe, ELMo & BERT #L8.2",No Preview Available
pdf792.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,What Are Word Embeddings for Text? #L8.1,No Preview Available
pdf793.pdf,https://arxiv.org/pdf/1810.04805.pdf,An introduction to BERT #L8.2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
pdf794.pdf,https://direct.mit.edu/coli/article/49/1/253/113643/Pretrained-Transformers-for-Text-Ranking-BERT-and,New paper for #8.3,Pretrained Transformers for Text Ranking: BERT and Beyond
pdf795.pdf,https://towardsdatascience.com/glove-elmo-bert-9dbbc9226934,more details #8.2,"GloVe, ELMo & BERT
A guide to state-of-the-art text classification using Spark NLP"
pdf796.pdf,https://ai.stackexchange.com/questions/26739/what-is-the-difference-between-a-language-model-and-a-word-embedding,#8.1,What is the difference between a language model and a word embedding?
pdf797.pdf,https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model,a brief overview on the background and the mechanism of BERT,#8.2 BERT neural language model
pdf798.pdf,https://arxiv.org/pdf/2010.01057.pdf,LUKE - a pre-trained BERT model with entity-aware self-attention mecha...,"The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on th..."
pdf799.pdf,https://arxiv.org/pdf/2010.01057.pdf,LUKE - a pre-trained BERT model with entity-aware self-attention mecha...,"The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on th..."
pdf800.pdf,https://en.wikipedia.org/wiki/Ranking_(information_retrieval),Ranking (information retrieval) #L8.3,Ranking (information retrieval)
pdf801.pdf,https://ankitnitjsr13.medium.com/text-rank-algorithm-a8c2cc58ea9c,Math behind TextRank Algorithm #L8.3,Math behind TextRank Algorithm
pdf802.pdf,https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf,TextRank: Bringing Order into Texts #L8.3,No Preview Available
pdf803.pdf,https://www.youtube.com/watch?v=pO_6Jk0QtKw,Word Embedding #L8.1,Word Embedding - Natural Language Processing| Deep Learning
pdf805.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,Word Embeddings Introduction#L8.1,Word Embeddings
pdf806.pdf,https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/,ELMo #L8.2,ELMo for Extracting Features from Text
pdf807.pdf,https://en.wikipedia.org/wiki/Long_short-term_memory,LSTM,Long short-term memory #L8.2
pdf808.pdf,https://en.wikipedia.org/wiki/BERT_(language_model),Bert,BERT (language model) #L8.1
pdf809.pdf,https://insights.daffodilsw.com/blog/what-are-language-models-in-nlp,#L9 Proof of Learning,These language models are based on neural networks and are often considered as an advanced approach to execute NLP tasks. Neural language models overcome the shortcomings of classical models such...
pdf810.pdf,https://direct.mit.edu/coli/article/49/1/253/113643/Pretrained-Transformers-for-Text-Ranking-BERT-and,A book on text ranking based on Transformers. Useful for Lecture #L8.3,Pretrained Transformers for Text Ranking: BERT and Beyond
pdf811.pdf,https://en.wikipedia.org/wiki/Long_short-term_memory,Wikipedia page of LSTM. Relevant to Lecture #L8.2,Long short-term memory (LSTM) is an artificial neural network used in the fields of artificial intelligence and deep learning.
pdf812.pdf,https://machinelearningmastery.com/what-are-word-embeddings/,A blog introducing word embeddings. Related to Lecture #L8.1,"A word embedding is a learned representation for text where words that have the same meaning have a similar representation.

It is this approach to representing words and documents that may be co..."
pdf813.pdf,https://www.york.ac.uk/staff/research/research-impact/impact-definition/,"An website explaining what is research impact(discussed in #L9.2), sum...","Research impact is the effect research has beyond academia. The York Research Impact Statement (PDF , 286kb) describes research impact as “…when the knowledge generated by our research contributes ..."
pdf814.pdf,https://softwaredoug.com/blog/2022/01/17/lambdamart-in-depth.html,A great tutorial explaining the intuition behind LambdaMART algorithm ...,"LambdaMART is a classic. It’s the endlessly tinkerable classic car of ranking algorithms. If you can grok the algorithm, you can play with the model architecture, coming up with your own variations..."
pdf815.pdf,https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/,A comprehensive and friendly introduction to the diffusion model.,"Diffusion Models are generative models, meaning that they are used to generate data similar to the data on which they are trained. Fundamentally, Diffusion Models work by destroying training data t..."
pdf816.pdf,https://aclanthology.org/2021.naacl-tutorials.1.pdf,Bert and other models have shown to be effective for text ranking task...,Neural Language Models for Text Ranking
pdf817.pdf,https://arxiv.org/pdf/1906.05474.pdf,It evaluate the performance of BERT and ElMo on biomedical NLP tasks o...,"Transfer Learning in Biomedical Natural Language Processing: An
Evaluation of BERT and ELMo on Ten Benchmarking Datasets"
pdf818.pdf,https://en.wikipedia.org/wiki/Word_embedding,The introduction of word embedding by wikipedia #L8.1,Word embedding
pdf819.pdf,https://www.arxiv-sanity-lite.com/?rank=pid,A survey about neural language models for text ranking #L8.3,"Modeling long texts has been an essential technique in the field of natural language processing (NLP). With the ever-growing number of long documents, it is important to develop effective modeling ..."
pdf820.pdf,https://arxiv.org/,#L9.2 arXiv - free paper distribution,"arXiv is a free distribution service and an open-access archive for 2,231,518 scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance,..."
pdf821.pdf,https://medium.com/@nikhilbd/intuitive-explanation-of-learning-to-rank-and-ranknet-lambdarank-and-lambdamart-fe1e17fac418,"Intuitive explanation of Learning to Rank (and RankNet, LambdaRank and...",No Preview Available
pdf822.pdf,https://www.geeksforgeeks.org/page-rank-algorithm-implementation/,#L9.1 More detail on PageRank,Page Rank Algorithm and Implementation
pdf823.pdf,https://arxiv.org/abs/1904.07531,Understanding the Behaviors of BERT in Ranking #L8.3,No Preview Available
pdf824.pdf,https://jalammar.github.io/illustrated-bert/,"The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning...",No Preview Available
pdf825.pdf,https://dl.acm.org/doi/abs/10.1145/3077136.3080831?casa_token=_wtB_f06d1MAAAAA:7Phagqbz4dnVqLyhUH8Jqtf147SJgZ61DQfw6Bw4Xy52kSA08eG_LzWO4xzx7Js9zWBZ03OwVKZs,Relevance-based Word Embedding #L8.1,No Preview Available
pdf826.pdf,https://arxiv.org/abs/2301.08801,Information Retrieval: Recent Advances and Beyond #L9.2,No Preview Available
pdf827.pdf,https://dl.acm.org/doi/abs/10.1145/3308558.3313447,Unbiased LambdaMART: An Unbiased Pairwise Learning-to-Rank Algorithm #...,No Preview Available
pdf828.pdf,https://www.researchgate.net/profile/Christopher-Burges/publication/228936665_From_ranknet_to_lambdarank_to_lambdamart_An_overview/links/00b49518c11a416a3b000000/From-ranknet-to-lambdarank-to-lambdamart-An-overview.pdf,"From RankNet to LambdaRank to
LambdaMART: An Overview #L9.1",No Preview Available
pdf829.pdf,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf,"The paper on RankNet, LambdaRank and LambdaMART, mentioned in #L9.1",No Preview Available
pdf830.pdf,https://en.wikipedia.org/wiki/Learning_to_rank,#L9.1 learning to rank wiki page,No Preview Available
pdf831.pdf,https://pdf.sciencedirectassets.com/280203/1-s2.0-S1877050922X00045/1-s2.0-S1877050922005191/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEN3//////////wEaCXVzLWVhc3QtMSJGMEQCIDzMEyJ0ddDLgy6P69oBKykiyS1OZlJP2qV1VZ34B22mAiA4MBPd3MjPv59omRx8/iqNVvf/h14bvfsbCpapg7Xmtyq7BQi2//////////8BEAUaDDA1OTAwMzU0Njg2NSIMT+NNaxH2h9vNs8CmKo8F3xMilMaiHhEk7sI705FCsrcJ8S4sAUi57H5veE6uXQSQaye08dN+C50F8JZ3KnNV37D/4JoNlerkkOAAVpIxKUn6ENKuORIFzDpu1Mo5Ii7hd3u3stvM+bbSx4PHpM3HiVwdoGJPy3N8NVqCO3XyXvNbdO6f2e8JK8TryZK1hEyLPR01cWvtEoKkeINPtbXijcaySsE9VnBCHltoylOSIpWV/BW7CVvMsJAh0YdXe1tmC9K6CjPdRO0yIb5I/a9OMR7dHdvKRaIEg3XhANw3EFhTh+HQPAOePqPU1t2K/uhEb9WEbzlqw/Og1p+ZfwztDT/bcutyGpxSPdUqp/LwWbFidu2wWOE+zawwQ9H0NTOD6RSTiFfpQNomayRwdBAw4ygKyryNsWbvx1xreesMeYJB3OBDspjZWUNPILGu4LlwKbt/+XHk02xXzexbKhTfx+wkLEwmyodm/7IaXwA7yvnjDaY4T7U1Kn74Jj/1HPCUk+sHSIHY/LNt4er+cwtFJ26sRun/petMYrwvXK0w1qxQHPoMrK3lFOA2ABwANap/O2kUDay28p61Sx6tVjb2QOUDbYIIHFFx77XdJpFMTFt9onqsZNaoLJGJ6MS835v2kkvcPLdY4+pnEuEV1we8BJc1WlW0r3eg7bhU+NtJHWpKcLfXr5smSVtrUMLKX2l8OgB0bqYvr+aZtoTF2ieVhC1tWd4RBTPljfXUyV952f7SA5hkEZsMn0aFzvL93F29TzbZEsrd40fgZre2efvwOiYaQ0FktbXVCibBiN+tsViZKPSRf8d+RCvZJoVYAvVMyhptqUOxh9Q9cz8bGVqmTInMu8Ft6USQ/TF5FlCE0WgQIlGsVZEO34NLU+48FzDLlKShBjqyAbhIHgJ/FRLMS0PiTK0+0aZmB+De5xqLHYFCqr67xh3dGkvG12YQCvi/axwcEKYttSBDywLsJ1vVQ/i24Pta29bKiuxFEBRCnpVekpXgBxEPHMSEUmnPGxQLW7dAIV4vgrf0+eltKvJbIG/iC+Zfpb/2DWsytEV2oWOu0mfXBdhmpX9ho4WgylGhVEa6iT2GfDRjxuqdNj7GhdaT30pMLhlimcKS6UxjOAH/Y6H7ZJluspU=,A Review on recent research in Information retrieval #L9.2,Discusses various research perspectives for study and compares information retrieval techniques.
pdf832.pdf,https://towardsdatascience.com/learning-to-rank-for-information-retrieval-a-deep-dive-into-ranknet-200e799b52f4,Learning to Rank for Information Retrieval: A Deep Dive into RankNet #...,An insight into the state-of-the-art ranking systems that can be used for Information Retrieval.
pdf833.pdf,https://libguides.ecsu.edu/c.php?g=1122540,#L9.2 Critical Thinking & Research,This guide is created to aid you in your development of critical thinking skills and in your ability to conduct research using library resources.
pdf834.pdf,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf,#L9.1  Learning to Rank: From Pairwise Approach to Listwise Approach,The paper proposes the permutation and top one probability for litewise approaches in IR
pdf835.pdf,https://journals.sagepub.com/doi/abs/10.1177/016555158400800105?journalCode=jisb,Theory and explanation in information retrieval research #L9.2,The relationship between theory and explanation in information retrieval research is analysed. Problems in the development of a generalisable information retrieval theory from information retrieval...
pdf836.pdf,https://www.sciencedirect.com/science/article/pii/S1877050922005191,#L9.2 A research survey of recent research in information retrieval,A Review on recent research in information retrieval
pdf837.pdf,https://medium.com/@nikhilbd/intuitive-explanation-of-learning-to-rank-and-ranknet-lambdarank-and-lambdamart-fe1e17fac418,#L9.1 An medium article about Intuitive explanation of Learning to Ran...,"Intuitive explanation of Learning to Rank (and RankNet, LambdaRank and LambdaMART)"
pdf838.pdf,https://www.quora.com/Information-Retrieval-What-are-some-hot-research-topics-in-IR,Research Topics in the field #L9.2,No Preview Available
pdf839.pdf,https://www.cs.huji.ac.il/w~shashua/papers/k-planes-nips02.pdf,Ranking with large margin principles #L9.1,No Preview Available
pdf840.pdf,https://www.microsoft.com/en-us/research/blog/ranknet-a-ranking-retrospective/,An article from MSR that looks back on the original RankNet paper. and...,L#9.1: RankNet: A ranking retrospective
pdf842.pdf,https://stats.stackexchange.com/questions/70417/regression-with-rank-order-as-dependent-variable,Regression Ranking #L9.2,Regression Ranking #L9.2
pdf843.pdf,https://en.wikipedia.org/wiki/Ranking_SVM,Ranking SVM #L9.1,Ranking SVM #L9.1
pdf844.pdf,https://resdev.web.illinois.edu/compliance-safety/research-integrity-ethics-and-misconduct,"UIUC guidelines around research integrity, ethics, and misconduct. #L9...",No Preview Available
pdf845.pdf,https://wellecks.wordpress.com/2015/02/21/peering-into-the-black-box-visualizing-lambdamart/,A visualization of how LambdaMART works. #L9.1,Visualizing LambdaMART
pdf846.pdf,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2008-164.pdf,Optimization of Research #L9.2,Direct Optimization of Information Retrieval
pdf847.pdf,https://www.iro.umontreal.ca/~nie/IFT6255/Books/Learning-to-rank.pdf,Learning to rank #L9.1,No Preview Available
pdf848.pdf,https://www.site.uottawa.ca/~bochmann/Projects/how-to-do-good-research/index.html,Related to Lecture #L9.2,How to do good research
pdf849.pdf,https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4,Related to Lecture #L9.1,Learning to Rank: A Complete Guide to Ranking using Machine Learning
pdf850.pdf,https://orfe.princeton.edu/research/optimization,Optimization of Research princeton L9.2,No Preview Available
pdf851.pdf,https://dl.acm.org/doi/10.1561/1500000016,Learning to Rank for Information Retrieval #L9.1,No Preview Available
pdf852.pdf,https://www.youtube.com/watch?v=zW2bUwg_nH0,An introduction to the topic learning to rank mentioned in #L9.1,"What Is Learning To Rank (LTR), Pointwise, Pairwise, and Listwise Ranking"
pdf853.pdf,https://medium.com/@nikhilbd/intuitive-explanation-of-learning-to-rank-and-ranknet-lambdarank-and-lambdamart-fe1e17fac418,"Intuitive explanation of Learning to Rank (and RankNet, LambdaRank and...","Intuitive explanation of Learning to Rank (and RankNet, LambdaRank and LambdaMART)"
pdf854.pdf,https://writing.wisc.edu/handbook/assignments/planresearchpaper/,#L9.2 Writing a research paper,No Preview Available
pdf856.pdf,https://medium.com/@nikhilbd/pointwise-vs-pairwise-vs-listwise-learning-to-rank-80a8fe8fadfd,#L9.1 Pointwise vs. Pairwise vs. Listwise Learning to Rank,No Preview Available
pdf857.pdf,https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html,"Learning to Rank: different approaches, libraries, and evaluations wit...",No Preview Available
pdf858.pdf,https://docs.aws.amazon.com/opensearch-service/latest/developerguide/learning-to-rank.html,AWS learning to rank service guideline # L9.1,Learning to Rank is an open-source plugin that lets you use machine learning and behavioral data to tune the relevance of documents. It uses models from the XGBoost and Ranklib libraries to rescore...
pdf859.pdf,https://medium.com/swlh/ranknet-factorised-ranknet-lambdarank-explained-implementation-via-tensorflow-2-0-part-i-1e71d8923132,Medium article that covers how to implement Learning to Rank algorithm...,"L#9.1, 9.2: RankNet, LambdaRank TensorFlow Implementation"
pdf860.pdf,https://link.springer.com/article/10.1007/s13735-014-0058-8,Optimization of Research,No Preview Available
pdf861.pdf,https://link.springer.com/book/10.1007/978-3-642-14267-3,Learning to rank : Explained,No Preview Available
pdf862.pdf,https://en.wikipedia.org/wiki/Learning_to_rank,The wikipidia page for Learning to rank #L9.1,Learning to rank
pdf863.pdf,https://proceedings.neurips.cc/paper_files/paper/2002/file/51de85ddd068f0bc787691d356176df9-Paper.pdf,Paper of Ranking with Large Margin Principles in Lecture #L10.2,Ranking with Large Margin Principles
pdf864.pdf,https://link.springer.com/chapter/10.1007/11776420_44,Subset Ranking using Regression in lecture #L10.1,"We study the subset ranking problem, motivated by its important application in web-search. In this context, we consider the standard DCG criterion (discounted cumulated gain) that measures the qual..."
pdf865.pdf,https://dl.acm.org/doi/pdf/10.1145/358923.358934,#L9.2 A good survey paper on IR,No Preview Available
pdf866.pdf,https://www.cs.cornell.edu/people/tj/publications/joachims_etal_05a.pdf,"#L9.1 - Accurately Interpreting Clickthrough Data as Implicit paper
F...",No Preview Available
pdf867.pdf,https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/,#L9.2: Recent breakthroughs in technology,No Preview Available
pdf868.pdf,https://theweek.com/health-and-science/1019386/12-recent-scientific-breakthroughs,#L9.2: Recent Scientific Breakthroughs,No Preview Available
pdf869.pdf,https://blog.degruyter.com/how-to-write-a-good-research-paper/,HOW TO WRITE AND PUBLISH A RESEARCH PAPER IN 7 STEPS #L9.2,HOW TO WRITE AND PUBLISH A RESEARCH PAPER IN 7 STEPS
pdf870.pdf,https://proceedings.neurips.cc/paper_files/paper/2002/file/51de85ddd068f0bc787691d356176df9-Paper.pdf,Ranking with Large Margin Principle: TwoApproaches #L9.1,"This explains about Ranking with Large Margin Principle: Two
Approaches #L9.1"
pdf871.pdf,https://www.engati.com/glossary/information-retrieval,"#L9.2 An article discussing the possible research topics in IR, and it...","What do you mean by information retrieval?
Information retrieval (IR) is the process of obtaining information system resources that are relevant to an information need from a collection of those r..."
pdf872.pdf,https://lucidworks.com/post/abcs-learning-to-rank/,"#L9.1 An article detailing the mechanism of learning to rank, and thei...","What Is Learning To Rank?
Learning to rank (LTR) is a class of algorithmic techniques that apply supervised machine learning to solve ranking problems in search relevancy. In other words, it’s wha..."
pdf873.pdf,https://www.quora.com/How-should-PhD-students-motivate-themselves-when-they-hit-a-wall-on-some-problems,# 9.2 What can you do when you hit a research wall,hit a wall on some problems
pdf874.pdf,https://medium.com/@nikhilbd/pointwise-vs-pairwise-vs-listwise-learning-to-rank-80a8fe8fadfd,#9.1 about Pointwise Learning to Rank,Pointwise approaches
pdf875.pdf,https://arxiv.org/pdf/2112.06489.pdf,"The paper introduces CMIMH, a method for unsupervised cross-modal retr...","Multi-Modal Mutual Information Maximization:
A Novel Approach for Unsupervised Deep
Cross-Modal Hashing"
pdf876.pdf,https://proceedings.neurips.cc/paper_files/paper/2002/file/51de85ddd068f0bc787691d356176df9-Paper.pdf,this paper introduces 2 optimal approaches for ranking k instances usi...,Ranking with large margin principle:two approaches
pdf877.pdf,https://papers.nips.cc/paper_files/paper/2002/hash/51de85ddd068f0bc787691d356176df9-Abstract.html,The paper that introduce Ranking with Large Margin Principle #L9.1,Ranking with Large Margin Principle: Two Approaches
pdf878.pdf,https://en.wikipedia.org/wiki/Ranking_SVM,The defination and explaination of Ranking SVM on wikipedia #L9.2,Ranking SVM
pdf879.pdf,https://en.wikipedia.org/wiki/Operations_research,The wikipidia page of operations research #L9.2,Operations research
pdf880.pdf,https://medium.com/@nikhilbd/pointwise-vs-pairwise-vs-listwise-learning-to-rank-80a8fe8fadfd,#L9.1: Pointwise vs. Pairwise vs. Listwise Learning to Rank,There is another kind of Learning to Rank approach called listwise. Listwise approaches directly look at the entire list of documents and try to come up with the optimal ordering for it.
pdf881.pdf,https://www.indeed.com/career-advice/career-development/research-problem,L9.2: How to define a research problem,No Preview Available
pdf882.pdf,https://www.quora.com/What-are-the-differences-between-pointwise-pairwise-and-listwise-approaches-to-Learning-to-Rank,"L9.1 Difference between Pointwise, pairwise and listwise approaches to...",No Preview Available
pdf883.pdf,https://medium.com/@nikhilbd/pointwise-vs-pairwise-vs-listwise-learning-to-rank-80a8fe8fadfd,"L9.1 Pointwise, Pairwise vs Listwise learning to rank",No Preview Available
pdf884.pdf,https://dl.acm.org/doi/pdf/10.1145/511793.511830,Research problems in IR #L9.2,Research problems in IR
pdf885.pdf,https://medium.com/digital-diplomacy/how-to-look-for-ideas-in-computer-science-research-7a3fa6f4696f,How to Look for Ideas in Computer Science Research. #L9.2,No Preview Available
pdf886.pdf,https://medium.com/digital-diplomacy/how-to-look-for-ideas-in-computer-science-research-7a3fa6f4696f,How to Look for Ideas in Computer Science Research. #L9.2,How to Look for Ideas in Computer Science Research
pdf887.pdf,https://www.geeksforgeeks.org/page-rank-algorithm-implementation/,Pagerank algorithm for ranking document importance,#9.2 ranking on relevance
pdf888.pdf,https://medium.com/@nikhilbd/pointwise-vs-pairwise-vs-listwise-learning-to-rank-80a8fe8fadfd,Pointwise vs. Pairwise vs. Listwise Learning to Rank. #L9.1,No Preview Available
pdf889.pdf,https://medium.com/@nikhilbd/pointwise-vs-pairwise-vs-listwise-learning-to-rank-80a8fe8fadfd,Pointwise vs. Pairwise vs. Listwise Learning to Rank,No Preview Available
pdf890.pdf,https://dl.acm.org/doi/10.1145/2682862.2682863,This article gives as great insights to the researach in BM25 describe...,Improvements to BM25 and Language Models Examined
pdf891.pdf,https://en.wikipedia.org/wiki/Research,explain about #L9.2,"Research is ""creative and systematic work undertaken to increase the stock of knowledge"""
pdf892.pdf,https://medium.com/data-science-at-microsoft/search-and-ranking-for-information-retrieval-ir-5f9ca52dd056,A usefule description of ranking in #L9.1,"One simple approach may be ranking these documents on the similarity. However, there are many situations in which the search query is exactly present in the document, but the document still might n..."
pdf893.pdf,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2008-164.pdf,#L9.2 A general approximation framework for optimization of Informatio...,"Recently direct optimization of information retrieval (IR) measures
becomes a new trend in learning to rank. Several methods have been
proposed and the effectiveness of them has also been empirical..."
pdf894.pdf,https://towardsdatascience.com/learning-to-rank-for-information-retrieval-a-deep-dive-into-ranknet-200e799b52f4,#L9.1 A Deep Dive into Rank Net,Machine Learning and Artificial Intelligence are currently driving innovation in the field of Computer Science and they are being applied on a multitude of fields across disciplines.
pdf895.pdf,https://aclanthology.org/2021.naacl-tutorials.1.pdf,#L8.3 Pre trained transformers for text ranking,"The goal of text ranking is to generate an ordered
list of texts retrieved from a corpus in response to
a query for a particular task."
pdf896.pdf,https://towardsdatascience.com/glove-elmo-bert-9dbbc9226934,"#L8.2 GloVe, ELMo and BERT",One of the most challenging tasks for machine learning models is finding the best way to to generate numeric representations for words so the model can use that information in its calculations.
pdf897.pdf,https://pureai.com/articles/2020/01/06/neural-word-embeddings.aspx,#L8.1 Understanding Neural Word Embeddings,The data scientists at Microsoft Research explain how word embeddings are used in natural language processing
pdf898.pdf,https://www.site.uottawa.ca/~bochmann/Projects/how-to-do-good-research/index.html,#L9.2 How to do good research in computer science?,How to do good research
pdf899.pdf,https://www.feedough.com/repositioning-meaning-reasons-examples/,#L9.2: What is Repositioning?,"It talks about repositioning as a marketing strategy but one line can be used to create connections between repositioning a product to repositioning a research paper: ""Repositioning refers to the p..."
pdf900.pdf,https://pureai.com/articles/2020/01/06/neural-word-embeddings.aspx,#L8.1 Intro to Neural Word Embeddings,Most of the advanced neural architectures in NLP use word embeddings
pdf901.pdf,https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4,More introduction of Learning to Rank that covered in the lecture #L9....,Learning to Rank: A Complete Guide to Ranking using Machine Learning
pdf903.pdf,https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_information_retrieval.htm,The description of starting IR research that covered in the lecture #9...,NLP - Information Retrieval
pdf904.pdf,https://medium.com/@mayurbhangale/pointwise-pairwise-and-listwise-learning-to-rank-baf0ad76203e,The more introduction of Pairwise Learning to Rank that covered in the...,"Pointwise, Pairwise and Listwise Learning to Rank"
pdf905.pdf,https://www.monash.edu/library/help/assignments-research/developing-research-questions,#9.2 how to get a good research topic,Developing research questions
pdf906.pdf,https://medium.com/swlh/ranknet-factorised-ranknet-lambdarank-explained-implementation-via-tensorflow-2-0-part-i-1e71d8923132,#9.1 Implementation for kinds of rank,"RankNet, LambdaRank TensorFlow Implementation"
pdf907.pdf,https://libguides.elmira.edu/research,How to Do Research: A Step-By-Step Guide #L9.2,No Preview Available
pdf908.pdf,https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4,Learning to Rank: A Complete Guide to Ranking using Machine Learning #...,No Preview Available
pdf909.pdf,https://www.techtarget.com/whatis/definition/operations-research-OR,This article gives a great explanation for OR described in Lecture #L9...,"Operations research (OR) is an analytical method of problem-solving and decision-making that is useful in the management of organizations. In operations research, problems are broken down into basi..."
pdf910.pdf,https://www.youtube.com/watch?v=qMTuMa86NzU,Gaussian Mixture Models and EM #L5.2,No Preview Available
pdf911.pdf,https://www.site.uottawa.ca/~bochmann/Projects/how-to-do-good-research/index.html,How to do a good research #L9.2,No Preview Available
pdf913.pdf,https://arxiv.org/abs/1810.04805,BERT for #L8.3,No Preview Available
pdf914.pdf,https://github.com/IntelLabs/rnnlm,RNNLM for #L8.2,No Preview Available
pdf915.pdf,https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c,Skip-gram #L8.1,No Preview Available
pdf916.pdf,https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2,#L8.3 Walkthrough of GPT models,No Preview Available
pdf917.pdf,https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,#L8.2 Detailed BERT explain.,No Preview Available
pdf918.pdf,https://www.site.uottawa.ca/~bochmann/Projects/how-to-do-good-research/index.html,Research related topic covered in #L9.2,How to do good research
pdf919.pdf,https://dl.acm.org/doi/10.1007/11776420_44,A paper explaining Subset ranking using regression covered in #L9.1,Subset ranking using regression
pdf920.pdf,https://dl.acm.org/doi/10.1145/3437963.3441662,Beyond Probability Ranking Principle,No Preview Available
pdf921.pdf,https://dl.acm.org/doi/10.1145/2911451.2911543,Interface Interactive Model,No Preview Available
pdf922.pdf,https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=8920,Interactive IR,No Preview Available
pdf923.pdf,https://bennettlive.com/challenges/,Bennett's design challenges,No Preview Available
pdf924.pdf,http://karpathy.github.io/2016/09/07/phd/,#L9.2,PhD Survival Guide by Andrej Karpathy
pdf925.pdf,https://link.springer.com/content/pdf/10.1007/978-3-319-29659-3.pdf,#L9.1 Book on recommender systems,No Preview Available
pdf926.pdf,https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=8920,An overview of Interactive Information Retrieval #L10,No Preview Available
pdf927.pdf,https://sites.google.com/view/cair-ws/,International Workshop on Conversational Approaches to Information Ret...,International Workshop on Conversational Approaches to Information Retrieval (CAIR)
pdf929.pdf,https://medium.com/@nikhilbd/intuitive-explanation-of-learning-to-rank-and-ranknet-lambdarank-and-lambdamart-fe1e17fac418,"Intuitive explanation of Learning to Rank (and RankNet, LambdaRank and...","Intuitive explanation of Learning to Rank (and RankNet, LambdaRank and LambdaMART)"
pdf930.pdf,https://arxiv.org/pdf/1706.03847.pdf,An interesting paper about RNN for ranking and recommendation #L9.1,No Preview Available
pdf931.pdf,https://arxiv.org/abs/1803.00710,Related to Lecture #L10.4,"Reinforcement Learning to Rank in E-Commerce Search Engine: Formalization, Analysis, and Application"
pdf932.pdf,https://experts.illinois.edu/en/publications/information-retrieval-as-card-playing-a-formal-model-for-optimizi,Related to Lecture #L10.3,Information retrieval as card playing: A formal model for optimizing interactive retrieval interface
pdf933.pdf,https://www.sciencedirect.com/science/article/abs/pii/030645739290007M,Evaluation measures for interactive information retrieval #L10,This study aims to identify the best evaluation measure(s) for interactive IR performance.
pdf934.pdf,https://dl.acm.org/doi/abs/10.1145/3077136.3080785,Related to Lecture #L10.2,Information Retrieval Meets Game Theory: The Ranking Competition Between Documents' Authors
pdf935.pdf,https://www.cambridge.org/core/books/abs/interactive-information-seeking-behaviour-and-retrieval/interactive-information-retrieval-history-and-background/E1D59A73006C27FF68EFE87B62049E93,Related to lecture #L10.1,Interactive information retrieval: history and background
pdf936.pdf,http://www.koreascience.or.kr/article/JAKO201328052539177.page,Interactive Information Retrieval: An Introduction #L10 IIR,Interactive Information Retrieval: An Introduction
pdf937.pdf,https://link.springer.com/article/10.1007/s10791-021-09398-0,Neural Ranking in IR,No Preview Available
pdf938.pdf,https://informationr.net/ir/8-3/paper152.html,IIR Evaluation,No Preview Available
pdf939.pdf,https://sigir.org/ictir2017/tutorials/,Differential Privacy in R,No Preview Available
pdf940.pdf,https://arxiv.org/abs/1910.13166,A paper of conversational search model in Lecture #10.1,Towards a Model for Spoken Conversational Search
pdf941.pdf,https://www.youtube.com/watch?v=pfkPT9Ihr_M,#L10: Theories of Conversation for Conversational IR,No Preview Available
pdf942.pdf,https://www.analyticsvidhya.com/blog/2020/03/6-pretrained-models-text-classification/,pretrained model,Top 6 Open Source Pretrained Models for Text Classification you should use
pdf943.pdf,https://medium.com/swlh/ranknet-factorised-ranknet-lambdarank-explained-implementation-via-tensorflow-2-0-part-i-1e71d8923132,This article contains topics discussed in lecture #L9.2,"RankNet, LambdaRank TensorFlow Implementation"
pdf944.pdf,https://www.sciencedirect.com/science/article/abs/pii/0020027171900246,This article contains information about Relevance for information retr...,No Preview Available
pdf945.pdf,https://ciir-publications.cs.umass.edu/getpdf.php?id=1407,"A Deep Look into Neural Ranking Models for
Information Retrieval",No Preview Available
pdf946.pdf,https://link.springer.com/article/10.1007/s10791-021-09398-0,Neural Ranking Models,No Preview Available
pdf947.pdf,https://dl.acm.org/doi/10.1145/3397271.3401424,This is a paper by Prof. Zhai regarding the mechanism and evaluation o...,interactive/conversational information retrieval
pdf948.pdf,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/ftir-online-evaluation-final-journal.pdf,Related to Lecture #L10.6,Online Evaluation for Information Retrieval
pdf949.pdf,http://ignited.in/I/a/306132,Related to lecture #L10.5,A Review of Information Seeking Behaviour (ISB) and its Models
pdf950.pdf,https://amitness.com/2020/08/information-retrieval-evaluation/,#L10.6 A short article that introduces various methods to perform eval...,Evaluation Metrics For Information Retrieval
pdf952.pdf,https://arxiv.org/abs/1601.04605,L#10.4 A paper that presents the theoretical frameworks and applicatio...,Theoretical frameworks like the Probability Ranking Principle and its more recent Interactive Information Retrieval variant have guided the development of ranking and retrieval algorithms for decad...
pdf953.pdf,https://dl.acm.org/doi/10.1145/2911451.2911543,"#L10.3 A paper that proposes a novel expansion of the ICM model, with ...","The Interface Card model is a promising new theoretical framework for modeling and optimizing interactive retrieval interfaces, but how to systematically instantiate it to solve concrete interface ..."
pdf954.pdf,https://dl.acm.org/doi/10.1145/2911451.2911543,"#L10.3 A paper that proposes a novel expansion of the ICM model, with ...","The Interface Card model is a promising new theoretical framework for modeling and optimizing interactive retrieval interfaces, but how to systematically instantiate it to solve concrete interface ..."
pdf955.pdf,https://content.iospress.com/articles/education-for-information/efi00699,#L10.2 A paper that talks about the IR game and its educational merits,Computer-supported learning environment (CSLE) Information Retrieval Game (IR Game) is described and evaluated. The IR Game is based on the idea that test collections used in laboratory-based IR e...
pdf956.pdf,https://www.cambridge.org/core/books/abs/interactive-information-seeking-behaviour-and-retrieval/interactive-information-retrieval-history-and-background/E1D59A73006C27FF68EFE87B62049E93,#10 History and background of interactive IR,Interactive information retrieval: history and background
pdf957.pdf,https://pages.gseis.ucla.edu/faculty/bates/berrypicking.html,L10.1 An article introducing the frameworks of Bate's berry picking me...,"THE DESIGN OF BROWSING AND BERRYPICKING TECHNIQUES
FOR THE ONLINE SEARCH INTERFACE"
pdf958.pdf,https://www.researchgate.net/publication/261960300_On_the_application_of_game_mechanics_in_information_retrieval,A paper on application of game mechanics in information retrieval #L10...,application of game mechanics in information retrieval
pdf959.pdf,https://www.researchgate.net/publication/264148319_Interactive_Information_Retrieval_An_Introduction,This is a paper on Interactive Information Retrieval #L10.1,Interactive Information Retrieval: An Introduction
pdf960.pdf,https://en.wikipedia.org/wiki/Learning_to_rank,Introduction of Learning to rank,Learning to rank and machine-learned ranking (MLR)
pdf961.pdf,https://dl.acm.org/doi/10.1561/1500000016,Learning to Rank for Information Retrieval#L9.1,Learning to Rank for Information Retrieval
pdf962.pdf,https://sites.google.com/view/cair-ws/,Conversational Approaches to Information Retrieval #L10,Conversational Approaches to Information Retrieval
pdf963.pdf,https://notesonai.com/Conversational Information Retrieval,Introduction of Conversational Information Retrieval #L10,Conversational Information Retrieval
pdf964.pdf,https://www.microsoft.com/en-us/research/publication/recent-advances-in-conversational-information-retrieval/,"Recent Advances in Conversational Information Retrieval
#L10",Recent Advances in Conversational Information Retrieval
pdf965.pdf,https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=8920,An overview of interactive IR #L10,An overview of interactive IR #L10
pdf966.pdf,https://en.wikipedia.org/wiki/N-gram,#L2.2 N gram,Wiki page for N gram model
pdf967.pdf,https://code.google.com/archive/p/word2vec/,#L8.1 word2vec,the official page on google's word embedding. Gives instruction on training process and performance
pdf968.pdf,https://arxiv.org/abs/1810.04805,#L8.2 Bert,"Bert paper, given some more incite of the model"
pdf969.pdf,https://www.inf.unibz.it/~ricci/ISR/papers/p41-marchionini.pdf,Marchionini’s Typology of Search Tasks #10,No Preview Available
pdf970.pdf,https://www.semanticscholar.org/paper/Cognitive-Perspectives-of-Information-Retrieval-of-Ingwersen/294f4ded016f8f22ec9c698a34684c295a56eb73,#L10.5 Elements of a Cognitive IR theory,The objective of the paper is to amalgamate theories of text retrieval from various research traditions into a cognitive theory for information retrieval interaction.
pdf971.pdf,https://towardsdatascience.com/ranking-algorithms-know-your-multi-criteria-decision-solving-techniques-20949198f23e,#L10.4 Ranking algorithms an optimization,Let’s go through some of the basic algorithms to solve complex decision-making problems influenced by multiple criteria. We will discuss why we need such techniques and explore available algorithms...
pdf972.pdf,https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=8920,#L10.3 Interactive information retrieval an overview,"This was an endeavour for making more effective and to utilize the current
modernized regulatory databases in supplanting the costly printed card catalogue"
pdf973.pdf,https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13053,#L10.2 Semantic memory Word game: cooperative game model,"Considerable work during the past two decades has focused on modeling the structure of semantic memory,"
pdf974.pdf,https://arxiv.org/abs/2201.05176,#L10.1 Neural approaches to conversational information retrieval,A conversational information retrieval (CIR) system is an information retrieval (IR) system with a conversational interface which allows users to interact with the system to seek information via mu...
pdf975.pdf,https://link.springer.com/article/10.1007/s10791-008-9045-0,A paper on probability ranking principle for IIR #L10.4,A probability ranking principle for interactive information retrieval
pdf976.pdf,https://dl.acm.org/doi/10.1145/2766462.2767761,This is the paper related to IR as card playing #L10.3,Information Retrieval as Card Playing
pdf977.pdf,https://library.georgetown.edu/tutorials/research-guides/15-steps,15 Steps to Good Research #L9.2,15 Steps to Good Research
pdf978.pdf,https://www.nhcc.edu/academics/library/doing-library-research/basic-steps-research-process,Basic Steps in the Research Process #L9.2,Basic Steps in the Research Process
pdf979.pdf,https://libguides.elmira.edu/research,How to Do Research #L9.2,How to Do Research: A Step-By-Step Guide: Get Started
pdf980.pdf,https://reader.elsevier.com/reader/sd/pii/0020027173900636?token=2801556F2A16C99236C4BF340B1B8911D9C9F5BEE3486189033D6DABF6FD76F2EEF737615B8A474690B0D1B8A6D6CD48,Application of Bayesian decision theory to interactive information ret...,No Preview Available
pdf981.pdf,https://dl.acm.org/doi/pdf/10.1145/2766462.2767761,"The original paper on Interface Card Model (ICM), discussed in #L10.3",No Preview Available
pdf982.pdf,https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=8920,"An overview of interactive information retrieval, related to #L10.1",No Preview Available
pdf983.pdf,https://arxiv.org/pdf/2112.04426.pdf,"The paper on Retrieval-Enhanced Transformer (RETRO), mentioned in #L11",No Preview Available
pdf984.pdf,https://dl.acm.org/doi/pdf/10.1145/2766462.2767761,"The original paper on Interface Card Model (ICM), discussed in #L10",No Preview Available
pdf985.pdf,https://arxiv.org/abs/1809.02413,A paper that explains challenges for measuring usefulness of IIR syste...,Challenges for Measuring Usefulness of Interactive IR Systems
pdf986.pdf,https://openproceedings.org/2009/conf/edbt/MishraK09.pdf,A paper on Interactive Query Refinement #L10.5,Interactive Query Refinement
pdf987.pdf,https://notesonai.com/Conversational Information Retrieval,Conversational Information Retrieval L10.1,No Preview Available
pdf988.pdf,https://arxiv.org/abs/2102.11903,#L11 Neural ranking models for information retrieval,Ranking models are the main components of information retrieval systems. Several approaches to ranking are based on traditional machine learning algorithms using a set of hand-crafted features.
pdf989.pdf,https://amitness.com/2020/08/information-retrieval-evaluation/,#L10.6 Evaluation metrics for information retrieval,"Most software products we encounter today have some form of search functionality integrated into them. We search for content on Google, videos on YouTube, products on Amazon, messages on Slack, ema..."
pdf990.pdf,https://proceedings.neurips.cc/paper_files/paper/2002/file/51de85ddd068f0bc787691d356176df9-Paper.pdf,"Ranking with Large Margin Principle: Two
Approaches*",A good supplementary talking about the special two approaches in ranking and it's easy to understand. #L9.1
pdf991.pdf,https://www.cwauthors.com/article/novelty-effect-how-to-ensure-your-research-ideas-are-original-and-new,#L9.2 Ensure novelty,No Preview Available
pdf992.pdf,https://en.wikipedia.org/wiki/Ranking_SVM,#L9.1 Ranking SVM algorithm,No Preview Available
pdf993.pdf,https://arxiv.org/pdf/2102.11903v1.pdf,Background reading I found useful for learning more about neural ranki...,L#11: NEURAL RANKING MODELS FOR DOCUMENT RETRIEVAL
pdf994.pdf,https://ils.unc.edu/courses/2017_fall/inls509_002/papers/FnTIR-Press-Kelly.pdf,Background reading that I found useful to learn more about IIR,L#10: Methods for Evaluating Interactive Information Retrieval Systems with Users
pdf995.pdf,https://link.springer.com/article/10.1007/s10791-021-09398-0,#L11 Neural Ranking Models,Neural ranking models for document retrieval
pdf996.pdf,https://www.site.uottawa.ca/~bochmann/Projects/how-to-do-good-research/index.html,"A guide on how to do good research, related to #L9.2",No Preview Available
pdf997.pdf,https://dl.acm.org/doi/pdf/10.1145/2911451.2911469,"A framework for the simulation of interactive information retrieval, r...",No Preview Available
pdf998.pdf,https://reader.elsevier.com/reader/sd/pii/S0306457304001001?token=40134DA7C60B31C3DB9CD68CAF761ECC30896059890DFE75ADE4CC95DF6185737D09A809742CE9E5A9A71FA8EDEDCBBD,"An implicit feedback approach for interactive
information retrieval, ...",No Preview Available
pdf999.pdf,https://link.springer.com/referenceworkentry/10.1007/978-0-387-39940-9_930,A brief explanation of the probability ranking principle mentioned in ...,No Preview Available
pdf1000.pdf,https://arxiv.org/pdf/1903.06902.pdf,A Deep Look into Neural Ranking Models for Information Retrieval #11,"The paper gives an overview of neural ranking models, analyzing their assumptions, design principles, learning strategies, and performance. #L11"
pdf1001.pdf,https://arxiv.org/pdf/1903.06902.pdf,A Deep Look into Neural Ranking Models for Information Retrieval,"This paper gives  an overview of neural ranking models, analyzing their assumptions, design principles, learning strategies, and performance.#11"
pdf1002.pdf,https://core.ac.uk/download/pdf/158302276.pdf,INFORMATION RETRIEVAL AS CARD PLAYING: A FORMAL MODEL FOR OPTIMIZING I...,This paper proposes a new model to optimize interactive information retrieval by selecting interface cards to minimize effort and maximize user benefits.#10
pdf1003.pdf,https://ai.stackexchange.com/questions/35023/what-is-the-difference-between-a-loss-function-and-reward-penalty-in-deep-reinfo,What is the difference between a loss function and reward/penalty in D...,StackExchange discussion about the loss function and reward/penalty with examples and equations. Easy to understand. #L10.6
pdf1004.pdf,https://towardsdatascience.com/loss-functions-and-their-use-in-neural-networks-a470e703f1e9,Loss Functions and Their Use In Neural Networks,"A science blog that introduces loss function overview and MSE, MAE, binary cross-entropy/Log loss. #L10.5"
pdf1005.pdf,https://ielab.io/files/1-bemchi-tutorial-introduction.pdf,Building Economic Models ofHuman-Computer Interaction,A good slide talking about the economic models in human-computer interactions and how to build and apply it.#L10.4
pdf1006.pdf,"https://www.semanticscholar.org/paper/Interactive-Information-Retrieval:-Models,-and-Zhai/a1f92891d79d55a6f2b666948ca079f6bfd3b8a0","Interactive Information Retrieval: Models, Algorithms, and Evaluation","Blog taking about many introductions and applications, algorithms of IIR. #L10.3"
pdf1007.pdf,https://link.springer.com/chapter/10.1007/978-3-540-75171-7_1,Introduction to Bayesian Methods and Decision Theory,A good supplementary for Bayesian and decision theory in interactive retrieval. #L10.2
pdf1008.pdf,https://www.researchgate.net/publication/264148319_Interactive_Information_Retrieval_An_Introduction,Interactive Information Retrieval: An Introduction,A clear understanding from ResearchGate about the introduction of IIR. #L10.1
pdf1009.pdf,https://www.researchgate.net/post/What-is-the-meaning-of-the-word-optimization-in-research,What is the meaning of the word 'optimization' in research ?,A discussion in ResearchGate talking about the meaning of optimization in research and is relevant to #L9.2
pdf1010.pdf,https://mediaspace.illinois.edu/media/t/1_xyj0pkjd,This is the first lecture for CS510,CS510 L1.1: Topic Overview
pdf1011.pdf,https://en.wikipedia.org/wiki/Maximum_likelihood_estimation,#L1.2  A useful wikipedia page about Maximum likelihood estimation.,Maximum likelihood estimation
pdf1012.pdf,https://mediaspace.illinois.edu/media/t/1_x5ndj6p4,CS510 L2.3: N-Gram Language Models: Part 2,CS510 L2.3: N-Gram Language Models: Part 2
pdf1013.pdf,https://mediaspace.illinois.edu/media/t/1_yssl4cre,CS510 L2.2: N-Gram Language Models: Part 1,CS510 L2.2: N-Gram Language Models: Part 1
pdf1014.pdf,https://mediaspace.illinois.edu/media/t/1_syf5f0jp,CS510 L2.1: Overview of Statistical Language Models,CS510 L2.1: Overview of Statistical Language Models
pdf1015.pdf,https://mediaspace.illinois.edu/media/t/1_2p98ojr7,CS510 L1.3: Information Theory,CS510 L1.3: Information Theory
pdf1016.pdf,https://mediaspace.illinois.edu/media/t/1_jffu2cy7,CS510 L1.2: Probability and Statistics,CS510 L1.2: Probability and Statistics
pdf1017.pdf,https://mediaspace.illinois.edu/media/t/1_xyj0pkjd,CS510 L1.1: Topic Overview,CS510 L1.1: Topic Overview
pdf1018.pdf,https://learn.illinois.edu/mod/page/view.php?id=7642964,cs510 weekly schedule Spring 2023,Weekly Schedule (Anticipated)
pdf1019.pdf,https://learn.illinois.edu/course/view.php?id=72674,CS510 Spring 2023 homepage on Moodle,"CS 510 A1 SP23: Advanced Information Retrieval (Zhai, C)"
pdf1020.pdf,https://ieeexplore.ieee.org/document/8257905,Paper about TextScope discussed in #L1.1,TextScope: Enhance human perception via text mining
pdf1021.pdf,https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1,"#L1.2: A helpful explanation of the concept of ""Maximum likelihood est...",Probability concepts explained: Maximum likelihood estimation
pdf1022.pdf,https://www.engati.com/glossary/statistical-language-modeling,"#L1.1: A helpful explanation of the topic ""Statistical Language Models...",Statistical Language Modeling
pdf1023.pdf,https://towardsdatascience.com/maximum-likelihood-vs-bayesian-estimation-dd2eb4dfda8a,Discussion of the difference between Maximum Likelihood and Bayesian E...,"Maximum Likelihood vs. Bayesian Estimation
A comparison of parameter estimation methods"
pdf1024.pdf,https://jxmo.io/posts/retrieval,A useful explanation of text retrieval in lecture #L1.1,A Brief Survey of Text Retrieval in 2022
pdf1025.pdf,https://learn.illinois.edu/course/view.php?id=72674,Intro page to CS510,No Preview Available
pdf1026.pdf,https://learn.illinois.edu/course/view.php?id=72674,Weekly proof-of-learning submission,"After watching the lecture videos in each week (we strongly recommend you to do that by Friday of the week), you are required to make a proof-of-learning submission using the Community Digital Libr..."
pdf1027.pdf,https://learn.illinois.edu/course/editsection.php?id=1946945,Weekly proof-of-learning,No Preview Available
pdf1028.pdf,https://learn.illinois.edu/course/editsection.php?id=1946945,proof-of-learning submission instructions,weekly proof of learning
pdf1029.pdf,https://openai.com/,Open AI homepage,No Preview Available
pdf1030.pdf,https://towardsdatascience.com/enhancing-human-perception-f5e6c82baf44,A more detailed explanation and understanding of TextScope covered in ...,"Enhancing Human Perception with ML, AI, IR and NLP"
pdf1031.pdf,https://www.ibm.com/topics/text-mining,a more detailed explanation on techniques used commonly in text mining...,"The process of text mining comprises several activities that enable you to deduce information from unstructured text data. Before you can apply different text mining techniques, you must start with..."
pdf1032.pdf,https://www.linguamatics.com/what-text-mining-text-analytics-and-natural-language-processing,Clear introduction of text mining/analysis mentioned in #L1.1,"What is Text Mining, Text Analytics and Natural Language Processing?"
pdf1033.pdf,https://www.youtube.com/watch?v=XepXtl9YKwc&t=311s,A great video with example that explains what is maximum likelihood es...,"If you hang out around statisticians long enough, sooner or later someone is going to mumble ""maximum likelihood"" and everyone will knowingly nod. After this video, so can you!
Also, some viewers ..."
pdf1034.pdf,https://agustinus.kristia.de/techblog/2017/01/01/mle-vs-map/,A good explanation on similarities and differences of MLE vs MAP in #L...,"Comparing both MLE and MAP equation, the only thing differs is the inclusion of prior 
P
(
θ
)
P(θ) in MAP"
pdf1035.pdf,https://www.fortinet.com/resources/cyberglossary/spam-filters,"A great website talking about spam filter, which is covered in #L1.1. ...","How Does a Spam Filter Work?
Spam filters all have the same basic objective: to keep unwanted emails out of users’ inboxes. However, there are several different types of spam filters, and they eac..."
pdf1036.pdf,https://bytepawn.com/cross-entropy-joint-entropy-conditional-entropy-and-relative-entropy.html,Further explanation about entropies and their relationship #L1.3,"Cross entropy, joint entropy, conditional entropy and relative entropy"
pdf1037.pdf,https://www.statlect.com/asymptotic-theory/empirical-distribution,"#L1.3: A helpful explanation to the concept of ""Empirical distribution...",Empirical distribution
pdf1038.pdf,https://plato.stanford.edu/entries/bayes-theorem/,A useful explanation about Bayes' Rule in #L1.2,Bayes’ Theorem
pdf1039.pdf,https://monkeylearn.com/blog/natural-language-processing-challenges/,A useful explanation about challenges of NLP covered in #L1.1,Major Challenges of Natural Language Processing (NLP)
pdf1040.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,This explains a topic modelling technique called LDA. From the documen...,#L1.1 Latent Dirichlet Allocation (LDA)
pdf1041.pdf,https://online.stat.psu.edu/stat415/lesson/23/23.2,They key difference is the assumptions one makes about a parameter - i...,#L1.2 key difference between frequentist statisticians and Bayesian statisticians
pdf1042.pdf,https://jihongju.github.io/2019/08/24/ml-me-duality/,This describes the relationship between entropy and maximum likelihood...,#L1.3 The Maximize Entropy and Maximize Likelihood duality states that the two problems are convex duals of each other.
pdf1043.pdf,https://www.ibm.com/topics/natural-language-processing,This page explains what NLP is and showcases popular NLP tools and met...,What is natural language processing?
pdf1044.pdf,https://www.youtube.com/watch?v=ErfnhcEV1O8,"Youtube video introducing relationship among Entropy, Cross-Entropy an...","A Short Introduction to Entropy, Cross-Entropy and KL-Divergence"
pdf1045.pdf,https://machinelearningmastery.com/a-gentle-introduction-to-method-of-lagrange-multipliers/,Clear introduction with examples of Lagrange Multipliers involved in M...,A Gentle Introduction To Method Of Lagrange Multipliers
pdf1046.pdf,https://medium.com/@monadsblog/the-kullback-leibler-divergence-5071c707a4a6,A detailed explanation of Kullback-Leibler divergence with examples an...,"The Kullback-Leibler divergence
A statistical distance to compare two distributions"
pdf1047.pdf,https://quantdare.com/what-is-mutual-information/,"A great tutorial about mutual information(discussed in #L1.3), which c...","The main difference is that correlation is a measure of linear dependence, whereas mutual information measures general dependence (including non-linear relations)."
pdf1048.pdf,https://machinelearningmastery.com/what-is-information-entropy/,More introduction and discussion of Information Entropy covered in Lec...,A Gentle Introduction to Information Entropy
pdf1049.pdf,https://medium.datadriveninvestor.com/maximum-likelihood-estimation-v-s-bayesian-estimation-bfac171a8b85,The differences between Maximum Likelihood and Bayesian Estimation in ...,Maximum Likelihood Estimation v.s. Bayesian Estimation
pdf1050.pdf,https://jxmo.io/posts/retrieval,#L1.1 Good explanation and background about the Text  Retrieval,"Text retrieval is a fast-growing field, and this post will try to summarize some of the most important ideas in the area. We’ll inevitably leave some important things out but hopefully the outline ..."
pdf1051.pdf,https://rpubs.com/leomak/TextPrediction_KBO_Katz_Good-Turing,This document summarizes the various smoothing techniques we learned i...,#L2.3: The backoff smoothing is to approximate the probability of an unseen N-gram by resorting to more frequent lower order N-grams.
pdf1052.pdf,https://www.youtube.com/watch?v=WYO8Rc4JB_Y,"This video explains Zipf's law. If we find the counts of words, rank t...",#L2.2: Zipf's law is that rank * frequency = constant
pdf1053.pdf,https://www.tutorialspoint.com/what-are-the-methods-of-text-retrieval,#L1.1,"Text retrieval is the process of transforming unstructured text into a structured format to identify meaningful patterns and new insights. By using advanced analytical techniques, including Naïve B..."
pdf1054.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,This book chapter talks all about n-gram language models. To calculate...,#L2.1: computing P(w|h) for n-gram language models
pdf1055.pdf,https://www.youtube.com/watch?v=1R34AAHa-2U,#L1.2 An example of Maximum Likelihood Estimation calculation with exp...,No Preview Available
pdf1056.pdf,https://thinkml.ai/what-are-the-natural-language-processing-challenges-and-how-to-fix-them/,#L1.1 A brief introduction of why NLP is difficult and its challenges,No Preview Available
pdf1057.pdf,http://www.cs.cmu.edu/~roni/10601-slides/info-theory.pdf,Addition to Information Theory. Same concepts with examples,No Preview Available
pdf1058.pdf,https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616,"A brief explanation on cross-entropy; what is cross-entropy, how it wo...","A brief explanation on cross-entropy; what is cross-entropy, how it works, and example code"
pdf1059.pdf,https://www.quora.com/What-is-the-difference-between-probability-and-likelihood-1/answer/Jason-Eisner?share=cbfeda82,What is the difference between probability and likelihood? #L1.2,"Suppose you have a probability model with parameters θ.
p(x | θ) has two names.
It can be called the probability of x (given θ),
or the likelihood of θ (given that x was observed)."
pdf1060.pdf,https://brilliant.org/wiki/maximum-likelihood-estimation-mle/,An MLE explanation with example #L1.2,Maximum Likelihood Estimation (MLE)
pdf1061.pdf,https://www.ibm.com/topics/text-mining,A useful explanation of text mining in Lecture #L1.1,What is text mining?
pdf1062.pdf,https://arxiv.org/abs/1212.2065,A comprehensive survey that introduces text information retrieval #L1....,No Preview Available
pdf1063.pdf,https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained,"The website describes a scenario, where given empirical probability di...",#L.3 Explaining KL-Divergence and Entropy with an Example
pdf1064.pdf,https://spia.uga.edu/faculty_pages/rbakker/pols7014/MLE-mini-lecture.pdf,This lecture gives a short introduction on MLEs. It uses a few easy-to...,#L1.2 Maximum Likelihood Estimators and Why
pdf1065.pdf,http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf,KL divergence can also be calculated between two distributions defined...,The continuous version of the KL divergence
pdf1066.pdf,https://www.engati.com/glossary/statistical-language-modeling,This website gives a brief introduction on what are statistical langua...,#L1.1 Understanding Statistical Language Models at a High Level
pdf1067.pdf,https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained,"#L1.3 Information loss, optimizing and entropy of Kullback-Leibler Div...",KL Divergence has its origins in information theory. The primary goal of information theory is to quantify how much information is in data. The most important metric in information theory is called...
pdf1068.pdf,https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained,L1.3 Information lost and optimizing using Kullback-Leibler Divergence,KL Divergence has its origins in information theory. The primary goal of information theory is to quantify how much information is in data. The most important metric in information theory is called...
pdf1069.pdf,https://www.freecodecamp.org/news/bayes-rule-explained/,#L1.2  Some examples applied Bayes' Rule and conditional probability,"Bayes' Rule lets you calculate the posterior (or ""updated"") probability. This is a conditional probability. It is the probability of the hypothesis being true, if the evidence is present.

Think ..."
pdf1070.pdf,https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8,A very good guide on understanding KL - Divergence #L1.3,No Preview Available
pdf1071.pdf,https://medium.datadriveninvestor.com/maximum-likelihood-estimation-v-s-bayesian-estimation-bfac171a8b85,Advantages and Disadvantages of Bayesian Estimation and Maximum Likely...,No Preview Available
pdf1072.pdf,https://monkeylearn.com/blog/natural-language-processing-challenges/,Difficulties of NLP #L1.1,Homonyms – two or more words that are pronounced the same but have different definitions – can be problematic for question answering and speech-to-text applications because they aren’t written in t...
pdf1073.pdf,https://medium.com/swlh/text-retrieval-vs-database-retrieval-3d13965ea067,Difference between Text Retrieval and Database Retrieval #L1.1,No Preview Available
pdf1074.pdf,https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8,#L1.3 An example of information theory: Decision Tree,No Preview Available
pdf1075.pdf,https://en.wikipedia.org/wiki/Shannon's_source_coding_theorem,The theorem that relates entropy to limits of compression #L1.3,"In information theory, Shannon's source coding theorem (or noiseless coding theorem) establishes the limits to possible data compression, and the operational meaning of the Shannon entropy."
pdf1076.pdf,https://www.youtube.com/watch?v=HZGCoVF3YvM,A visual explanation of Bayes' theorem #L1.2,No Preview Available
pdf1077.pdf,https://ocw.mit.edu/courses/18-02sc-multivariable-calculus-fall-2010/ebbeb8e61827a8058d2c45b674d003b3_MIT18_02SC_notes_22.pdf,A simple proof of why Lagrange multipliers work #L1.2,No Preview Available
pdf1078.pdf,https://people.ischool.berkeley.edu/~hearst/papers/acl99/acl99-tdm.html,A detailed discussion about the scope and applications of text mining ...,No Preview Available
pdf1079.pdf,https://wandb.ai/sauravmaheshkar/cross-entropy/reports/What-Is-Cross-Entropy-Loss-A-Tutorial-With-Code--VmlldzoxMDA5NTMx,A tutorial about Cross Entropy Loss with code #L1.3,What Is Cross Entropy Loss?
pdf1080.pdf,https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f,Lecture #L1.2 -- Implementation of MLE in python,Implementing MLE in your data science modeling pipeline
pdf1081.pdf,https://towardsdatascience.com/probability-concepts-explained-introduction-a7c0316de465,Supplementary reading for probability concepts - Lecture L#1.2,Probability concepts explained
pdf1082.pdf,https://www.rosoka.com/blog/10-biggest-issues-natural-language-processing-nlp,#L1.1 Overview of NLP and its challenges,No Preview Available
pdf1083.pdf,https://data-science-ua.com/blog/nlp-achievements-trends-and-challenges/,"#L1.1 A brief overview of benefits, applications, and challenges of Na...",No Preview Available
pdf1084.pdf,https://math.stackexchange.com/questions/549887/bayes-theorem-with-multiple-random-variables,Bayes Theorem with multiple conditions #L1.2,No Preview Available
pdf1085.pdf,https://cs.nyu.edu/~davise/ai/ambiguity.html,Some more examples of ambiguity that NLP has to take into account from...,No Preview Available
pdf1086.pdf,https://www.tutorialspoint.com/what-are-the-methods-of-text-retrieval,A  discussion of various methods of Text Retrieval covered in Lecture ...,What are the methods of Text Retrieval?
pdf1087.pdf,https://www.analyticsinsight.net/techniques-and-applications-of-text-analytics/,A useful explanation of techniques and applications covered in Lecture...,Techniques and Applications of Text Analytics
pdf1088.pdf,https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint,Introduction and examples of Lagrange multipliers #L1.2,The Lagrange multiplier technique lets you find the maximum or minimum of a multivariable function when there is some constraint on the input values you are allowed to use.
pdf1089.pdf,https://quantdare.com/what-is-mutual-information/,Mutual Information #L1.3,"The Mutual Information between two random variables measures non-linear relations between them. Besides, it indicates how much information can be obtained from a random variable by observing anothe..."
pdf1090.pdf,https://link.springer.com/referenceworkentry/10.1007/978-0-387-39940-9_417,the defination of teext retrieval,Text retrieval (also called document retrieval) is a branch of information retrieval in which the information is stored primarily in the form of text. Text retrieval is defined as the matching of s...
pdf1091.pdf,https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence,An interesting discussion about the kl divergence and cross entrophy...,"From the equation, we could see that KL divergence can depart into a Cross-Entropy of p and q (the first part), and a global entropy of ground truth p (the second part).

In many machine learning..."
pdf1092.pdf,https://www.promptcloud.com/blog/best-applications-of-text-mining-analysis/,other applications of text data #L1.1,Applications of Text Data Mining and Analysis
pdf1093.pdf,https://www.oracle.com/big-data/what-is-big-data/,"A detailed explanation of big data presented in L 1.1
#L1.1",What is Big Data
pdf1094.pdf,https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c,An explanation of the properties that entropy has. #L1.3,No Preview Available
pdf1095.pdf,https://stats.stackexchange.com/questions/74082/what-is-the-difference-in-bayesian-estimate-and-maximum-likelihood-estimate,Maximum Likelihood vs. Bayesian #L1.2,Maximum Likelihood vs. Bayesian #L1.2
pdf1096.pdf,https://towardsdatascience.com/recommendation-systems-explained-a42fc60591ed,Recommender Systems #L1.1,Recommender Systems #L1.1
pdf1097.pdf,https://towardsdatascience.com/what-is-cross-entropy-3bdb04c13616,Cross entropy better explained #L1.3,Cross Entropy #L1.3
pdf1098.pdf,https://en.wikipedia.org/wiki/Entropy_(information_theory),A useful explanation of entropy covered in Lecture #L1.3,No Preview Available
pdf1099.pdf,https://en.wikipedia.org/wiki/Information_theory,A useful explanation of information theory covered in Lecture #L1.3,No Preview Available
pdf1100.pdf,https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348,This article gives an introduction to Bayes Rule and how we can use it...,Probability concepts explained: Bayesian inference for parameter estimation.
pdf1101.pdf,https://link.springer.com/chapter/10.1007/978-1-84996-226-1_4,This paper describes the concepts of Information Retrieval and Text Mi...,Information Retrieval and Text Mining
pdf1102.pdf,https://stats.stackexchange.com/questions/112451/maximum-likelihood-estimation-mle-in-layman-terms,Explanation of Maximum Likelihood Estimation #L1.2,"The maximum likelihood (ML) estimate of a parameter is the value of that parameter under which your actual observed data are most likely, relative to any other possible values of the parameter."
pdf1103.pdf,https://www.researchgate.net/publication/221397810_Using_Kullback-Leibler_Distance_for_Text_Categorization,This paper explains how to use KL Divergence for Text Categorization #...,Using Kullback-Leibler Distance for Text Categorization
pdf1104.pdf,https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/,This article gives an intuitive introduction to Maximum Likelihood Est...,A Gentle Introduction to Maximum Likelihood Estimation for Machine Learning
pdf1105.pdf,https://towardsdatascience.com/a-guide-text-analysis-text-analytics-text-mining-f62df7b78747,"A basic guide regarding what is Text Analysis, Text Analytics and Text...","A Guide: Text Analysis, Text Analytics & Text Mining"
pdf1106.pdf,https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348,Bayes' Theorem and Bayesian Inference #L1.2,Bayesian inference for parameter estimation
pdf1107.pdf,https://towardsdatascience.com/what-is-bayes-rule-bb6598d8a2fd,Bayes Rule definition with an example. #L1.2,No Preview Available
pdf1108.pdf,https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-are-probabilistic-models,Expalanation about probabilistic models #L1.2,Probabilistic Models in Machine Learning
pdf1109.pdf,https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348,Bayesian Inference for parameter estimation,No Preview Available
pdf1110.pdf,https://www.britannica.com/science/statistics/Random-variables-and-probability-distributions,#L1.2 A useful resource to understand random variables in probability ...,A random variable is a numerical description of the outcome of a statistical experiment.
pdf1111.pdf,https://developer.twitter.com/en/use-cases/do-research/academic-research/resources,Official Twitter documentation for how to use tweets data for knowledg...,Learn the fundamentals of using Twitter data for academic research
pdf1112.pdf,https://towardsdatascience.com/kl-divergence-python-example-b87069e4b810,A useful introduction K-L Divergence with Python code and graphs #L1.3,No Preview Available
pdf1113.pdf,https://gaussian37.github.io/ml-concept-infomation_theory/,"A clear explanation about cross entropy, KL divergence, and mutual inf...",No Preview Available
pdf1114.pdf,https://www.rob-mcculloch.org/2019_ml/webpage/notes19/mle-bic/mle-opt_494_2019-10-17.pdf,how to apply Lagrange multiplier to solve MLE,No Preview Available
pdf1115.pdf,https://ieeexplore.ieee.org/document/8257905,An IEEE talk about TextScope #L1.1,No Preview Available
pdf1116.pdf,https://chrispiech.github.io/probabilityForComputerScientists/en/part5/parameter_estimation/,"A good explanation of parameter estimation, MLE, MAP and other concept...",Parameter Estimation
pdf1117.pdf,https://people.cs.umass.edu/~elm/Teaching/Docs/mutInf.pdf,Describes mutual information with some details and examples,"Mutual information is a quantity that measures a relationship between two
random variables that are sampled simultaneously. In particular, it measures
how much information is communicated, on ave..."
pdf1118.pdf,https://gaussian37.github.io/ml-concept-infomation_theory/,"This page talks about the concepts Entropy,Cross Entropy and KL Diverg...","Information Theory (Entropy, KL Divergence, Cross Entropy)"
pdf1119.pdf,https://medium.com/@benjohnsonlaird/text-classification-with-feature-selection-using-likelihoods-part-2-c28793575cbf,This blog article explains how can we do Text classification using the...,Text classification with feature selection using mutual information
pdf1120.pdf,https://www.alastore.ala.org/sites/default/files/pdfs/chowdhuryIR1.pdf,A useful explanation of how important of information retrieval which c...,No Preview Available
pdf1121.pdf,https://nlp.stanford.edu/IR-book/pdf/11prob.pdf,This research paper explores the use of probability in the field of te...,No Preview Available
pdf1122.pdf,https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence,This text explains the difference bewteen cross-entropy and KL diverge...,You will need some conditions to claim the equivalence between minimizing cross entropy and minimizing KL divergence. I will put your question under the context of classification problems using cro...
pdf1123.pdf,https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1,def of ml estimation,Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by t...
pdf1124.pdf,https://www.ibm.com/topics/text-mining,This article provided a overview of what text mining is and how it is ...,Analyzing vast collections of textual materials to capture key concepts #L1.1
pdf1125.pdf,https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f,Explanation of Maximum Likelihood Estimation #L1.2,A Gentle Introduction to Maximum Likelihood Estimation
pdf1126.pdf,https://machinelearningmastery.com/cross-entropy-for-machine-learning/,"Detail comparison between Cross-Entropy and KL Divergence.

#L1.3","Cross-Entropy Versus KL Divergence
Cross-entropy is not KL Divergence.

Cross-entropy is related to divergence measures, such as the Kullback-Leibler, or KL, Divergence that quantifies how much ..."
pdf1127.pdf,https://bytepawn.com/cross-entropy-joint-entropy-conditional-entropy-and-relative-entropy.html,Different variations of Entropy #L1.3,No Preview Available
pdf1128.pdf,https://www.investopedia.com/terms/b/bayes-theorem.asp,Explanation of Baye's Theorem presented in #L1.2,What Is Bayes' Theorem
pdf1129.pdf,https://machinelearningmastery.com/cross-entropy-for-machine-learning/,Useful resource to understand difference between cross entropy and KL ...,"Cross-entropy is related to divergence measures, such as the KLDivergence that quantifies how much one distribution differs from another."
pdf1130.pdf,https://data-ox.com/data-extraction-vs-data-mining/,It discusses the difference between data mining and information retrie...,"Data mining involves extracting valid information by using advanced approaches like machine learning techniques. However, applying the right algorithm to acquire the necessary knowledge to solve a ..."
pdf1131.pdf,https://machinelearningmastery.com/divergence-between-probability-distributions/,"This website introduces Statistical Distance, Kullback-Leibler Diverge...","Statistical distance is the general idea of calculating the difference between statistical objects like different probability distributions for a random variable.
Kullback-Leibler divergence calcu..."
pdf1132.pdf,https://medium.com/nlplanet/a-brief-timeline-of-nlp-bc45b640f07d,"#L1.1 A brief history of advancement of NLP over the years, problems i...",A Brief Timeline of NLP
pdf1133.pdf,https://yassineelkhal.medium.com/entropy-cross-entropy-mutual-information-and-kullback-leibler-divergence-5e76dadd29b5,"Entropy, Cross entropy, mutual information and KL divergence a detaile...","The universe is overflowing with information. Everything we say, hear, think and see is an information. Everything wraps an information in it, that must follow a certain rule no matter the format...."
pdf1134.pdf,https://gaussian37.github.io/ml-concept-infomation_theory/,"Cross entropy, KL divergence, mutual information","Thinking about the characteristic of entropy, entropy is maximized when all events which have same probability of occurrence."
pdf1135.pdf,https://stats.stackexchange.com/questions/74082/what-is-the-difference-in-bayesian-estimate-and-maximum-likelihood-estimate,Difference between Maximum Likelihood estimation(MLE) and Bayes estima...,It is a very broad question and my answer here only begins to scratch the surface a bit. I will use the Bayes's rule to explain the concepts.
pdf1136.pdf,https://www.cs.cmu.edu/~roni/papers/survey-slm-IEEE-PROC-0004.pdf,Two Decades of Statistical Language Models #L1.1,"Statistical language models are usually used in the context of a Bayes classifier, where they can play the role of
either the prior or the likelihood function."
pdf1137.pdf,https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained,Kullback-Leibler Divergence Explained,Kullback-Leibler Divergence Explained
pdf1138.pdf,https://towardsdatascience.com/an-introduction-to-information-theory-for-data-science-4fcbb4d40878,A useful explanation of the role of Information Theory which covered i...,No Preview Available
pdf1139.pdf,https://towardsdatascience.com/text-classification-using-naive-bayes-theory-a-working-example-2ef4b7eb7d5a,A useful explanation of the role of Bayes rule which covered in Lectur...,No Preview Available
pdf1140.pdf,http://www.ece.tufts.edu/ee/194NIT/lect01.pdf,#L1.3,No Preview Available
pdf1141.pdf,https://www.dummies.com/article/academics-the-arts/math/statistics/probability-for-dummies-cheat-sheet-208653/,#L1.2,No Preview Available
pdf1142.pdf,https://stats.stackexchange.com/questions/487012/are-mutual-information-and-kullback-leibler-divergence-equivalent,A very interesting read related to #L1.3,How can this be given that MI is a metric and DKL is not?
pdf1143.pdf,https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8,a useful explanation of KL divergence introduced in lecture #L1.3,"Intuitively it makes sense to give priority to correctly match the truly highly probable events in the approximation. Mathematically, this allows you to automatically ignore the areas of the distri..."
pdf1144.pdf,https://www.tutorialspoint.com/what-are-the-methods-of-text-retrieval,#L1.1,No Preview Available
pdf1145.pdf,https://towardsdatascience.com/maximum-likelihood-vs-bayesian-estimation-dd2eb4dfda8a,a useful explanation of maximum likelihood and bayesian estimation cov...,"The central idea behind Bayesian estimation is that before we’ve seen any data, we already have some prior knowledge about the distribution it came from. Such prior knowledge usually comes from exp..."
pdf1147.pdf,https://towardsdatascience.com/enhancing-human-perception-f5e6c82baf44,a useful explanation of textscope covered in Lecture #L1.1,"Exactly as you need the microscope, or telescope, or macroscope depending on what you want to analyze of the real world, you need a textscope for studying text. This tool must be an intelligent and..."
pdf1148.pdf,https://m-clark.github.io/models-by-example/maximum-likelihood.html,It is a supplement material that explains Maximum Likelihood with an e...,"This is a brief refresher on maximum likelihood estimation using a standard regression approach as an example, and more or less assumes one hasn’t tried to roll their own such function in a program..."
pdf1149.pdf,https://www.quora.com/How-is-information-theory-related-to-statistics,A high-level read for interesting relations between information theory...,No Preview Available
pdf1150.pdf,https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/4a8de32565ebdefbb7963b4ebda904b2_MIT18_05S14_Reading10b.pdf,A thorough introduction of Maximum Likelihood Estimate for different d...,Maximum Likelihood Estimates
pdf1151.pdf,https://math.stackexchange.com/questions/4217131/relation-between-cross-entropy-and-conditional-entropy,#L1.3 Relation between cross entropy and conditional entropy,Relation between cross entropy and conditional entropy
pdf1152.pdf,https://www.analyticsvidhya.com/blog/2021/06/part-20-step-by-step-guide-to-master-nlp-information-retrieval/,A clear explanation of Information retrieval with models and cons and ...,Step by Step Guide to Master NLP – Information Retrieval
pdf1153.pdf,https://www.aptech.com/blog/beginners-guide-to-maximum-likelihood-estimation-in-gauss/,#L1.2 Introduction to Maximum Likelihood Estimation,Beginner's Guide To Maximum Likelihood Estimation
pdf1154.pdf,https://www.analyticsvidhya.com/blog/2015/04/information-retrieval-system-explained/,Blog about information retrieval. #L1.1,Information Retrieval System explained in simple terms!
pdf1155.pdf,https://www.promptcloud.com/blog/best-applications-of-text-mining-analysis/,Describes the applications of text data as covered in Lecture #1.1,No Preview Available
pdf1156.pdf,https://heartbeat.comet.ml/the-biggest-challenges-in-nlp-and-how-to-overcome-them-93c3c04ae617,#Lec1.1 A brief description of the biggest challenges in NLP and how t...,"words make up text data, however, words and phrases have different meanings depending on the context of a sentence. As humans, from birth, we learn and adapt to understand the context. Although NLP..."
pdf1157.pdf,https://www.youtube.com/watch?v=YtebGVx-Fxw,A video explains entropy in #L1.3,Entropy (for data science) Clearly Explained
pdf1158.pdf,https://towardsdatascience.com/bayesian-method-1-1cbdb1e6b4,A post introduces prior distribution in the Bayesian method of #L1.2,"Bayesian method
The prior distribution"
pdf1159.pdf,https://en.wikipedia.org/wiki/Information_retrieval,The Wikipedia page introduces the information retrieval from #L1.1,Information retrieval
pdf1160.pdf,https://monkeylearn.com/text-analysis/,This explains about the difference between the text analysis and text ...,The Text Analysis vs. Text Mining vs. Text Analytics
pdf1161.pdf,https://www.linkedin.com/pulse/what-type-model-used-text-retrieval-gabriele-maggiolo/,#L1.1,types of model used for text retrieval
pdf1162.pdf,https://www.investopedia.com/terms/r/random-variable.asp,A detailed and useful explanation of random variables covered in Lectu...,No Preview Available
pdf1163.pdf,https://machinelearningmastery.com/what-is-information-entropy/,Method to calculate entropy of a random variable #L1.3,"Calculate the Entropy for a Random Variable
We can also quantify how much information there is in a random variable.

For example, if we wanted to calculate the information for a random variable..."
pdf1164.pdf,https://reliability.readthedocs.io/en/latest/How does Maximum Likelihood Estimation work.html,How does Maximum Likelihood Estimation work #L1.2,How does Maximum Likelihood Estimation work
pdf1165.pdf,https://www.youtube.com/watch?v=pXh53lqPkHs&list=PL05umP7R6ij0bo4UtMdzEJ6TiLOqj4ZCm&index=16,A nice introduction to information theory #L1.3,MaDL - Information and Entropy
pdf1166.pdf,https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8,Explanation about KL divergence #L1.3,Understanding KL Divergence
pdf1167.pdf,https://www.youtube.com/watch?v=DeNFAaA6o_M,"A great inro video about language models(discussed in #L2.1), which co...","NLP - Natural Language Processing
Language Model, 
Unigram, Bigram, 
Trigram, N-Gram, Tokenization"
pdf1168.pdf,https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-17138ffab87b,"A detailed page talking about the difference between Entropy, Cross En...","Entropy, Cross Entropy, and KL Divergence"
pdf1169.pdf,https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a,#L1.3 An easy to understand article on entropies,"Entropy, Cross-Entropy, and KL-Divergence Explained!"
pdf1170.pdf,https://www.analyticsvidhya.com/blog/2020/11/entropy-a-key-concept-for-all-data-science-beginners/,#L1.3 Explains entropy in a very simple language.,No Preview Available
pdf1171.pdf,https://www.formpl.us/blog/text-analytics-for-research-applications-benefits-uses-cases,This article describes text analysis and comapres it with text mining....,Text analysis tools allow you to draw conclusions about a piece of text; it is usually referred to as “data mining”. Text mining tools help you process large volumes of text data quickly by storing...
pdf1172.pdf,https://www.dummies.com/article/technology/information-technology/data-science/general-data-science/a-brief-guide-to-understanding-bayes-theorem-268197/,#L1.2 Brief guide to Bayes theorem,No Preview Available
pdf1173.pdf,https://math.stackexchange.com/questions/1210175/intuitively-why-does-bayes-theorem-work,#L1.2 Explains Bayes theorem working with examples,No Preview Available
pdf1174.pdf,https://learn.illinois.edu/course/view.php?id=72674,test,"on a relevant web page, (optionally) highlight any relevant passage of the page (e.g., the title), and open the CDL Chrome Extension. Click on the ""Submit"" tab on the top of the pop-up window. Your..."
pdf1175.pdf,https://www.geeksforgeeks.org/kullback-leibler-divergence/,"This covers the topic ""Kullback-Leibler Divergence"" discussed in Lectu...",Kullback-Leibler Divergence
pdf1176.pdf,https://analyticsindiamag.com/a-comprehensive-guide-to-maximum-likelihood-estimation-and-bayesian-estimation/,Article explaining MLE and Bayes with diagrams #L1.2,A Comprehensive Guide to Maximum Likelihood Estimation and Bayesian Estimation
pdf1177.pdf,https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained,Explaining the Kullback-Liebler divergence. #L1.3,The key point here is that we can use KL Divergence as an objective function to find the optimal value for any approximating distribution we can come up with.
pdf1178.pdf,https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1,This explains the topic Maximum Likelihood estimation covered in Lectu...,Maximum likelihood estimation
pdf1179.pdf,https://www.youtube.com/watch?v=XepXtl9YKwc,Maximum Likelihood explanation. #L1.2,"Maximum Likelihood, clearly explained!!!"
pdf1180.pdf,https://www.khanacademy.org/science/chemistry/thermodynamics-chemistry,Lec1.3 Some useful resources of Entropy,No Preview Available
pdf1181.pdf,https://www.ischool.berkeley.edu/courses/info/202,Another IR course in UCB #L1.1,No Preview Available
pdf1182.pdf,https://medium.com/analytics-vidhya/the-art-of-converting-hindsight-to-foresight-bayes-theorem-f307f32d2e53,Direct understanding of Bayes Rule #L1.2,conditional probability equation for calculating probability of A given B
pdf1183.pdf,http://www.stat.columbia.edu/~liam/teaching/4107-fall05/notes3.pdf,A useful and thorough explanation of estimation theory related to Lect...,No Preview Available
pdf1184.pdf,https://gnarlyware.com/blog/kl-divergence-online-demo/,Graphical demo of KL divergence #L1.3,KL Divergence Online Demo
pdf1185.pdf,https://towardsdatascience.com/a-guide-text-analysis-text-analytics-text-mining-f62df7b78747,This webpage produced a guideline about text analysis. It provides the...,"A Guide: Text Analysis, Text Analytics & Text Mining #L1.1"
pdf1186.pdf,https://towardsdatascience.com/a-guide-text-analysis-text-analytics-text-mining-f62df7b78747,"This webpage produced a guideline of text analysis, it provides the de...","A Guide: Text Analysis, Text Analytics & Text Mining"
pdf1187.pdf,https://www.statlect.com/fundamentals-of-probability/Kullback-Leibler-divergence,KL-divergence,No Preview Available
pdf1188.pdf,https://www.geeksforgeeks.org/bayes-theorem-in-data-mining/,Bayes theorem's purpose in data mining,"Bayes’ Theorem describes the probability of an event, based on precedent knowledge of conditions which might be related to the event. In other words, Bayes’ Theorem is the add-on of Conditional Pro..."
pdf1189.pdf,https://www.khanacademy.org/math/statistics-probability,#Lec1.2 Some useful resources of probability knowledge,No Preview Available
pdf1190.pdf,https://jxmo.io/posts/retrieval,Overview of Text Retrieval and reserach #L1.1,brief overview of the field of text retrieval
pdf1191.pdf,https://www.youtube.com/watch?v=ErfnhcEV1O8,"Video explanation on  Entropy, Cross-Entropy and KL-Divergence #L1.3","Introduction to Entropy, Cross-Entropy and KL-Divergence"
pdf1192.pdf,https://web.mit.edu/6.933/www/Fall2001/Shannon2.pdf,Information Theory and the Digital Age #L1.3 [PDF],No Preview Available
pdf1193.pdf,https://rpsychologist.com/likelihood/,Understanding Maximum Likelihood: An Interactive Visualization #L1.2,"Understanding Maximum Likelihood
An Interactive Visualization"
pdf1194.pdf,https://ieeexplore.ieee.org/document/8257905,IEEE Paper on TextScope #L1.1,"Recent years have seen a dramatic growth of natural language text data (e.g., web pages, news articles, scientific literature, emails, enterprise documents, blog articles, forum posts, product revi..."
pdf1195.pdf,https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1,Some clarifications of differences between maximum likelihood and prob...,"These expressions are equal! So what does this mean? Let’s first define P(data; μ, σ)? It means “the probability density of observing the data with model parameters μ and σ”. It’s worth noting that..."
pdf1196.pdf,https://www.codingninjas.com/codestudio/library/scope-of-data-mining,"Overviewing scopes in datamining

#L1.1",Prediction of behaviors and trends: Data mining is helpful in the process of predicting information automatically from large databases. It is faster than the traditional way of analysis and is more...
pdf1197.pdf,https://nlp.stanford.edu/IR-book/information-retrieval-book.html,"It includes basic concepts in Information retrieval, it's related to l...",Textbook about Information Retrieval
pdf1198.pdf,https://www.researchgate.net/publication/2282868_Information_Retrieval_and_Information_Theory,The paper establishes connections between information retrieval and in...,No Preview Available
pdf1199.pdf,https://tselab.stanford.edu/mirror/ee376a_winter1617/Lecture_2.pdf,"#L1.3 Information theory lecture note. Summary of entropy, mutual info...",No Preview Available
pdf1200.pdf,https://ieeexplore.ieee.org/document/7339149,Lecture #L1.1 is about a brief review of the purpose of text mining. T...,No Preview Available
pdf1201.pdf,https://towardsdatascience.com/spam-detection-in-emails-de0398ea3b48,spam filter detection #L1.1,"All these tasks are done through Natural Language Processing (NLP), which processes text into useful insights that can be applied to future data. In the field of artificial intelligence, NLP is one..."
pdf1202.pdf,https://web.stanford.edu/~montanar/RESEARCH/BOOK/partA.pdf,Paper on the  fundamental concepts of information theory and probabili...,No Preview Available
pdf1203.pdf,https://cs-people.bu.edu/mbun/courses/591_F19/notes/lec6.pdf,helps understand conditional entropy and mutual information concepts b...,No Preview Available
pdf1204.pdf,https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP,Introduction to NLP,Natural language processing (NLP)
pdf1205.pdf,https://iopscience.iop.org/article/10.1088/1757-899X/226/1/012091/pdf,This paper analyses the naive bayes algorithm for spam filter in multi...,Spam Email Filter using Naive Bayes Algorithm #L1.1
pdf1206.pdf,https://analyticsindiamag.com/a-comprehensive-guide-to-maximum-likelihood-estimation-and-bayesian-estimation/,Detailed explanation and differences between MLE and Bayesian Estimati...,No Preview Available
pdf1207.pdf,https://en.wikipedia.org/wiki/Kullback–Leibler_divergence,Kullback–Leibler divergence explanation #L1.3,Kullback–Leibler divergence
pdf1208.pdf,https://quantdare.com/what-is-mutual-information/,A better interpretation of Mutual Information #L1.3,"The Mutual Information between two random variables measures non-linear relations between them. Besides, it indicates how much information can be obtained from a random variable by observing anothe..."
pdf1209.pdf,https://en.wikipedia.org/wiki/Kullback–Leibler_divergence,Defination and simple interpretation of Kullback-Leibler divergence #L...,"In mathematical statistics, the Kullback–Leibler divergence (also called relative entropy and I-divergence[1]), denoted {\displaystyle D_{\text{KL}}(P\parallel Q)}, is a type of statistical distanc..."
pdf1210.pdf,https://en.wikipedia.org/wiki/Language_model,Definition of Language Model in #L1.1,"A language model is a probability distribution over sequences of words.[1] Given any sequence of words of length m, a language model assigns a probability {\displaystyle P(w_{1},\ldots ,w_{m})} to ..."
pdf1211.pdf,https://en.wikipedia.org/wiki/Language_model,Definition of language model,"A language model is a probability distribution over sequences of words.[1] Given any sequence of words of length m, a language model assigns a probability {\displaystyle P(w_{1},\ldots ,w_{m})} to ..."
pdf1212.pdf,https://www.folio3.ai/blog/why-natural-language-processing-is-difficult/,Why NLP is important and difficult,No Preview Available
pdf1213.pdf,https://en.wikipedia.org/wiki/Kullback–Leibler_divergence,a useful explanation of K-L Divergence covered in Lecture #L1.3,"For discrete probability distributions {\displaystyle P} and {\displaystyle Q} defined on the same sample space, {\displaystyle {\mathcal {X}}}, the relative entropy from {\displaystyle Q} to {\dis..."
pdf1214.pdf,https://en.wikipedia.org/wiki/Von_Neumann_entropy,Lecture #L1.3 is a review of information theory. This page extend the ...,No Preview Available
pdf1215.pdf,https://www.analyticsvidhya.com/blog/2020/11/entropy-a-key-concept-for-all-data-science-beginners/,Clear Understanding of Entropy #L1.3,"The entropy measures the “amount of information” present in a variable. Now, this amount is estimated not only based on the number of different values that are present in the variable but also by t..."
pdf1216.pdf,https://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html,Lecture #L1.2 is a brief overview of probability. This page introduces...,No Preview Available
pdf1217.pdf,https://quantdare.com/what-is-mutual-information/,Useful information about mutual information,No Preview Available
pdf1218.pdf,https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf,This paper includes the full story of Information Theory by Shannon #L...,THE ENTROPY OF AN INFORMATION SOURCE
pdf1219.pdf,https://towardsdatascience.com/text-classification-using-naive-bayes-theory-a-working-example-2ef4b7eb7d5a,text analysis using bayes rule #L1.2,"Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols (i.e. strings) cannot be fed directly to the algorithms themselves as most of..."
pdf1220.pdf,https://towardsdatascience.com/a-quick-introduction-to-text-summarization-in-machine-learning-3d27ccf18a9f,Explains text summarization mentioned in #L1.1,Automatic text summarization is a common problem in machine learning and natural language processing (NLP).
pdf1221.pdf,https://dl.acm.org/doi/abs/10.1145/945546.945549?casa_token=JBFae-lx5_cAAAAA:jNfcNYnmPxIC0R0Ns1EgD9AF-KMVvj0buTehwexVtBUNpHj7fDfkBiFWGvPVpVr7lvlW5Y7RM-4g,Challenges in information retrieval systems composed in the early 2000...,No Preview Available
pdf1222.pdf,https://nlp.stanford.edu/IR-book/pdf/13bayes.pdf,Stanford nlp book on Naive Bayes covered in #L1.2,No Preview Available
pdf1223.pdf,https://towardsdatascience.com/likelihood-probability-and-the-math-you-should-know-9bf66db5241b,#L1.2 Relation of likelihood and probability and their roles in machin...,No Preview Available
pdf1224.pdf,http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html,Article on cross-entropy for lecture #L1.3,"It's like if you'd played the game '20 questions' with your friend Alice so many times that you've got to learn the kind of objects she chooses, and tailored the sequence of questions to her. When ..."
pdf1225.pdf,https://machinelearningmastery.com/what-is-information-entropy/,Provides an introduction to information entropy with code examples,Introduction to Information Entropy in #L1.3
pdf1226.pdf,https://towardsdatascience.com/bayes-theorem-explained-1b501d52ae37,Bayes Theorem explained,Explanation of Bayes' Theorem in #L1.2
pdf1227.pdf,https://www.youtube.com/watch?v=kNkCfaH2rxc&list=PLaZQkZp6WhWwoDuD6pQCmgVyDbUWl_ZUi,Interesting videos on information retrieval #L1.1,Introduction to Information Retrieval
pdf1228.pdf,https://en.wikipedia.org/wiki/Maximum_likelihood_estimation,Maximum likelihood estimation defination and explination #L1.2,Maximum likelihood estimation
pdf1229.pdf,https://en.wikipedia.org/wiki/Bayes'_theorem,Prof of Bayes Theorem in #L1.2,"Bayes' theorem may be derived from the definition of conditional probability:

{\displaystyle P(A\mid B)={\frac {P(A\cap B)}{P(B)}},{\text{ if }}P(B)\neq 0,}
where {\displaystyle P(A\cap B)} is the..."
pdf1230.pdf,https://isip.piconepress.com/courses/msstate/ece_8463/lectures/current/lecture_33/lecture_33.pdf,A great lecture slides about smoothing in N-gram language model(discus...,"Smoothing is the process of flattening a probability distribution implied by a language model so that all
reasonable word sequences can occur with some probability. This often involves broadening ..."
pdf1231.pdf,https://medium.datadriveninvestor.com/maximum-likelihood-estimation-v-s-bayesian-estimation-bfac171a8b85,#1.2 advantages and disadvantages of maximum likelihood and bayesian,"Log-likelihood function:


The partial derivative of every involved variable


Advantages
provides a consistent approach which can be developed for a large variety of estimation situations...."
pdf1232.pdf,https://dibyaghosh.com/blog/probability/kldivergence.html,#L1.3 Article Explaining the Uses of KL Divergence in Different Learni...,This post will talk about the Kullback-Leibler Divergence from a holistic perspective of reinforcement learning and machine learning. You've probably run into KL divergences before: especially if y...
pdf1233.pdf,https://www.analyticsvidhya.com/blog/2020/11/entropy-a-key-concept-for-all-data-science-beginners/,brief in formation about entropy introduced in #1.2,"Entropy is one of the key aspects of Machine Learning. It is a must to know for anyone who wants to make a mark in Machine Learning and yet it perplexes many of us.

The focus of this article is ..."
pdf1234.pdf,https://cci.drexel.edu/bigdata/bigdata2017/files/Keynote_Zhai.pdf,Slides about TextScope,"TextScope:
Enhance Human Perception via Text Mining"
pdf1235.pdf,https://www.britannica.com/science/Bayesian-analysis,#L1.2 Relevant Article With A Simple Illustration Of The Essence Of Ba...,"Bayesian analysis, a method of statistical inference (named for English mathematician Thomas Bayes) that allows one to combine prior information about a population parameter with evidence from info..."
pdf1236.pdf,https://www.youtube.com/watch?v=9_eZHt2qJs4,Youtube video explaining K-L Divergence in #L1.3,No Preview Available
pdf1237.pdf,https://www.mathsisfun.com/data/bayes-theorem.html,Gives basic explanation and calculation example of Bayes' Theorem ment...,Bayes' Theorem
pdf1238.pdf,https://news.berkeley.edu/2016/01/11/will-computers-ever-truly-understand-what-were-saying/,#L1.1 Relevant Article Detailing The Differences Of Language Comprehen...,"“Apple’s Siri focuses on statistical regularities, but communication is not about statistical regularities,” he said. “Statistical regularities may get you far, but it is not how the brain does it...."
pdf1239.pdf,https://web.stanford.edu/~montanar/RESEARCH/BOOK/partA.pdf,#1.2 #1.3 The first several pages of this pdf recapped the concept of ...,INTRODUCTION TO INFORMATION THEORY
pdf1240.pdf,https://www.nowpublishers.com/article/Details/INR-008,"A survey reviewing existing work of SLM for information retrieval, use...","The purpose of this survey is to systematically and critically review the existing work in applying statistical language models to information retrieval, summarize their contributions, and point ou..."
pdf1241.pdf,https://learn.illinois.edu/course/view.php?id=72674,test,test
pdf1242.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,"An useful wiki about smoothing for language Models, which covers all t...","Smoothing

MLE may overfit the data: it will assign 0 probabilities to words it hasn't seen
What to do with it?
Bayesian Parameter Estimation can both maximize the data likelihood and incorpora..."
pdf1243.pdf,https://handwiki.org/wiki/Noisy_text_analytics,Lecture #L1.1: Noise in text data (a reason to convert big text data t...,No Preview Available
pdf1244.pdf,https://towardsdatascience.com/maximum-likelihood-vs-bayesian-estimation-dd2eb4dfda8a,Apply MLE or Bayesian Estimation? Lecture #1.2,When to use MLE? Bayesian estimation?
pdf1245.pdf,https://machinelearningmastery.com/a-gentle-introduction-to-jensens-inequality/,"Jensen's inequality, used in the derivation of property of cross entro...",No Preview Available
pdf1246.pdf,https://jon.dehdari.org/tutorials/lm_overview.pdf,A powerpoint that help introduce the overview of Statistical Language ...,A Short Overview of Statistical Language Models
pdf1247.pdf,https://reader.elsevier.com/reader/sd/pii/S1877050918317939?token=09A7BFDB341BA8154636302DD1A05AD40C91DDB1CB6051F166DC513EA03C473B9E9A0BF40855B9B7C7E2477291537DCF,#L1.3: KL Divergence used in ML applications.,This report made a conclusion about how the KL Divergence is used in ML for various applications.
pdf1248.pdf,https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/,#L2.1: Applications of SLM in bayes classifier,A practical explanation of a Naive Bayes classifier
pdf1249.pdf,https://www.rev.com/blog/resources/understanding-n-gram-language-models,Article containing an example of n-gram models,Understanding N-Gram Language Models
pdf1250.pdf,https://jxmo.io/posts/retrieval,#L1.1,A Brief Survey of Text Retrieval in 2022
pdf1251.pdf,https://www.3pillarglobal.com/insights/blog-posts/document-classification-using-multinomial-naive-bayes-classifier/,Solved example explaining application of Bayes Theorem for text classi...,DOCUMENT CLASSIFICATION USING MULTINOMIAL NAIVE BAYES CLASSIFIER
pdf1252.pdf,https://www.youtube.com/watch?v=_PG-jJKB_do,A video explaining concepts of entropy using the examples mentioned in...,Intuitive explanation of entropy
pdf1253.pdf,https://en.wikipedia.org/wiki/Kneser–Ney_smoothing,"Some further explanation of modification of Kneser-Ney, which consider...",No Preview Available
pdf1254.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,A page about smoothing methods for language models useful for Lecture ...,Smoothing for Language Models
pdf1255.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,"A textbook chapter describing details of n-gram models, relevant to Le...",No Preview Available
pdf1256.pdf,https://en.wikipedia.org/wiki/Information_theory,This wiki article gives a great intro + overview on information theory...,"Information theory is the scientific study of the quantification, storage, and communication of information.[1] The field was originally established by the works of Harry Nyquist and Ralph Hartley,..."
pdf1257.pdf,https://www.math.net/probability-and-statistics,This article gives a great overview and introduction to probability an...,"Probability and statistics are two branches of mathematics concerning the collection, analysis, interpretation, and display of data in the context of random events. They are often studied together ..."
pdf1258.pdf,https://www.tutorialspoint.com/what-are-the-methods-of-text-retrieval,A brief introduction to text retrieval covered in lecture #L1.1,Text retrieval is the process of transforming unstructured text into a structured format to identify meaningful patterns and new insights.
pdf1259.pdf,https://learn.illinois.edu/course/view.php?id=72674,A useful introduction to text retrieval covered in Lecture #L1.1,A useful introduction to text retrieval covered in Lecture #L1.1
pdf1260.pdf,https://en.wikipedia.org/wiki/Natural_language_processing,It's the wikipedia page for NLP #1.1,"Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language..."
pdf1261.pdf,https://arxiv.org/pdf/2010.06705.pdf,"Recent studies on sentiment analysis, which is an important task in in...",No Preview Available
pdf1262.pdf,https://www.analyticsvidhya.com/blog/2021/04/improve-naive-bayes-text-classifier-using-laplace-smoothing/,Example of Laplace Smoothing for Text Classifying (#L2.2),Improve Naive Bayes Text classifier using Laplace Smoothing
pdf1264.pdf,https://www.youtube.com/watch?v=bkLHszLlH34,Information Theory Basics,Information Theory Basics
pdf1265.pdf,https://seeing-theory.brown.edu/bayesian-inference/index.html,Bayesian Inference,Chapter 5: Bayesian Inference
pdf1266.pdf,https://en.wikipedia.org/wiki/Information_retrieval,The definition of IR.,Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resource...
pdf1267.pdf,https://en.wikipedia.org/wiki/Entropy_(information_theory),Entropy covered in Lecture 1.3,Entropy (information theory)
pdf1268.pdf,https://www.analyticsvidhya.com/blog/2021/06/part-20-step-by-step-guide-to-master-nlp-information-retrieval/,NLP and information retrieval in Lecture #1.1,NLP – Information Retrieval
pdf1269.pdf,https://www.freecodecamp.org/news/bayes-rule-explained/,Bayes' rule covered in Lecture #1.2,Bayes' Rule
pdf1270.pdf,https://machinelearningmastery.com/what-is-information-entropy/,#L1.3 An introduction to Information Entropy,A Gentle Introduction to Information Entropy
pdf1271.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,Smoothing #L2.3,Smoothing #L2.3
pdf1272.pdf,https://en.wikipedia.org/wiki/N-gram,What is n-gram #L2.2,What is n-gram #L2.2
pdf1273.pdf,https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799,Unigram #L2.1,Unigram #L2.1
pdf1274.pdf,https://towardsdatascience.com/maximum-likelihood-vs-bayesian-estimation-dd2eb4dfda8a,#L1.2 A Medium article blog about Maximum Likelihood vs. Bayesian Esti...,Maximum Likelihood vs. Bayesian Estimation
pdf1275.pdf,https://www.mathsisfun.com/data/bayes-theorem.html,Useful page about Bayes' Rule for Lecture #1.2,Bayes' Theorem is a way of finding a probability when we know certain other probabilities.
pdf1276.pdf,https://ieeexplore.ieee.org/abstract/document/32278,#L1.1 A research article about statistical language model,A tree-based statistical language model for natural language speech recognition
pdf1277.pdf,https://fedtechmagazine.com/article/2020/01/assisted-intelligence-vs-augmented-intelligence-and-autonomous-intelligence-perfcon,Talks about the application of assisted AI in government. Lecture #L1....,"While artificial intelligence has been around for some time, only recently has it reached the level many predicted for the technology at the outset. Today, the growing toolkit of AI is capable of m..."
pdf1278.pdf,https://en.wikipedia.org/wiki/Entropy,Wikipedia page for entropy #1.3,"Entropy is a scientific concept, as well as a measurable physical property, that is most commonly associated with a state of disorder, randomness, or uncertainty."
pdf1279.pdf,https://www.mathsisfun.com/data/random-variables.html,An interesting explanation for random variable #1.2,A Random Variable is a set of possible values from a random experiment.
pdf1280.pdf,https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799,An detailed article with examples demostrating n-gram language model #...,N-gram language models
pdf1281.pdf,https://www.youtube.com/watch?v=GwP8gKa-ij8,Explanation of Good Turing Smoothing #L2.3,Good-Turing Smoothing
pdf1282.pdf,https://www.engati.com/glossary/statistical-language-modeling,some disadvantages of statistical language model #L2.1,"What are the drawbacks of statistical language modeling?
1. Zero probabilities
If we have a tri-gram language model that conditions of two words and has a vocabulary of 10,000 words. The we have ..."
pdf1283.pdf,https://web.mit.edu/6.933/www/Fall2001/Shannon2.pdf,A textbook introducing information theory #L1.3,Information Theory
pdf1284.pdf,https://people.math.harvard.edu/~ctm/papers/home/text/class/harvard/154/course/course.pdf,"This is a note of probability theory, it introduce probability in math...",Probability Theory
pdf1285.pdf,https://www.nlpworld.co.uk/nlp-glossary/f/feedback/,what is Feedback? #L4.2,what is Feedback? #L4.2
pdf1286.pdf,https://www.exploredatabase.com/2021/02/what-is-smoothing-in-nlp-and-why-do-we-need-smoothing.html,what really is smoothing and why? #L4.1,what really is smoothing and why? #L4.1
pdf1287.pdf,https://en.wikipedia.org/wiki/Okapi_BM25,BM25 #L3.2,BM25 #L3.2
pdf1288.pdf,https://en.wikipedia.org/wiki/Probabilistic_relevance_model,Probabilistic Retrieval Models #L3.1,Probabilistic Retrieval Models #L3.1
pdf1289.pdf,https://www.researchgate.net/publication/251806120_An_Investigation_of_Dirichlet_Prior_Smoothing's_Performance_Advantage,Discussions of Dirichlet Prior Smoothing mentioned at #L2.3,An Investigation of Dirichlet Prior Smoothing's Performance Advantage
pdf1290.pdf,https://en.wikipedia.org/wiki/Entropy_coding,Entropy Coding: https://en.wikipedia.org/wiki/Entropy_coding,Entropy coding
pdf1291.pdf,https://slideplayer.com/slide/14297401/,"More slides about textscope from professor:
https://slideplayer.com/s...",TextScope
pdf1292.pdf,https://nlp.stanford.edu/courses/cs224n/2001/gruffydd/smoothing.html,A website from Stanford introducing Bayesian smoothing with Bayesian p...,"Perhaps the simplest and most elegant method of estimating a multinomial distribution is a generalization of an approach originally taken by Laplace. In the following discussion, I follow the prese..."
pdf1293.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,A useful pdf file covers maximum likelihood of n-grams LM #L2.2,"Markov models are the class of probabilistic models
that assume we can predict the probability of some future unit without looking too
far into the past. We can generalize the bigram (which looks..."
pdf1294.pdf,https://www.fon.hum.uva.nl/rob/Courses/InformationInSpeech/CDROM/Literature/LOTwinterschool2006/homepages.inf.ed.ac.uk/s0450736/slm.html,#L2.1 Some common Statistical Language Modeling (SLM) techniques with ...,The goal of Statistical Language Modeling is to build a statistical language model that can estimate the distribution of natural language as accurate as possible. A statistical language model (SLM)...
pdf1295.pdf,http://asr.cs.cmu.edu/spring2010/class.19apr/ngrams.pdf,A powerpoint demostrating the application of N-gram language models fo...,N-gram language models for speech recognition
pdf1296.pdf,https://builtin.com/data-science/dirichlet-distribution,More about Dirichlet distribution #L2.3,The Dirichlet Distribution: What Is It and Why Is It Useful?
pdf1297.pdf,https://machinelearninginterview.com/topics/natural-language-processing/what-order-of-markov-assumption-is-done-for-n-grams/,Markov assumption in n-grams model #L2.2,What order of Markov assumption does n-grams model make ?
pdf1298.pdf,https://en.wikipedia.org/wiki/Information_theory,Wikipedia page of Information Theory. Relevant to Lecture #L1.3,"Information theory is the scientific study of the quantification, storage, and communication of information.[1] The field was originally established by the works of Harry Nyquist and Ralph Hartley,..."
pdf1299.pdf,https://www.engati.com/blog/applications-of-generative-language-models,3 applications of Generative Language Models #L2.1,3 applications of Generative Language Models
pdf1300.pdf,https://towardsdatascience.com/transformers-89034557de14,#L1.1 Intro to NLP with Transformer,No Preview Available
pdf1301.pdf,https://stats.stackexchange.com/questions/378274/how-to-construct-a-cross-entropy-loss-for-general-regression-targets,#L1.3 Cross-entropy and maximum likelihood in neural networks,"Plug the network output to the parameters of the distribution and take the -log then minimize the network weights.
This is equivalent to Maximum Likelihood Estimation of the parameters."
pdf1302.pdf,https://www.cs.cmu.edu/~roni/papers/survey-slm-IEEE-PROC-0004.pdf,#L2.1 History of SLM,TWO DECADES OF STATISTICAL LANGUAGE MODELING: WHERE DO WE GO FROM HERE?
pdf1303.pdf,https://www.cs.cmu.edu/~roni/papers/survey-slm-IEEE-PROC-0004.pdf,A comprehensive discussion about major SLM technologies in recent year...,Statistical Language Models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies.
pdf1304.pdf,https://maroo.cs.umass.edu/getpdf.php?id=694,A paper that investigate the advantage of Dirichlet Prior Smoothing in...,Dirichlet prior smoothing’s performance advantage appears to come more from an implicit prior favoring longer documents than from better estimation of the document model.
pdf1305.pdf,https://arxiv.org/abs/1301.3781,#L2.1 An example of an untraditional text modeling scheme (as opposed ...,Efficient Estimation of Word Representations in Vector Space
pdf1306.pdf,https://maroo.cs.umass.edu/getpdf.php?id=294,"#L4.2: In this paper, they use the feedback to improve query likelihoo...","[They] investigate relevant query feedback, in which [they] update a document’s representation based on a set of relevant queries."
pdf1307.pdf,https://ils.unc.edu/courses/2020_fall/inls509_001/lectures/07-QueryLikelihoodModel.pdf,#L4.1: These slides have an example of how the query-likelihood models...,Example
pdf1308.pdf,https://nlp.stanford.edu/IR-book/pdf/11prob.pdf,#L3.1: This book chapter talks about probabilistic retrieval models in...,"Based on these two representations, a system tries to determine how well documents satisfy information needs."
pdf1309.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,#L3.2: This describes the affect of certain assumptions we make about ...,The type of uncertainty presumably affects the estimations and derivation of the probabilistic model.
pdf1310.pdf,https://www.youtube.com/watch?v=fCn8zs912OE,#L2.2 VSAUCE Video explaining Zipf's law,No Preview Available
pdf1311.pdf,https://ieeexplore.ieee.org/abstract/document/817452,#L2.3 Survey about smoothing techniques,A survey of smoothing techniques for ME models
pdf1312.pdf,https://repository.lib.fit.edu/bitstream/handle/11141/682/ZHANG-THESIS.pdf?sequence=1,A in-depth comparison of the effects of different smoothing techniques...,No Preview Available
pdf1313.pdf,https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html,Some applications and limitations of the N-gram Language Model #L2.2,"The N-gram language model corrects the noise by using probability knowledge. Likewise, this model is used in machine translations for producing more natural statements in target and specified langu..."
pdf1314.pdf,https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/,A good complement in teaching how to compute ML estimate of N-gram LMS,Maximum Likelihood Estimation of N-Gram Model Parameters
pdf1315.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,"A complement of smoothing for language models including Interpolation,...","Smoothing

MLE may overfit the data: it will assign 0 probabilities to words it hasn't seen
What to do with it?
Bayesian Parameter Estimation can both maximize the data likelihood and incorpora..."
pdf1316.pdf,https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799,A detailed description of N-gram LMS with Unigram and examples of how ...,N-gram language models
pdf1317.pdf,https://aclanthology.org/N18-1049.pdf,#L2.2 An interesting work that use n-gram embeddings to compose senten...,Sent2Vec: a simple unsupervised model allowing to compose sentence embeddings using word vectors along with n-gram embeddings.
pdf1318.pdf,https://aclanthology.org/N10-2012.pdf,#L2.2 An overview report of N-gram Applications by Microsoft,No Preview Available
pdf1319.pdf,https://link.springer.com/chapter/10.1007/11846406_41,#L2.2 A good discussion about why n-gram may not work well,Another Look at the Data Sparsity Problem
pdf1320.pdf,https://dev.to/amananandrai/language-models-in-nlp-21jn,More description of Statistical language models in lecture #L2.1.,Introduction to Statistical language models
pdf1321.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,#L2.3 More information on Bayesian smoothing,Dirichlet Prior Smoothing
pdf1322.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,#L2.2 Another description of smoothing methods in n-gram language mode...,3.5.1 Laplace Smoothing
pdf1323.pdf,https://jon.dehdari.org/teaching/uds/lt1/ngram_lms.pdf,Different smoothing techniques and examples # L2.3,No Preview Available
pdf1324.pdf,https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799,"A project on Unigram LM, explaining the effect of smoothing and bias-v...",No Preview Available
pdf1325.pdf,https://towardsdatascience.com/evolution-of-language-models-n-grams-word-embeddings-attention-transformers-a688151825d2,#L2.1 A more detailed history on language models,"Evolution of Language Models: N-Grams, Word Embeddings, Attention & Transformers"
pdf1326.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,Describing the Dirichlet Prior Smoothning - shows how it is Bayesian s...,Dirichlet Prior Smoothing overview #L2.3
pdf1327.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,Chapter 3 from Speech and Language Processing Textbook (Jurafsky et al...,Overview and examples of N-gram Language Models #L2.2
pdf1328.pdf,http://www.cs.cmu.edu/~akalai/coltfinal/node8.html,Overview of SLMs and a unique application in stocks #L2.1,Application to Statistical Language Modeling #L2.1
pdf1329.pdf,http://www.cs.cmu.edu/~akalai/coltfinal/node8.html,Overview of SLMs and a unique application in stocks,Application to Statistical Language Modeling #L2.1
pdf1330.pdf,https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/dirichlet.pdf,Help understand Dirichlet Prior #L2.3,"Dirichlet Distribution, Dirichlet Process and
Dirichlet Process Mixture"
pdf1331.pdf,https://jofrhwld.github.io/teaching/courses/2022_lin517/lectures/ngram/02_smoothing.html,N-gram smoothing in python #L2.2,N-gram smoothing in python
pdf1332.pdf,http://www.foldl.me/2014/kneser-ney-smoothing/,Explanation of Kneser-Ney smoothing as mentioned in #L2.3,No Preview Available
pdf1333.pdf,https://devopedia.org/n-gram-model,Lists examples and applications of N-gram LMs #L2.1,No Preview Available
pdf1334.pdf,https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/,Introduces Statistical Language Models as mentioned in #L2.1,No Preview Available
pdf1335.pdf,https://en.wikipedia.org/wiki/Vector_space_model,An overview of Vector space model (an alternative to language models) ...,No Preview Available
pdf1336.pdf,https://jon.dehdari.org/tutorials/lm_overview.pdf,A Short Overview of Statistical Language Models #L2.1,A Short Overview of Statistical Language Models
pdf1337.pdf,https://medium.com/codex/statistical-language-model-n-gram-to-calculate-the-probability-of-word-sequence-using-python-2e54a1084250,Statistical Language Model: N-gram example in python #L2.1,Statistical Language Model: N-gram to calculate the Probability of word sequence using Python.
pdf1339.pdf,https://medium.com/geekculture/grab-and-go-series-n-gram-model-8703279c70cb,Brief explanation of n-gram model that covered in the lecture #L2.2,Grab-and-go series: n-gram model
pdf1340.pdf,https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplace-smoothing-when-we-have-unknown-words-in,#L2.2 Why we have to consider the unseen word probability,"In Naive Bayes, why bother with Laplace smoothing when we have unknown words in the test set?"
pdf1341.pdf,https://stats.stackexchange.com/questions/83203/laplace-smoothing-and-dirichlet-prior,#L2.3 Laplace smoothing and Dirichlet prior,This is essentially the observation that the Dirichlet distribution is a conjugate prior for the multinomial distribution.
pdf1342.pdf,http://www.cs.cmu.edu/~czhai/paper/cikm2001-fb.pdf,#L4.2 Useful resource to go in depth of feedback as model interpolatio...,Model-based feedback to LMs
pdf1343.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,"#L3.2 Describes the Robertson Sparck Jones model, covering the notatio...",Robertson Sparck Jones model
pdf1344.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,Covers how to use query likelihood for ranking documents #L4.1,Query likelihood model
pdf1345.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,"Describes the Robertson Sparck Jones model, covering the notation, rep...",Robertson Sparck Jones model
pdf1346.pdf,https://aspoerri.comminfo.rutgers.edu/InfoCrystal/Ch_2.html,#L3.1 Overview of probabilistic model and its comparison with other ty...,Probabilistic Model #L3.1
pdf1347.pdf,https://courses.engr.illinois.edu/cs447/fa2018/Slides/Lecture04.pdf,Some N-gram Model Examples (with Smoothing) #L2.2,No Preview Available
pdf1348.pdf,https://monkeylearn.com/natural-language-processing/,#2.1 A brief introduction of NLP,No Preview Available
pdf1349.pdf,https://hyperskill.org/learn/step/24084,#L2.1: Different Language Models,Language Models
pdf1350.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,Wiki for smoothing for LMs #L2.3,Smoothing for Language Models
pdf1351.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,Introduction to N-Gram Language Models #L2.2,"Statistical language models, in its essence, are the type of models that assign probabilities to the sequences of words. In this article, we’ll understand the simplest model that assigns probabilit..."
pdf1352.pdf,https://gawron.sdsu.edu/compling/course_core/lectures/prob_parse.htm,Introduction to probabilistic context free grammar #L2.1,A probabilistic context free grammar is a context free grammar with probabilities attached to the rules.
pdf1353.pdf,https://www.youtube.com/watch?v=QGT6XTeA3YQ,#L2.2 Introduction to N-Gram Smoothing Techniques,No Preview Available
pdf1354.pdf,https://www.sciencedirect.com/science/article/pii/S0167639303001055,Statistical Language model adaptation. #L2.1,No Preview Available
pdf1355.pdf,https://builtin.com/data-science/beginners-guide-language-models,#L2.1 A Broad Overview of Language Models,No Preview Available
pdf1356.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,#2.2 Introduce to N-gram,No Preview Available
pdf1357.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,A website from Stanford covers how to using query likelihood language ...,"Language modeling is a quite general formal approach to IR, with many variant realizations. The original and basic method for using language models in IR is the query likelihood model . In it, we c..."
pdf1358.pdf,http://boston.lti.cs.cmu.edu/callan/Workshops/lmir01/WorkshopProcs/Papers/lafferty.pdf,A paper explained the differences of two basic probabilistic retrieval...,No Preview Available
pdf1359.pdf,https://thegradient.pub/understanding-evaluation-metrics-for-language-models/,#L2.1 Perplexity in context of information bits and entropy,"Understanding perplexity, bits-per-character, and cross entropy"
pdf1360.pdf,https://arxiv.org/abs/2212.06002,"A pretty interesting topic mining paper that was recently published, r...","We propose an iterative framework, SeedTopicMine, which jointly learns from the three types of contexts and gradually fuses their context signals via an ensemble ranking process. Under various sets..."
pdf1361.pdf,https://analyticsindiamag.com/a-comprehensive-guide-to-maximum-likelihood-estimation-and-bayesian-estimation/,This page provides a nice comparison/examples between MLE and Bayesian...,"In statistics, the Bayesian estimation is a method of estimating the parameters by minimizing the posterior expected loss function where the posterior expected value is a conditional probability th..."
pdf1362.pdf,https://gnarlyware.com/blog/kl-divergence-online-demo/,This page contains an online demo of the KL divergence between compute...,KL Divergence Online Demo
pdf1363.pdf,https://medium.com/@dennyc/a-simple-numerical-example-for-kneser-ney-smoothing-nlp-4600addf38b8,Kneser-Ney Smoothing explained with an example #L2.3,No Preview Available
pdf1364.pdf,https://devopedia.org/n-gram-model,"Applications, Limitations and Evaluation of N-gram model #L2.2","An N-gram model is one type of a Language Model (LM), which is about finding the probability distribution over word sequences."
pdf1365.pdf,https://dev.to/amananandrai/language-models-in-nlp-21jn,Anatomy of Language Models #L2.1,"Language models power all the popular NLP applications we are familiar with like Google Assistant, Siri, Amazon’s Alexa, etc."
pdf1366.pdf,https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058,An introduction to n-gram models for NLP #L2.2,No Preview Available
pdf1367.pdf,https://medium.com/@rachel_95942/language-models-and-rnn-c516fab9545b,An article that goes over n-grams and more advanced language models su...,No Preview Available
pdf1368.pdf,https://www.usna.edu/Users/cs/nchamber/courses/nlp/f13/slides/set4-smoothing.pdf,Smoothing Language Models #L2.2,No Preview Available
pdf1369.pdf,https://www.youtube.com/watch?v=nfBNOWv1pgE,#L2.3: Dirichlet distribution explained visually,No Preview Available
pdf1370.pdf,https://github.com/PangYunsheng8/Smoothing-Techniques-in-NLP/blob/master/smoothing.ipynb,Python Implementation of smoothing techniques commonly used in NLP men...,"Smoothing techniques commonly used in NLP
Laplacian (add-one) Smoothing
Lidstone (add-k) Smoothing
Absolute Discounting
Katz Backoff
Kneser-Ney Smoothing
Interpolation"
pdf1371.pdf,https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/,A good blog for contents in #L2.2 and #L2.3. ML estimation for n-gram,No Preview Available
pdf1372.pdf,https://faculty.wcas.northwestern.edu/robvoigt/courses/2021_fall/ling334/assignments/a2.html,#L2.2 A good practice (homework) for N-Gram,No Preview Available
pdf1373.pdf,https://github.com/bentrevett/pytorch-pos-tagging,"#L2.1 Pytorch resources for the applications of SLMs, specifically PoS...",No Preview Available
pdf1374.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,N-gram Language Model,No Preview Available
pdf1375.pdf,https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/,Neural Language Model,No Preview Available
pdf1376.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,different smoothing methods #2.3,No Preview Available
pdf1377.pdf,https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799,A project about N-gram #2.2,No Preview Available
pdf1378.pdf,https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/,into to SLM #2.1,"Gentle Introduction to Statistical Language Modeling and Neural Language Models
by Jason Brownlee on November 1, 2017 in Deep Learning for Natural Language Processing
Tweet Tweet  Share Share
La..."
pdf1379.pdf,https://www.codingninjas.com/codestudio/library/smoothing-in-nlp,Overview of different types of Smoothing #L2.3,Smoothing refers to the technique we use to adjust the probabilities used in the model so that our model can perform more accurately and even handle the words absent in the training set.
pdf1380.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,#L2.3 Smoothing Techniques used in N-Gram model,No Preview Available
pdf1381.pdf,https://towardsdatascience.com/word2vec-explained-49c52b4ccb71,Explanation of Non standard Statistical language model Word2Vec #L2.1,Word2Vec Explained
pdf1382.pdf,https://devopedia.org/n-gram-model,#L2.2 N-Gram Model Example,No Preview Available
pdf1383.pdf,https://www.freecodecamp.org/news/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24/,#L2.1 POS Tagging,No Preview Available
pdf1384.pdf,http://www.scholarpedia.org/article/Neural_net_language_models,Introduction and history about Neural language models #L2.1,"A neural network language model is a language model based on Neural Networks , exploiting their ability to learn distributed representations to reduce the impact of the curse of dimensionality."
pdf1385.pdf,https://www.cs.upc.edu/~gatius/mai-ihlp/languagemodel2018.pdf,slm notes,No Preview Available
pdf1386.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,N-gram language models better understood,No Preview Available
pdf1387.pdf,https://towardsdatascience.com/statistical-language-models-4e539d57bcaa,Introduction to Statistical Language models  with examples and code sn...,Statistical Language Models
pdf1388.pdf,https://dev.to/balapriya/understanding-n-gram-language-models-3g72,Week 2 Proof of learning,N-Gram Language Models - A Beginner's Guide
pdf1389.pdf,https://stats.stackexchange.com/questions/83203/laplace-smoothing-and-dirichlet-prior?rq=1,#L2.3 A further discussion of Laplace smoothing and Dirichlet Prior,No Preview Available
pdf1390.pdf,https://www.cs.cmu.edu/~roni/papers/survey-slm-IEEE-PROC-0004.pdf,#2.1 A great survey on Statistical Language Modeling,No Preview Available
pdf1391.pdf,https://aclanthology.org/www.mt-archive.info/ICUKL-2002-Majumder.pdf,N-gram: a language independent approach to IR and NLP,No Preview Available
pdf1392.pdf,https://www.fon.hum.uva.nl/rob/Courses/InformationInSpeech/CDROM/Literature/LOTwinterschool2006/homepages.inf.ed.ac.uk/s0450736/slm.html,A gentle introduction to Statistical Language Modeling (SLM) and its t...,What is Statistical Language Modeling (SLM)
pdf1393.pdf,https://towardsdatascience.com/statistical-language-models-4e539d57bcaa,An Introduction to Language Models and their types along with their us...,Statistical Language Models
pdf1394.pdf,https://www.techtarget.com/searchenterpriseai/definition/language-modeling,Types of statistical language models #L2.1,Some common statistical language modeling types
pdf1395.pdf,https://dl.acm.org/doi/abs/10.1145/319950.320022,The Study examines about how Statistical language modeling has been su...,A general language model for information retrieval
pdf1396.pdf,https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained,#L1.3 Kullback-Leibler Divergence explanation with example,No Preview Available
pdf1397.pdf,https://math.stackexchange.com/questions/1716421/bayes-theorum-with-multiple-conditions-with-independent-ancenstors,#L1.2 - Conditional probability based on multiple variables,"P[A|B,C]=P[A,B,C]P[B,C]=P[B,C|A]P[A]P[B,C]"
pdf1398.pdf,https://math.stackexchange.com/questions/784799/conditional-probability-constraints,#L1.2 - Constraints to be checked in conditional probability,"P(B)=P(A∩B)+P(A¯∩B)
���
(
���
)
=
���
(
���
∩
���
)
+
���
(
���
¯
∩
���
)
 so

P(B|A)P(A)≤P(B)≤P(B|A)P(A)+1−P(A)
���
(
���
|
���
)
���
(
���
)
≤
���
(
���
)
≤..."
pdf1399.pdf,https://rstudio-pubs-static.s3.amazonaws.com/271652_1525c0598da74774bfa4047803cee0d5.html,Predicting Next Word Using Katz Backoff #L2.3,No Preview Available
pdf1400.pdf,https://eliteai-coep.medium.com/building-n-gram-language-model-from-scratch-9a5ec206b520,Article describing how to build an N-gram language model using NLTK,Building N-gram Language Model From Scratch
pdf1401.pdf,https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799,"Interesting medium article, covers what we have learnt from the lectur...","N-gram language models
Part 1: Unigram model"
pdf1402.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,Gives a brief overview and the formulas involved for various smoothing...,Smoothing for Language Models
pdf1403.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,Gives a brief overview of smoothing technique for Language Models and ...,Quick Introduction to Smoothing Techniques for Language Models
pdf1404.pdf,https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/,Gives a brief overview of N-gram language model and how to implement i...,N-Gram Language Modelling with NLTK
pdf1405.pdf,https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799,A background on language models and the various ways to estimate param...,Unigram Language model
pdf1406.pdf,https://link.springer.com/article/10.1007/s11042-020-10305-w,a useful explanation of dirichlet smoothing covered in Lecture #L 2.3,"Basically, Dirichlet smoothing (DS) model is widely used to retrieve DRO documents. DS model uses a smoothing parameter μ which plays a strong role in finding the value of the unseen terms to avoid..."
pdf1407.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,a useful explanation of n-gram language model covered in Lecture #L2.2,This assumption that the probability of a word depends only on the previous word is also known as Markov assumption.
pdf1408.pdf,https://devopedia.org/n-gram-model,N-gram language model and interpolation,"Interpolation is another technique in which we can estimate an n-gram probability based on a linear combination of all lower-order probabilities. For instance, a 4-gram probability can be estimated..."
pdf1409.pdf,https://towardsdatascience.com/part-of-speech-tagging-for-beginners-3a0754b2ebba,a useful explanation of POS tagging covered in Lecture #L2.1,the idea as shown in this example is that the POS tag that is assigned to the next word is dependent on the POS tag of the previous word.
pdf1410.pdf,https://www.youtube.com/watch?v=6P2z9PDRWTw,Lecture recording for #L2.1,No Preview Available
pdf1411.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,#L2.2 : Basic introduction to n-gram models,No Preview Available
pdf1412.pdf,https://citeseerx.ist.psu.edu/document?repid=rep1,#L2.1 : One interesting use case of SLM,No Preview Available
pdf1413.pdf,https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html,This article is an interesting abstract and introduction to n-gram lan...,"N-gram is a sequence of the N-words in the modeling of NLP. Consider an example of the statement for modeling. “I love reading history books and watching documentaries”. In one-gram or unigram, the..."
pdf1414.pdf,https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/,A guide to contents covered in #L2.2,No Preview Available
pdf1415.pdf,https://www.mygreatlearning.com/blog/pos-tagging/,Explanation of POS tagging with examples.,Explanation of Part of Speech tagging #L2.1
pdf1416.pdf,https://sigir.org/wp-content/uploads/2017/06/p268.pdf,#L2.3 A general study on smoothing techniques for Information Retrieva...,No Preview Available
pdf1417.pdf,https://www.sciencedirect.com/science/article/pii/S1877050921012382,#L2.2 N-gram LMs estimation,No Preview Available
pdf1418.pdf,https://link.springer.com/book/10.1007/978-3-031-02130-5,#L2.1 Book on the scope of SLM for Information Retrieval,No Preview Available
pdf1419.pdf,https://link.springer.com/referenceworkentry/10.1007/978-0-387-39940-9_923,L2.1: Springer paper describing basics of LMs along with additional co...,Language Models
pdf1420.pdf,https://dl.acm.org/doi/pdf/10.5555/1667583.1667691,Introducing a method which can improve smoothing technology for N-gram...,No Preview Available
pdf1421.pdf,https://vitalflux.com/n-gram-language-models-explained-examples/,brief summary for N-Gram Language Models with useful examples #L2.2,No Preview Available
pdf1422.pdf,https://www.fon.hum.uva.nl/rob/Courses/InformationInSpeech/CDROM/Literature/LOTwinterschool2006/homepages.inf.ed.ac.uk/s0450736/slm.html,Very useful resource about SLM with basic demonstration and practical ...,No Preview Available
pdf1423.pdf,https://www.cs.cmu.edu/~roni/papers/survey-slm-IEEE-PROC-0004.pdf,#L2.1 - A good background on statistical models over 2 decades,No Preview Available
pdf1424.pdf,https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/,Explanation of MLE of N-gram models #L2.2,Maximum Likelihood Estimation of N-Gram Model Parameters
pdf1425.pdf,https://devopedia.org/n-gram-model,Explanation of N gram model #L2.2,N-Gram Model
pdf1426.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,#2.1 A Concrete Example on N-gram and Markov Assumption,No Preview Available
pdf1427.pdf,https://www.ccs.neu.edu/home/vip/teach/IRcourse/1_retrieval_models/slides/smoothing_slides.pdf,Linear Interpolation Smoothing for #L2.3,No Preview Available
pdf1428.pdf,https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/,N-Gram Language Modelling for #L2.2,N-Gram Language Modelling
pdf1429.pdf,https://vitalflux.com/n-gram-language-models-explained-examples/,#L2.3 n-gram models explained with example,No Preview Available
pdf1430.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,#L2.3 - Different smoothing models,No Preview Available
pdf1431.pdf,https://www.techtarget.com/searchenterpriseai/definition/language-modeling,Language modeling and purposes#L2.1,No Preview Available
pdf1432.pdf,https://www.techtarget.com/searchenterpriseai/definition/language-modeling,Language modeling and puposes,No Preview Available
pdf1433.pdf,http://mlwiki.org/index.php/Statistical_Language_Models,Very basic understanding pf SLM#L2.1,No Preview Available
pdf1434.pdf,https://www.youtube.com/watch?v=ptV3E1zwaEw,#L2.2 - N Grams model Laplace smoothing,No Preview Available
pdf1435.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,Explanation of Smoothing techniques #L2.2,Introduction to Smoothing Techniques for Language Models
pdf1436.pdf,https://towardsdatascience.com/part-of-speech-tagging-for-beginners-3a0754b2ebba,"#L2.1: An helpful explanation of the topic ""POS tagging"" covered in L2...",Part Of Speech Tagging for Beginners
pdf1437.pdf,https://en.wikipedia.org/wiki/Katz's_back-off_model,Katz backoff model #L2.3,No Preview Available
pdf1438.pdf,https://isip.piconepress.com/courses/msstate/ece_8463/lectures/current/lecture_33/lecture_33.pdf,Simpler explanation of why smoothing is important #L2.2,"Smoothing is the process of flattening a probability distribution implied by a language model so that all
reasonable word sequences can occur with some probability. This often involves broadening ..."
pdf1439.pdf,https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799,Good explanation of unigram models #L2.1,No Preview Available
pdf1440.pdf,https://builtin.com/data-science/beginners-guide-language-models,Starter Guide for language model #2.1,A language model is a probability distribution over words or word sequences.
pdf1441.pdf,https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar,Detailed explanation of Probabilistic context-free grammar covered in ...,No Preview Available
pdf1442.pdf,http://www.cs.cmu.edu/~roni/11761/Presentations/h015a-techreport.pdf,#2.3 A document of some useful smoothing methods,No Preview Available
pdf1443.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/types-of-language-models-1.html,A useful explanation of different types of language models explained i...,Types of language models
pdf1444.pdf,https://resources.mpi-inf.mpg.de/departments/d5/teaching/ws13_14/irdm/slides/irdm-3-4-5.pdf,A good set of slides on SLMs #L2.1,Statistical Language Models
pdf1445.pdf,https://isip.piconepress.com/courses/msstate/ece_8463/lectures/current/lecture_33/lecture_33.pdf,#L2.3 Lecture about smoothing N-Gram models,No Preview Available
pdf1446.pdf,https://cseweb.ucsd.edu/~nnakashole/teaching/eisenstein-nov18.pdf,A section on n-gram models from Eisenstein's book #L2.2,6.1 N-gram language models
pdf1447.pdf,https://cseweb.ucsd.edu/~nnakashole/teaching/eisenstein-nov18.pdf,A section on smoothing and discounting from Jacob Eisenstein's book #L...,6.2 Smoothing and discounting
pdf1448.pdf,https://dl.acm.org/doi/abs/10.1145/3130348.3130377?casa_token=0ruRdFLXWAsAAAAA:3fGm3PdFdjXG53TbfvvwG84uyWv1No16S5XeMdfQQZYFF-kVfrgYJ-pok4mvFk4GIV7kXxxqFcDOrg,#L2.3 Paper on smoothing methods,A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval
pdf1449.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,L2.2 Detailed book chapter on N-gram,No Preview Available
pdf1450.pdf,https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/,Estimation of N gram language models#L2.2,No Preview Available
pdf1451.pdf,https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/,Estimation of N gram language models,No Preview Available
pdf1452.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,The smoothing formula for Jelinek-Mercer smoothing #L2.3,Jelinek-Mercer Smoothing
pdf1453.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,This shows how to derive the dirichlet prior smoothing method and show...,Dirichlet Prior Smoothing
pdf1454.pdf,https://www.cs.purdue.edu/homes/dgoldwas/Teaching/ml4nlp_fall2017/lectures/Lecture2.pdf,Language Models in #L2.2,No Preview Available
pdf1455.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,This portion of the text in stanfords book gives a decent introduction...,No Preview Available
pdf1456.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,Explains different smoothing methods covered in #L2.2,Smoothing for Language Models
pdf1457.pdf,https://www.engati.com/glossary/statistical-language-modeling,Great introduction to SLM's which is discussed in Lecture #L2.1,"Statistical Language Modeling, or Language Modeling and LM for short, is the development of probabilistic models that can predict the next word in the sequence given the words that precede it.

A..."
pdf1458.pdf,https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/,Use n-gram with NLTK #2.3,NLTK
pdf1459.pdf,https://en.wikipedia.org/wiki/N-gram,Mathematics view of n-gram #2.2,An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1)–order Markov model.
pdf1460.pdf,https://blog.xrds.acm.org/2017/10/introduction-n-grams-need/,"This webpage provids a introduction of N-grams model, and give some si...",An Introduction to N-grams: What Are They and Why Do We Need Them?
pdf1461.pdf,http://www.cs.cornell.edu/courses/cs4740/2014sp/lectures/smoothing backoff.pdf,"#L2.2: A slide from CS @ Cornell to explain the topic ""N-gram Models"" ...",N-gram models
pdf1462.pdf,https://isip.piconepress.com/courses/msstate/ece_8463/lectures/current/lecture_33/lecture_33.pdf,Smoothing techniques for N-gram models#L2.3,No Preview Available
pdf1463.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,#L2.3 A quick guide to various smoothing methods,Smoothing for Language Models
pdf1464.pdf,https://www.statlect.com/fundamentals-of-probability/Kullback-Leibler-divergence,#L1.3 A useful website explaining KL divergence,No Preview Available
pdf1465.pdf,https://towardsdatascience.com/maximum-likelihood-vs-bayesian-estimation-dd2eb4dfda8a,#L1.2 A comparison between maximum likelihood and bayesian estimate,No Preview Available
pdf1466.pdf,https://nlp.stanford.edu/IR-book/information-retrieval-book.html,#L1.1 A companion website for the book on information retrieval,No Preview Available
pdf1467.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,#L2.2 A detailed PDF on N-gram language models,No Preview Available
pdf1468.pdf,https://nlp.stanford.edu/courses/cs224n/2001/gruffydd/smoothing.html,#L2.3 A page about bayesian smoothing,No Preview Available
pdf1469.pdf,https://www.researchgate.net/publication/267363186_Introduction_to_statistical_language_models_and_their_applications,#L2.1 Useful technical report about statistical language models and th...,No Preview Available
pdf1470.pdf,https://www.statlect.com/probability-distributions/Dirichlet-distribution,#L2.3,"The Dirichlet distribution is a multivariate generalization of the Beta distribution.

Denote by  the probability of an event. If  is unknown, we can treat it as a random variable, and assign a B..."
pdf1471.pdf,https://aclanthology.org/J92-4003.pdf,L2.3 N-Gram models for NLP,No Preview Available
pdf1472.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,"This page provides introduction of smoothing techniques, include the r...",Quick Introduction to Smoothing Techniques for Language Models
pdf1473.pdf,https://dl.acm.org/doi/pdf/10.1145/3130348.3130377,#L2.2 A study of smoothing methods for information retrieval,No Preview Available
pdf1474.pdf,https://en.wikipedia.org/wiki/Interpolation,Explains interpolation of lecture #L2.3,Interpolation
pdf1475.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,#L2.3,Smoothing for Language Models
pdf1476.pdf,https://dl.acm.org/doi/pdf/10.1145/319950.320022,#L2.1 A general model for information retrieval,No Preview Available
pdf1477.pdf,https://leimao.github.io/blog/Maximum-Likelihood-Estimation-Ngram/,#L2.2,Maximum Likelihood Estimation of N-Gram Model Parameters
pdf1478.pdf,https://aclanthology.org/I05-3019.pdf,#L2.1,"This paper describes a Chinese word
segmentation system based on unigram
language model for resolving segmentation ambiguities"
pdf1479.pdf,https://www.fon.hum.uva.nl/rob/Courses/InformationInSpeech/CDROM/Literature/LOTwinterschool2006/homepages.inf.ed.ac.uk/s0450736/slm.html,#L2.1,The goal of Statistical Language Modeling is to build a statistical language model that can estimate the distribution of natural language as accurate as possible. A statistical language model (SLM)...
pdf1480.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,This article has the summary of all smoothing techniques which can be ...,No Preview Available
pdf1481.pdf,https://citeseerx.ist.psu.edu/document?repid=rep1,#L2.1,A review of statistical language model adaptation.
pdf1482.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,"This note introduced many different smoothing for language model, some...",Smoothing for Language Models
pdf1483.pdf,https://www.freecodecamp.org/news/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24/,This article explains POS tagging and its solutions which use HMM whic...,No Preview Available
pdf1484.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,This article explains laplace smoothing explained in #L2.2 and also ot...,No Preview Available
pdf1485.pdf,https://en.wikipedia.org/wiki/Part-of-speech_tagging,Explains POS tagging in #L2.1,Part-of-speech tagging
pdf1486.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,Provides an introduction of smoothing and why we need to use smoothing...,Introduction to Smoothing Techniques for Language Models
pdf1487.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,#L2.2,"A notable problem with the MLE approach is sparse data. Meaning, any N-gram that appeared a sufficient number of times might have a reasonable estimate for its probability. But because any corpus i..."
pdf1488.pdf,file:///C:/Users/maiph/uiuc/spring2023/CS 510/Estimation_of_probabilities_from_Sparse_data_for_t.pdf,This paper describe another method to deal with probability of rare ev...,reduce unreliable probability estimates given by the observed frequencies and redistribute the “freed” probability “mass” among m-grams which never occurred in the text.
pdf1489.pdf,https://towardsdatascience.com/part-of-speech-tagging-for-beginners-3a0754b2ebba,"This webpage introducte Part of Speech Tagging, also it introduce the ...",Part Of Speech Tagging
pdf1490.pdf,https://devopedia.org/n-gram-model,Explains n-gram model. (#L2.2),"Given a sequence of N-1 words, an N-gram model predicts the most probable word that might follow this sequence. It's a probabilistic model that's trained on a corpus of text. Such a model is useful..."
pdf1491.pdf,http://mlwiki.org/index.php/Statistical_Language_Models,Explains statistical language models (#L2.1),"A statistical language model (Language Model for short) is a probability distribution over sequences of words (i.e. over sentences)

It assigns any sentence a probability it's a probability of se..."
pdf1492.pdf,https://en.wikipedia.org/wiki/Language_model,Explains language model (#L2.1),"A language model is a probability distribution over sequences of words.[1] Given any sequence of words of length m, a language model assigns a probability 
���
(
���
1
,
…
,
���
���
)
 t..."
pdf1493.pdf,https://en.wikipedia.org/wiki/Katz's_back-off_model,Explains a backoff model for n-gram (#L2.3),Katz's back-off model
pdf1494.pdf,https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/,Explains a python package for n-gram models (#L2.2),N-Gram Language Modelling with NLTK
pdf1495.pdf,https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf,Introduction to smoothing methods and their comparison.,Introduction to smoothing techniques.#L2.3
pdf1496.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,N-gram language model,"Statistical language models, in its essence, are the type of models that assign probabilities to the sequences of words. In this article, we’ll understand the simplest model that assigns probabilit..."
pdf1497.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,N-gram introduction about #L2.2,N-gram Language Model
pdf1498.pdf,https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/,Statistical language model,Gentle Introduction to Statistical Language Modeling and Neural Language Models
pdf1499.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,A chapter for N-gram language model,A detailed overview of N-gram Language Model.#L2.2
pdf1500.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,a useful explanation of Dirichlet Prior Smoothing covered in Lecture #...,"Dirichlet Prior Smoothing
It's a Bayesian Smoothing with special prior: Dirichlet Distribution

Dir(θ∣α)=Γ(∑iαi)∏iΓ(α1)⋅∏iθαi−1i
Dir
(
���
∣
���
)
=
Γ
(
∑
���
���
���
)
∏
���
Γ..."
pdf1501.pdf,https://en.wikipedia.org/wiki/Additive_smoothing,a useful explanation of Additive smoothing covered in Lecture #L2.2,Additive smoothing
pdf1502.pdf,https://en.wikipedia.org/wiki/Language_model,a useful explanation of Language Model covered in Lecture #L2.1,"A language model is a probability distribution over sequences of words.[1] Given any sequence of words of length m, a language model assigns a probability 
���
(
���
1
,
…
,
���
���
)
 t..."
pdf1503.pdf,https://www.youtube.com/watch?v=QGT6XTeA3YQ,Smoothing Techniques with examples#L2.3,N-Gram Smoothing Techniques
pdf1504.pdf,https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.html,This describes some useful concepts related Smoothing methods mentione...,N-gram smoothing models
pdf1505.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,This describes some useful concepts discussed in lecture #L2.1,Language Models: N-Gram
pdf1506.pdf,https://towardsdatascience.com/statistical-language-models-4e539d57bcaa,Real-World applications of SLM #L2.1,Statistical Language Models
pdf1507.pdf,https://www.engati.com/glossary/statistical-language-modeling,Give a defination of statistical language modeling and several statist...,statistical language modeling
pdf1508.pdf,https://web.stanford.edu/~jurafsky/slp3/4.pdf,"#L2.1 Naive bayes for text classification tasks, as a language model","Section 4.5, 4.6"
pdf1509.pdf,https://jon.dehdari.org/tutorials/lm_overview.pdf,Comprehensive overview of N-gram models #L2.2,A Short Overview of Statistical Language Models
pdf1510.pdf,https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799,Practice of the Unigram model,"In part 1 of the project, I will introduce the unigram model i.e. model based on single words. Then, I will demonstrate the challenge of predicting the probability of “unknown” words — words the mo..."
pdf1511.pdf,https://www.sciencedirect.com/science/article/pii/S1877050921012382,Research paper on N-Gram language model,"Modeling a natural language aims to build systems that are able to reproduce, correct and more or less predict the structure of a given language; moreover it summarizes the general knowledge relate..."
pdf1512.pdf,https://www.tidytextmining.com/ngrams.html,N-Gram and correlations,"In this chapter, we’ll explore some of the methods tidytext offers for calculating and visualizing relationships between words in your text dataset. This includes the token = ""ngrams"" argument, whi..."
pdf1513.pdf,https://towardsdatascience.com/statistical-language-models-4e539d57bcaa,"A useful explanation of SLM covered in Lecture #L2.1

From simple to...",Statistical Language Models
pdf1514.pdf,https://en.wikipedia.org/wiki/Katz's_back-off_model,Explanation of Katz's back-off model #L2.3,Katz's back-off model
pdf1515.pdf,https://towardsdatascience.com/laplace-smoothing-in-naïve-bayes-algorithm-9c237a8bdece,L2.2 Explanation of Laplace Smoothing,Laplace smoothing is a smoothing technique that handles the problem of zero probability in Naïve Bayes
pdf1516.pdf,https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/,A useful explanation of SLM covered in Lecture #L2.1,Gentle Introduction to Statistical Language Modeling and Neural Language Models
pdf1517.pdf,https://academic.oup.com/comjnl/article/35/3/243/525633,A paper of probabilistic models in information retrieval in #L3.1,an introduction and survey over probabilistic information retrieval (IR)
pdf1518.pdf,https://en.wikipedia.org/wiki/Conjugate_prior,"#L2.3: The WIKIPEDIA page for the topic ""Conjugate prior"" covered in L...",Conjugate prior
pdf1519.pdf,https://dl.acm.org/doi/abs/10.1145/2792838.2796547,#L2.1 An article illustrates that applying Statistical Language Models...,Exploring Statistical Language Models for Recommender Systems
pdf1520.pdf,https://math.stackexchange.com/questions/549887/bayes-theorem-with-multiple-random-variables,explanation on how the special case are derived from bays rule,No Preview Available
pdf1521.pdf,https://numpy-ml.readthedocs.io/en/latest/numpy_ml.ngram.html,#L2.2 Smoothing techniques in Numpy,Good-Turing smoothing is a more sophisticated technique which takes into account the identity of the particular n-gram when deciding the amount of smoothing to apply.
pdf1522.pdf,http://www.phontron.com/slides/nlp-programming-en-01-unigramlm.pdf,L2.1: Language Models Brief overview,No Preview Available
pdf1523.pdf,https://www.researchgate.net/publication/319185209_Estimation_of_Gap_Between_Current_Language_Models_and_Human_Performance,A paper that attempts to gauge the performance gap between current neu...,Estimation of Gap Between Current Language Models and Human Performance
pdf1524.pdf,https://cacm.acm.org/magazines/2022/7/262080-language-models/abstract,Language Models: Brief History,No Preview Available
pdf1525.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,A useful explanation of Smoothing covered in Lecture #L2.3,Smoothing for Language Models
pdf1526.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,A useful explanation of SLM covered in Lecture #L2.3,Language Models: N-Gram
pdf1527.pdf,https://jalammar.github.io/illustrated-word2vec/,L2.1 Skip gram and CBOW architectures as examples of LMs,This is called a Continuous Bag of Words architecture and is described in one of the word2vec papers [pdf]. Another architecture that also tended to show great results does things a little differen...
pdf1528.pdf,https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/,#L2.2 A python implementation of N-Gram Language Model,N-Gram Language Modelling with NLTK
pdf1529.pdf,http://www.foldl.me/2014/kneser-ney-smoothing/,Explanation of Kneser-Ney smoothing #L2.3,Kneser-Ney smoothing
pdf1530.pdf,https://ieeexplore.ieee.org/document/8257905,This page was informative as it contained a useful explanation of text...,TextScope: Enhance human perception via text mining
pdf1531.pdf,https://aclanthology.org/W98-1121.pdf,#L2.1 An older paper that introduces the concept of POS tagging using ...,No Preview Available
pdf1532.pdf,https://www.youtube.com/watch?v=GiyMGBuu45w,N-gram language model Introduction covered in lecture 2.2,No Preview Available
pdf1533.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,An article explaining the different smoothing techniques used in langu...,"Smoothing techniques in NLP are used to address scenarios related to determining probability / likelihood estimate of a sequence of words (say, a sentence) occuring together when one or more words ..."
pdf1535.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,Explanation of various language smoothing models #L2.3,Smoothing for Language Models
pdf1536.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,#L2.3 An introduction of smoothing techniques in Language Model,Quick Introduction to Smoothing Techniques for Language Models
pdf1537.pdf,https://en.wikipedia.org/wiki/Mixed_Poisson_distribution,"explaining mixed Poisson model, which used to model frequency to impro...",No Preview Available
pdf1538.pdf,https://zhuanlan.zhihu.com/p/27201427,#L2.1#L2.2#L2.3 All three lectures is about statistical language model...,No Preview Available
pdf1539.pdf,https://www.youtube.com/watch?v=GwP8gKa-ij8,L2.3 A tutorial of Good-Turing smoothing,No Preview Available
pdf1540.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,This page explains about good turing smoothing covered in #L2.3,Quick Introduction to Smoothing Techniques for Language Models
pdf1541.pdf,https://medium.com/analytics-vidhya/n-gram-language-models-9021b4a3b6b,This page consists of explanation with examples of N Gram language mod...,N-Gram Language Models
pdf1542.pdf,https://www.fon.hum.uva.nl/rob/Courses/InformationInSpeech/CDROM/Literature/LOTwinterschool2006/homepages.inf.ed.ac.uk/s0450736/slm.html,The page has useful explanation of Statistical Language Modeling cover...,What is Statistical Language Modeling (SLM)
pdf1543.pdf,https://sites.stat.washington.edu/mmp/courses/stat544/Handouts/l7slides-conjugate.pdf,#L2.3 Conjugate priors for the exponential family,No Preview Available
pdf1544.pdf,https://www.analyticsvidhya.com/blog/2020/11/entropy-a-key-concept-for-all-data-science-beginners/,This page consists of details related to entropy that was discussed in...,Entropy – A Key Concept for All Data Science Beginners
pdf1545.pdf,https://citeseerx.ist.psu.edu/document?repid=rep1,An article detailing the performance advantages of Dirichlet Smoothing...,"An Investigation of Dirichlet Prior Smoothing’s
Performance Advantage"
pdf1546.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,"A good overview of the various smoothing techniques, including Additiv...",No Preview Available
pdf1547.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html,A useful explanation of Baye's theorem in text retrieval is explained ...,Text classification and Naive Bayes
pdf1548.pdf,https://en.wikipedia.org/wiki/Zipf's_law,"A more detailed explanation and interesting examples of Zipf's Law, wh...","Zipf's law was originally formulated in terms of quantitative linguistics, stating that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its ..."
pdf1549.pdf,https://www.youtube.com/watch?v=Vc2C1NZkH0E,A detailed code and math behind N-gram language model lecture 2.3,No Preview Available
pdf1550.pdf,https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58,#L2.3 N-Gram Language Models: Optimize model interpolation,Optimize model interpolation
pdf1551.pdf,https://vitalflux.com/n-gram-language-models-explained-examples/,#L2.2 N-Gram Language Models Explained with Examples,N-Gram Language Models Explained with Examples
pdf1552.pdf,https://towardsdatascience.com/evolution-of-language-models-n-grams-word-embeddings-attention-transformers-a688151825d2,"#L2.1 Evolution of Language Models: N-Grams, Word Embeddings, Attentio...","Evolution of Language Models: N-Grams, Word Embeddings, Attention & Transformers"
pdf1553.pdf,https://towardsdatascience.com/evolution-of-language-models-n-grams-word-embeddings-attention-transformers-a688151825d2,"#L2.1 Evolution of Language Models: N-Grams, Word Embeddings, Attentio...","Evolution of Language Models: N-Grams, Word Embeddings, Attention & Transformers"
pdf1554.pdf,https://thegradient.pub/understanding-evaluation-metrics-for-language-models/,#L2.3 Evaluation Metrics for SLMs,evaluation for language models
pdf1555.pdf,https://towardsdatascience.com/understanding-term-based-retrieval-methods-in-information-retrieval-2be5eb3dde9f,A great tutorial that first explains general term-based retrieval meth...,BM25 is a probabilistic retrieval framework that extends the idea of TF-IDF and improves some drawbacks of TF-IDF which concern with term saturation and document length. The full BM25 formula looks...
pdf1556.pdf,https://leimao.github.io/blog/Discriminative-Model-VS-Generative-Model/,A great blog that clearly explain the difference between generative mo...,"Sometimes, we would feel confused about the difference between a discriminative model and a generative model. This is because the mathematical definition was not explicitly defined.

In this blog..."
pdf1557.pdf,http://ctp.di.fct.unl.pt/~jmag/ir/slides/a05 Language models.pdf,"In addition to explaining basic probability of n-gram models, the slid...",#L2.3 Smoothing Approaches and Experimental Comparison
pdf1558.pdf,https://snlp2018.github.io/slides/n-grams-handout.pdf,"These slide go through basic concepts, real-life applications, and pro...","#L2.2 N-gram Language Models, Example, and Relating to MLE"
pdf1559.pdf,https://www.mygreatlearning.com/blog/pos-tagging/,This website explains the concepts and techniques of POS tagging in de...,#L2.1 Part of Speech Tagging with Hidden Markov Model
pdf1560.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec11.pdf,This page describes the derivation of the good turing smoothing method...,The good-turing smoothing method
pdf1561.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,This page describes how n-gram language models work #L2.1,N-gram Language Models
pdf1562.pdf,https://www.freecodecamp.org/news/bayes-rule-explained/,This page explains bayes rule and some of the uses for it #L1.2,Bayes' Rule – Explained For Beginners
pdf1563.pdf,https://medium.com/swlh/text-retrieval-vs-database-retrieval-3d13965ea067,This describes the main differences between the challenges faced in da...,Text Retrieval vs Database Retrieval
pdf1564.pdf,https://vitalflux.com/n-gram-language-models-explained-examples/,N-Gram Examples #L2.2,N-Gram Language Models Explained with Examples
pdf1565.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,N-Gram Introduction #L2.2,Language Models: N-Gram
pdf1566.pdf,https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9,N-Gram Introduction,Language Models: N-Gram
pdf1567.pdf,http://mlwiki.org/index.php/Statistical_Language_Models,Statistical Language Models Overview,Statistical Language Models
pdf1568.pdf,https://towardsdatascience.com/relevance-ranking-simplified-e8eeea829713,This describes how some of the basic ranking systems work and are deri...,"Relevance Ranking Simplified
How search engine actually works"
pdf1569.pdf,https://www.cs.toronto.edu/~amnih/papers/threenew.pdf,Three New Graphical Models for Statistical Language Modelling #L2.1,"Propose three new probabilistic language models
that define the distribution of the next word
in a sequence given several preceding words
by using distributed representations of those
words."
pdf1570.pdf,https://sigir.org/wp-content/uploads/2017/06/p268.pdf,"A Study of Smoothing Methods for Language Models
Applied to Ad Hoc In...",No Preview Available
pdf1571.pdf,https://www.geeksforgeeks.org/types-of-queries-in-ir-systems/,Types of Queries in IR Systems #L3.1,Types of Queries in IR Systems
pdf1572.pdf,https://web.stanford.edu/class/ee376a/files/scribes/lecture_notes.pdf,Stanford Information Theory Lecture Notes L1.3,No Preview Available
pdf1573.pdf,https://webspace.maths.qmul.ac.uk/p.j.cameron/notes/prob.pdf,Notes on Probability #L1.2,No Preview Available
pdf1574.pdf,https://medium.com/swlh/text-retrieval-vs-database-retrieval-3d13965ea067,Text Retrieval vs Database Retrieval #L1.1,No Preview Available
pdf1575.pdf,https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-356.pdf,A very clear explanation of probability. #L1.1,"This technical note describes straightforward techniques for document indexing and retrieval that have been solidly established through extensive testing and are easy to apply.
They are useful for..."
pdf1576.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,Smoothing for Language Models #L2.3,Smoothing for Language Models
pdf1577.pdf,https://safierinx-a.github.io/Katz-Backoff-Implementation/,Implementing Katz’s BackOff Model #L2.3,Implementing Katz’s BackOff Model
pdf1578.pdf,https://www.codingninjas.com/codestudio/library/smoothing-in-nlp,Major Smoothing Techniques #L2.2,Smoothing refers to the technique we use to adjust the probabilities used in the model so that our model can perform more accurately and even handle the words absent in the training set.
pdf1579.pdf,https://www.topbots.com/leading-nlp-language-models-2020/,Leading NLP Language Models #L2.1,"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
GPT2: Language Models Are Unsupervised Multitask Learners
XLNet: Generalized Autoregressive Pretraining for Langua..."
pdf1580.pdf,https://online.stat.psu.edu/stat415/lesson/1/1.2,Maximum Likelihood Estimation with three example solutions #L1.2,No Preview Available
pdf1581.pdf,http://boston.lti.cs.cmu.edu/callan/Workshops/lmir01/WorkshopProcs/Papers/lafferty.pdf,A comparison of document generation vs. query generation models for IR...,No Preview Available
pdf1582.pdf,https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf,An overview of probabilistic retrieval models based on document genera...,No Preview Available
pdf1583.pdf,https://web.stanford.edu/~jurafsky/slp3/3.pdf,A great introduction to N-Gram language models. #L2.3,Models that assign probabilities to sequences of words are called language model or LMs
pdf1584.pdf,https://aclanthology.org/H05-1025.pdf,"A great incite to predicting using N-Gram language model, #L2.2","Prediction of user behavior is a basis for the construction of assistance systems; it has therefore been
investigated in diverse application areas"
pdf1585.pdf,https://kmwllc.com/index.php/2020/03/20/understanding-tf-idf-and-bm-25/,tf-idf and BM-25 #L3.2,UNDERSTANDING TF-IDF AND BM-25
pdf1586.pdf,https://aclanthology.org/P03-1006.pdf,A great introduction to language models #L2.1,Generalized Algorithms for Constructing Statistical Language Models
pdf1587.pdf,https://www.britannica.com/science/information-theory,A great explanation of information theory,Information theory is a mathematical representation of the conditions and parameters affecting the transmission and processing of information.
pdf1588.pdf,https://www3.cs.stonybrook.edu/~skiena/jaialai/excerpts/node12.html,A great explanation of probability,"Probability deals with predicting the likelihood of future events, while statistics involves the analysis of the frequency of past events."
pdf1589.pdf,https://assets.cambridge.org/97811070/30657/excerpt/9781107030657_excerpt.pdf,What are Bayesian filtering and smoothing? #L2.3,No Preview Available
pdf1590.pdf,https://kavita-ganesan.com/what-are-n-grams/,N-grams model,No Preview Available
pdf1591.pdf,https://pypi.org/project/rank-bm25/,A Python package for BM25 #L3.2,No Preview Available
pdf1592.pdf,https://medium.com/@mlengineer/generative-and-discriminative-models-af5637a66a3,A useful page that helps understand the difference between generative ...,"A Generative Model ‌learns the joint probability distribution p(x,y). A Discriminative model ‌models the decision boundary between the classes."
pdf1593.pdf,https://www.analyticsvidhya.com/blog/2021/07/deep-understanding-of-discriminative-and-generative-models-in-machine-learning/,#L3.1 Introduction to Discriminative and Generative models in ML,No Preview Available
pdf1594.pdf,http://ethen8181.github.io/machine-learning/search/bm25_intro.html,Intuition and implementation of BM25 in #L3.2,Quick Introduction to Okapi BM25
pdf1595.pdf,https://aclanthology.org/P97-1064.pdf,Structured language model mentioned in #L2.1,No Preview Available
pdf1596.pdf,https://arxiv.org/pdf/1910.00577.pdf,An interesting application of structural language model for code compl...,No Preview Available
pdf1597.pdf,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/,A detailed discussion of possible explanations of Zipf's law #L2.2,No Preview Available
pdf1598.pdf,https://en.wikipedia.org/wiki/Conjugate_prior,"Wikipedia entry on conjugate prior, related to Dirichlet smoothing cov...",No Preview Available
pdf1599.pdf,https://medium.com/mti-technology/n-gram-language-models-70af02e742ad,How a N-gram LM differs form a Unigram LM in a real world example #L2....,No Preview Available
pdf1600.pdf,https://resources.mpi-inf.mpg.de/departments/d5/teaching/ws13_14/irdm/slides/irdm-3-3.pdf,Slide describing the RSJ (BIM) model with an example and how to apply ...,Binary Independence Model
pdf1601.pdf,https://watermark.silverchair.com/35-3-243.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAucwggLjBgkqhkiG9w0BBwagggLUMIIC0AIBADCCAskGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMLyYPYh6G8kObbmPiAgEQgIICmmI9jBnqwrWuYH27Wc2uRDhJzy7Py8A_JpdOhMaZilIQ0f1AnYAOibkmZ-tJf_57rsNp6FUG71PpvFHrdmSsFlD_Q79V7KtXyx059EuNyA63kWqYR5CDCvkQFQkW4rmfQpjUwl8vhf2xRDYop6ro01kq93du-aM80J6NReLOM7xbFG_6d4gU6vbRIXv2gnHu4SmhY365zw10w0g_rLyii2uGqw-96Y8nvwtiEnFmuJcD2n5JpuPoPLjqJlkZF7mzIitwGW8jVpYLUfVMKKgN8bHh0Vxg5srnZRIRdlTFpJzKPNAolMur7U4HTjhiqrf7VoMRPl_75s3xDnK9MDrI8wBx4UtUIDtzMYeaXvptT2WUns9AtDfIHtCeEJLZKvP_l17vSAuuVFQ5zfRn_Wl9kQDA1bWwCldpFocA_HINm3rMv1_r50xCealOH-siznnn7at07WUntVR3B1UsTOAPHb1tKYnsOQdwW6-UN7L5W0ny6v08usWzoErgmhNUuZGZr-hRIstCPbHC5bSjgxBwctIAM7pBWG1t8_9btaXz7WNluVGYRp4G4jhdYyOvLQMGCT9nGa-iS_MX62OrbWDR5pgyKl_Qhk76JQy2tBS6ykv87Da_FJAs5G4nVKKTYKeWixhURex_H2SivLLb_j5dFfg0GtcvpyVpafJovQ2kUJR4dgMghX1UvyZcbSjgsBAR_ULpH8sqICfhwGaNV4xENvwCsgpeTRBhrvL6FOA2CAozf37RGL2y2uTEpFG_3VyQGE5xMTlTKmuiVVUSe-0g12G25SOz_eiOrzpt1MHqcNaI1W-MyO7r6ANExe_H_w79COzYi5O_5RYxSmhfIVSq9vY2bp3AzPItqAL1RpN3WxHNRRI3DqtbzfBNPA,A review paper on Probabilistic Models in Information Retrieval. Relev...,Probabilistic Models in Information Retrieval
pdf1602.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html,The explanation of Query Model Estimation that covered in lecture #L3....,Estimating the query generation probability
pdf1603.pdf,http://mlwiki.org/index.php/Probabilistic_Retrieval_Model,More description about Probability of Relevance that covered in lectur...,Probabilistic Retrieval Model
pdf1604.pdf,https://sigir.org/wp-content/uploads/2017/06/p251.pdf,"#L3.1 Document Language Models, Query Models, and Risk Minimization fo...",A research paper present a framework for IR that combines the document and query generation model in
pdf1605.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,Expectation-maximization (EM) algorithm #L4.2,"Maximum likelihood estimation is an approach to density estimation for a dataset by searching across probability distributions and their parameters.

It is a general and effective approach that u..."
pdf1606.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,The okapi BM25 formula for standford #L4.1,"Okapi BM25: a non-binary model
The BIM was originally designed for short catalog records and abstracts of fairly consistent length, and it works reasonably in these contexts, but for modern full-t..."
pdf1607.pdf,https://www.quantamagazine.org/how-claude-shannons-information-theory-invented-the-future-20201222/,Claude Shannon And Information theory history,No Preview Available
pdf1608.pdf,https://www.quantamagazine.org/how-claude-shannons-information-theory-invented-the-future-20201222/,Claude Shannon And Information theory history,No Preview Available
pdf1609.pdf,https://www.sciencedirect.com/topics/neuroscience/information-theory,Information Theory,No Preview Available
pdf1610.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,Smoothing for Language Models #L3.2,Smoothing for Language Models
pdf1611.pdf,https://nlp.stanford.edu/IR-book/pdf/11prob.pdf,Probabilistic Information Retrieval #L3.1,No Preview Available
pdf1612.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,Introduction to Robertson/Sparck Jones Probabilistic Retrieval with ex...,Introduction to Robertson/Sparck Jones Probabilistic Retrieval
pdf1613.pdf,http://www.iro.umontreal.ca/~nie/IFT6255/Books/StatisticalLM.pdf,"Statistical Language Models
for Information Retrieval. Related to the...","Statistical Language Models
for Information Retrieval"
pdf1614.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,Introduction to Smoothing Techniques for Language Models as mentioned ...,No Preview Available
pdf1615.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,Explains Robertson-Sparck Jones Model  as mentioned in #L3.1,No Preview Available
pdf1616.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,A clear explanation and examples with the non-binary model for Okapi B...,Okapi BM25: a non-binary model
pdf1617.pdf,https://medium.com/analytics-vidhya/nlp-lecture-series-from-basic-to-advance-level-additional-content-1c1e51c9f936,A detailed extension for Probabilistic IR Model for #L3.1,"Probabilistic Retrieval
When we estimate how relevant a document is to a given query by feeding usual term and document frequency as parameters to a Bayesian model is termed as probabilistic retri..."
pdf1618.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,Lecture notes on Robertson-Sparck Jones Model. Relevant to Lecture #L3...,Robertson-Sparck Jones Model
pdf1619.pdf,https://kmwllc.com/index.php/2020/03/20/understanding-tf-idf-and-bm-25/,Compare TF-IDF and BM-25 #L3.2,UNDERSTANDING TF-IDF AND BM-25
pdf1620.pdf,https://maroo.cs.umass.edu/getpdf.php?id=592,"introduce dirichlet prior smoothing, dirichlet mixtures and the releva...",Dirichlet mixtures
pdf1621.pdf,https://maroo.cs.umass.edu/getpdf.php?id=592,"introduce dirichlet prior smoothing, dirichlet mixtures and the releva...",Dirichlet Mixtures for Query Estimation in Information Retrieval
pdf1622.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/ponte-and-crofts-experiments-1.html,a famous experiment of LM approaching to IR #L3.1,Ponte and Croft's Experiments
pdf1623.pdf,https://www.geeksforgeeks.org/what-is-information-retrieval/,"#L3.1: An clear explanation about the topic ""Information Retrieval"" co...",Information Retrieval
pdf1624.pdf,https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.4630270302,Original Paper in #L3.2,"Relevance weighting of search terms
S. E. Robertson, K. Sparck Jones"
pdf1625.pdf,https://www.researchgate.net/publication/327249698_Robertson-Sparck-Jones_Probabilistic_Model_Tutorial,A tutorial on the RSJ probabilistic model #L3.2,No Preview Available
pdf1626.pdf,https://medium.com/analytics-vidhya/nlp-lecture-series-from-basic-to-advance-level-additional-content-1c1e51c9f936,An article on Probabilistic Retrieval #L3.1,No Preview Available
pdf1627.pdf,https://www.countbayesie.com/blog/2019/6/12/logistic-regression-from-bayes-theorem,#L3.1 Logistic Regression with log odds explained,No Preview Available
pdf1628.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,"An in-depth read about the Robertson-Sparck-Jones (RSJ) model, with de...",#L3.3 Robertson-Sparck-Jones (RSJ) model and Binary Independence Model
pdf1629.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html,A refresh on the concept of TF-IDF weighting and a practice example to...,#L3.1 A Refresh on TF-IDF Weighting
pdf1630.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,#L3.2 Alternative explanation of BM25 model,Okapi BM25: a non-binary model
pdf1631.pdf,https://en.wikipedia.org/wiki/PageRank,#L3.1 PageRank: Example of a search model used for IR,PageRank
pdf1632.pdf,https://dl.acm.org/doi/abs/10.1145/564376.564387,document generation - language models,No Preview Available
pdf1633.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html,More notes on Language models for IR,No Preview Available
pdf1634.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,"#L3.2 A thorough deep-dive into the Robertson-Sparck Jones Model, incl...",No Preview Available
pdf1635.pdf,http://mlwiki.org/index.php/Probabilistic_Retrieval_Model,#L3.1 A condensed but helpful overview of probabilistic retrieval mode...,No Preview Available
pdf1636.pdf,https://www.researchgate.net/publication/327249698_Robertson-Sparck-Jones_Probabilistic_Model_Tutorial,#L3.2 An introduction of Robertson-Spärck-Jones (RSJ) model,Robertson-Spärck-Jones Probabilistic Model Tutorial
pdf1637.pdf,https://en.wikipedia.org/wiki/Relevance_(information_retrieval),#L3.1 A wikipedia page that explains Relevance in Information Retrieva...,Relevance (information retrieval)
pdf1638.pdf,https://stats.stackexchange.com/questions/434749/maximum-likelihood-as-minimizing-the-dissimilarity-between-the-empirical-distriu,A proof of maximum likelihood can be considered as minimizing the diss...,"Intuition:

With this said, I believe we can think of using the KL divergence for maximizing the likelihood as a way of making the predicted distribution (pmodel(X,θ))
(
���
���
���
���
���..."
pdf1639.pdf,https://math.stackexchange.com/questions/610165/prove-that-the-bm25-scoring-function-is-probabilistic,a good explanation of BM25 score #L3.2,"no. of documents which contain term qitotal no. of documents
no. of documents which contain term 
���
���
total no. of documents
is the probability that a single document, sampled uniformly fr..."
pdf1640.pdf,https://arxiv.org/abs/0705.1161,About Robertson-Spärck Jones model in #L3.2,A simple new derivation within the Robertson-Spärck Jones probabilistic model
pdf1641.pdf,https://en.wikipedia.org/wiki/Probabilistic_relevance_model,About Robertson-Sparck Jones Model in #L3.2,Probabilistic relevance model
pdf1642.pdf,https://www.researchgate.net/publication/327249698_Robertson-Sparck-Jones_Probabilistic_Model_Tutorial,#L3.2 Robertson-Spärck-Jones Probabilistic Model Tutorial,Robertson-Spärck-Jones Probabilistic Model Tutorial
pdf1643.pdf,https://www.cl.cam.ac.uk/teaching/1718/InfoRtrv/slides/lecture5-language-models.pdf,A much detailed slide from Cambridge on Language Modeling in Informati...,No Preview Available
pdf1644.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/probability-estimates-in-practice-1.html,Explains the Croft and Harper RSJ model. #L3.2,No Preview Available
pdf1645.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/relevance-feedback-and-pseudo-relevance-feedback-1.html,#L4.2 Another example - Involve the user directly in the feedback proc...,Relevance feedback and pseudo relevance feedback
pdf1646.pdf,https://en.wikipedia.org/wiki/Query_likelihood_model,The article goes over the query likelihood model. #L3.1,No Preview Available
pdf1647.pdf,http://www-personal.umich.edu/~qmei/pub/sigir07-poisson.pdf,#L4.1 Query generation using Poisson distribution,No Preview Available
pdf1648.pdf,http://www.cs.cornell.edu/courses/cs630/2006sp/guides/lec8.hp.pdf,#L3.2 RSJ Model,"Review of Probabilistic Retrieval models
1.2.1 Classic Probabilistic Retrieval: scoring by P(R=y|D=d)
Scoring functions for classic probabilistic retrieval were derived from P(R=y|D=d) where R an..."
pdf1650.pdf,https://www.nintex.com/process-automation/document-generation/learn/what-is-document-generation/,Introduction to document generation #L3.2,Understanding document generation
pdf1651.pdf,https://medium.com/analytics-vidhya/nlp-lecture-series-from-basic-to-advance-level-additional-content-1c1e51c9f936,Introduction to Probability of Relevance #L3.1,Probability of relevance
pdf1652.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html,#L3.1 Estimating the query generation probability using Maximum Likeli...,The probability of producing the query given the LM  of document using maximum likelihood estimation ( MLE )
pdf1653.pdf,https://towardsdatascience.com/understanding-term-based-retrieval-methods-in-information-retrieval-2be5eb3dde9f,This webpage talks about the most common term-based retrieval methods ...,Understanding Term-Based Retrieval Methods in Information Retrieval
pdf1654.pdf,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LearningBM25MSRTechReport.pdf,"A paper about"" A Machine Learning Approach for Improved BM25 Retrieval...","A Machine Learning Approach for Improved BM25
Retrieval"
pdf1655.pdf,https://www.youtube.com/watch?v=TaboD-gmHPo,A video that provides an introduction to the Probabilistic Retrieval ...,Lecture 19-Probabilistic Retrieval Model | UIUC
pdf1656.pdf,https://web.stanford.edu/class/cs276/handouts/lecture11-probir.pdf,Slides from Stanford containing information about the Probability Rank...,No Preview Available
pdf1657.pdf,https://link.springer.com/chapter/10.1007/978-3-540-31871-2_19,A book chapter that introduces estimating query language models with r...,No Preview Available
pdf1658.pdf,https://en.wikipedia.org/wiki/Tf–idf,About TF and IDF in #L3.2,tf–idf
pdf1659.pdf,https://towardsdatascience.com/understanding-term-based-retrieval-methods-in-information-retrieval-2be5eb3dde9f,"An introduction of term-based IR model (BM25, TF-IDF, Query Likelihood...",No Preview Available
pdf1660.pdf,https://findwise.com/blog/how-relevance-models-work/,#L3.2 Working of relevance model/Document Generation,No Preview Available
pdf1661.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,Explanation of Okapi BM25 covered in Lecture #L3.2,Okapi BM25: a non-binary model
pdf1662.pdf,https://medium.com/analytics-vidhya/nlp-lecture-series-from-basic-to-advance-level-additional-content-1c1e51c9f936,#L3.1 A good article of Probabilistic Relevance,No Preview Available
pdf1663.pdf,https://www.researchgate.net/publication/239066003_Probabilistic_IR_models_based_on_document_and_query_generation,Probabilistic IR Model,No Preview Available
pdf1664.pdf,https://medium.com/analytics-vidhya/nlp-lecture-series-from-basic-to-advance-level-additional-content-1c1e51c9f936,A useful explanation of Probability of relevance covered in Lecture #L...,"Probability of relevance:
Probability of relevance of a document ‘D’ given a query ‘Q’ can be expressed as: P(Rᵩ = X|D)"
pdf1665.pdf,https://www.quora.com/How-does-BM25-work,#L3.2 How does BM25 work,No Preview Available
pdf1666.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,#3.2 Introduction to Robertson/Sparck Jones,No Preview Available
pdf1667.pdf,http://mlwiki.org/index.php/Probabilistic_Retrieval_Model,#3.1 More information about Probabilistic retrival model,No Preview Available
pdf1668.pdf,https://arxiv.org/pdf/2202.05144.pdf,#3.2 A paper introducing a method of data augmentation for information...,"In this work, they harness the fewshot capabilities of large pretrained language models as synthetic data generators for IR tasks."
pdf1669.pdf,https://openreview.net/pdf?id=fSfcEYQP_qc,#3.2 A paper introducing a neural corpus indexer for document retrieva...,"This paper designed a sequence-to-sequence model, named Neural Corpus Indexer (NCI), which generates relevant document identifiers directly for a specific query. In their experiments, the proposed ..."
pdf1670.pdf,https://en.wikipedia.org/wiki/Information_retrieval,Inroducing of Information retrieval concepts #L1.1,Information retrieval
pdf1671.pdf,https://arxiv.org/pdf/0705.1161.pdf,Justify the effectiveness of inverse document frequentcy based a more ...,IDF Revistited: A simple New Derivation within the Robertson-Sparck Jones Probabilistic Model
pdf1672.pdf,http://boston.lti.cs.cmu.edu/callan/Workshops/lmir01/WorkshopProcs/Papers/lafferty.pdf,A description of framework which combined 2 retrivel models:document g...,Probabilistic IR Models Based on Document and Query Generation
pdf1673.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,A useful description of RSJ model covered in # L3.2,The main goal of probabilistic retrieval is to rank documents by the probability that they are relevant.
pdf1674.pdf,https://aspoerri.comminfo.rutgers.edu/InfoCrystal/Ch_2.html,A useful explanation of probabilistic models covered in # L3.1,"The probabilistic retrieval model is based on the Probability Ranking Principle, which states that an information retrieval system is supposed to rank the documents based on their probability of re..."
pdf1675.pdf,https://doc.dataiku.com/dss/latest/machine-learning/model-document-generator.html,Another good article about LM with document generation shown in Lectur...,You can use the Model Document Generator to create documentation associated with any trained model.
pdf1676.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html,This book for Stanford gives a good intro to LM show in lecture #3.1,"Language models for information retrieval
A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words ..."
pdf1677.pdf,https://sigir.org/files/museum/pub-21/36.pdf,L3.2: Probabilstic Models of Indexing and searching - Explains the pro...,No Preview Available
pdf1678.pdf,https://towardsdatascience.com/evaluation-of-language-models-through-perplexity-and-shannon-visualization-method-9148fbe10bd0,L3.1 Evaluation Metric of Language Model,No Preview Available
pdf1679.pdf,https://www.emerald.com/insight/content/doi/10.1108/eb026683/full/html,Probabilistic Models of Relevance Information,No Preview Available
pdf1680.pdf,https://www.cs.helsinki.fi/u/bmmalone/probabilistic-models-spring-2014/PoissonMixtureModels.pdf,#L3.2: Poisson Mixture Model,No Preview Available
pdf1681.pdf,https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/,L3.2: An interesting look at the current state-of-art language models ...,The emerging types of language models and why they matter
pdf1682.pdf,http://ciir.cs.umass.edu/pubfiles/ir-318.pdf,L3.1: This article reviews various language models applied in the fiel...,Statistical Language Models Review
pdf1683.pdf,http://mlwiki.org/index.php/Probabilistic_Retrieval_Model,Some useful information on Probabilistic Retrieval Model#L3.1,No Preview Available
pdf1684.pdf,https://www.researchgate.net/publication/327249698_Robertson-Sparck-Jones_Probabilistic_Model_Tutorial,"#L3.2: A tutorial about the topic ""Robertson-Sparck Jones Model"" cover...",Robertson-Spärck-Jones Probabilistic Model
pdf1685.pdf,https://resources.mpi-inf.mpg.de/departments/d5/teaching/ws13_14/irdm/slides/irdm-3-3.pdf,#L3.1: Probabilistic Retrieval Models with Some Examples,No Preview Available
pdf1686.pdf,https://github.com/nhirakawa/BM25,A Python implementation of the BM25 ranking function #L3.2,No Preview Available
pdf1687.pdf,http://ciir.cs.umass.edu/pubfiles/ir-171.pdf,A introduction of a new statistical language modeling for IR #L3.1,No Preview Available
pdf1688.pdf,http://www.cs.cmu.edu/~lemur/background.html,Related article for #3.2,"A statistical language model, or more simply a language model, is a probabilistic mechanism for generating text. Such a definition is general enough to include an endless variety of schemes. Howeve..."
pdf1689.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html,Introduction to IR #3.1,Language models for information retrieval
pdf1690.pdf,http://marksanderson.org/publications/my_papers/Challenges-in-IR.pdf,Challenges in Information Retrieval and Language Modeling .,No Preview Available
pdf1691.pdf,https://serpact.com/search-engine-ranking-models-ultimate-guide/,An introduction to search engine ranking models mentioned in Lecture 3...,No Preview Available
pdf1692.pdf,https://www.staff.city.ac.uk/~sbrp622/blockbuster/pmir-pt2-reprint.pdf,A probabilistic Model of Information Retrieval L3.1,No Preview Available
pdf1693.pdf,https://en.wikipedia.org/wiki/Probabilistic_relevance_model,Probabilistic relevance model intro,The probabilistic relevance model[1][2] was devised by Stephen E. Robertson and Karen Spärck Jones as a framework for probabilistic models to come. It is a formalism of information retrieval useful...
pdf1694.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,#L3.2,No Preview Available
pdf1695.pdf,https://web.stanford.edu/class/cs276/handouts/lecture11-probir.pdf,#L3.1 Stanford lectures on relevance probability,No Preview Available
pdf1696.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,intro to rsj model #3.1,No Preview Available
pdf1697.pdf,https://link.springer.com/chapter/10.1007/978-3-540-31871-2_19,Feedback Query Model #L3.2,No Preview Available
pdf1698.pdf,https://developers.google.com/machine-learning/gan/generative,Generative Model #L3.1,No Preview Available
pdf1699.pdf,http://personal.psu.edu/jol2/pub/csda04.pdf,#L3.2: A Paper on Poisson Mixture Model in Document Classification and...,No Preview Available
pdf1700.pdf,https://dl.acm.org/doi/pdf/10.5555/188490.188560,Inferring Probability of Relevance Using the Method of Logistic Regres...,No Preview Available
pdf1701.pdf,https://www.capgemini.com/insights/expert-perspectives/generative-language-models-and-the-future-of-ai/,An interesting article about the Generative Language Models #L#3.1,GENERATIVE LANGUAGE MODELS AND THE FUTURE OF AI
pdf1702.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,A short explanation of BM25 #L3.2,No Preview Available
pdf1703.pdf,https://www.youtube.com/watch?v=TaboD-gmHPo,Other video about Probabilistic Retrieval Model from our professor. #L...,Probabilistic Retrieval Model
pdf1704.pdf,https://www.researchgate.net/publication/327249698_Robertson-Sparck-Jones_Probabilistic_Model_Tutorial,#L3.2 - Helpful tutorial with working of Robertson-Sparck Jones Model,No Preview Available
pdf1705.pdf,https://nlp.stanford.edu/IR-book/pdf/11prob.pdf,#L3.1 - Deeper understanding of the concepts of probabilistic retrieva...,No Preview Available
pdf1706.pdf,https://www.cs.virginia.edu/~hw5x/Course/IR2021-Spring/_site/docs/PDFs/Probabilistic Ranking Principle.pdf,Document Generation in detail mentioned in Lecture 3.2,No Preview Available
pdf1707.pdf,https://github.com/fangrouli/Document-embedding-generation-models,An interesting repo containing code and information for Document Gener...,Document-embedding-generation-models
pdf1708.pdf,https://en.wikipedia.org/wiki/Okapi_BM25,An overview of the BM25 model #L3.2,Okapi BM25
pdf1709.pdf,https://www.engati.com/blog/applications-of-generative-language-models,An interesting introduction to GPT-3 which is a very popular generativ...,GPT-3 by Open AI is the most well-known and most anticipated among other popular language models. It has been trained with 175 billion parameters. Many applications have been built with GPT-3 to de...
pdf1710.pdf,https://watermark.silverchair.com/35-3-243.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsswggLHBgkqhkiG9w0BBwagggK4MIICtAIBADCCAq0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMLwGmlkefl66cp4pvAgEQgIICfq7Af_FbhjnLBu2CIr77-dd0zGnUgj2mCNAzD8RPrVP6jZKmlVT3LnwhCLwdysARGnIzPd7uspNG0d5iUUKfaIz92fRWcZFSzCsRCw-YSYfuVRCFH5KhT5LrkqxtHjIwEpLIrJSbsrsJgQGuXs3Czvc39giQ7xXbf993g0VGIO1VKua5FeXpy0MgzMsZMcVEXkxsrqaXdfP5Ma1H4QzQhdPejK8MBaBwkgkRCEF-N3rNYVeJwjcvr0ltEt-2dBGNkoCWIekVy6EnFlgE3IdJDLOt1K67eQu1Y-0V0YkHYRhALVTZ2ST1yao2xlpsnEdxl58YbnOWMB-BMYv05wqICsOHcQmVOMUTLXLcC4WHPlHh8aSujU8eZFIG9DvlVOQEwG2_bPAArOsyYjY7gibF23CPkhtCVAobniii9MsHbrNBNQanY1EU6t7B_VFjIUGUiQbeiF5uAF-uZYbVAZymwsBg8xOLIa9CEOyww62g1CQh_bLhYH936v6BFcC52kT9qrYrtqji-LbhgLX6wx2wlIaQY4QQzH30UxeQbbZtcQ0ypCLoqnbt9DIX3X8sEdL3l4twb8GnAdPCeHF8oE6W70De4CNRU4QzxgIAMzeUomK6ZlVBNdR2BKxgO8bdTNaQ42oXQvaPvIU4USSowP2hHP6QaGbwo-FShLHkY-P3W07edxgR3Wgx6sECQEnMeCL-OWgseYPzzAMpUgODcY9KX_A8_ASEoWgSga8TxuGdKljtJ822g_DSI2pRC0RZFPVj2HM0qAKMc_yyVmFDQOXlSv31jgzTgBvi62359kmyB65-uAMsQSAoVMtqrcJqO_6RrlW3yhbsLiFSbWjPofnY,#L3.1 A paper on Probabilistic Retrieval Models,No Preview Available
pdf1711.pdf,https://link-springer-com.proxy2.library.illinois.edu/content/pdf/10.1007/978-3-031-02130-5_5,a useful explanation of probabilistic distance retrieval models covere...,"In this
chapter, we introduce a new family of models called probabilistic distance retrieval models that can
better accommodate feedback.
In these models, we would represent a document with a do..."
pdf1712.pdf,https://www.elastic.co/guide/en/elasticsearch/guide/current/pluggable-similarites.html,This page introduces BM25 tuning. #L3.2,Tuning BM25
pdf1713.pdf,https://en.wikipedia.org/wiki/Okapi_BM25,BM25 reference#L3.1,No Preview Available
pdf1714.pdf,https://aspoerri.comminfo.rutgers.edu/InfoCrystal/Ch_2.html,a useful explanation of probabilistic retrieval model covered in Lectu...,"The probabilistic retrieval model is based on the Probability Ranking Principle, which states that an information retrieval system is supposed to rank the documents based on their probability of re..."
pdf1716.pdf,https://arxiv.org/pdf/0705.1161.pdf,A Simple derivation within the Robertson-Sparck Jones Probabilistic Mo...,No Preview Available
pdf1717.pdf,https://en.wikipedia.org/wiki/Okapi_BM25,Useful wikipedia for Okapi BM25#L3.2,Okapi BM25
pdf1718.pdf,https://archive.org/stream/arxiv-0705.1161/0705.1161_djvu.txt,#L3.2 An overview of different document generation models,"IDF Revisited: A Simple New Derivation within the 
Robertson-Sparck Jones Probabilistic Model"
pdf1719.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,Description of Okapi BM25#L3.2,Okapi BM25: a non-binary model
pdf1720.pdf,https://web.stanford.edu/class/cs276/19handouts/lecture7-probir-1per.pdf,Set of slides by on Probabilistic Information Retrieval #L3.2,Probabilistic Information Retrieval
pdf1721.pdf,https://arxiv.org/pdf/1707.08052.pdf,Research paper on document generation,No Preview Available
pdf1722.pdf,https://developers.google.com/machine-learning/gan/generative,#L3.2,"A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative m..."
pdf1723.pdf,http://ciir.cs.umass.edu/pubfiles/ir-171.pdf,A paper on different language modeling techniques for IR #L3.1,A General Language Model for Information Retrieval
pdf1724.pdf,https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/,This explains about BM25 #L3.2,Build your own NLP based search engine Using BM25
pdf1725.pdf,https://aspoerri.comminfo.rutgers.edu/InfoCrystal/Ch_2.html,#L3.1,"The probabilistic retrieval model is based on the Probability Ranking Principle, which states that an information retrieval system is supposed to rank the documents based on their probability of re..."
pdf1726.pdf,https://pdf.sciencedirectassets.com/271647/1-s2.0-S0306457300X00313/1-s2.0-S0306457300000169/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELP//////////wEaCXVzLWVhc3QtMSJHMEUCIDYC/8jU6tFUm0TOuDnUFGRH/rpqhMpKZ2oUawafkT7WAiEA7LIhDRWzOoQDiovFSqLClMVVVyaRwwB/ITRxYRiZ16wqzAQIPBAFGgwwNTkwMDM1NDY4NjUiDG8B18y/J/dK43G/HiqpBG3mmi9xUD21/j0SAjrwIk2xUeULxugBNVr+RyJQqNu62DD3TUlbsL848I8ycDnhx5USOTmjxCQxAPkCYI2S/tYBq/Lt07bryBrziZqdat5A6QleNXch0HrIeBuXnWA+Jl14JXt0Eq6Az42EJlbG5lzsPohkX4t4TQCM/eYtycEj45v6Ge4faIqpBRMyKo4xX9c1nKSFCGnHvnxnlVMZ8CaR5d8pgyrAybQjM9TymwbYRgXTPZIMVSyPtK6Lrh+Id7U+owV4Bz9BSuvWyIl6fP+3dO2D9La9HS9LnKmXr9zCAAgSdcm+SqFYMMrsgJNDO+/3pL2JW52RhUye5rNEfGg7SaEXIlj1OMDCtXgUH0frtbfPfKghPdyNDDBUHC3DCloqh6QOxV1vj9zQrMlnoivLYr1hKCb2Pj82ffXY89dndNxfLYuNCB+UvL5FrZsjoqoy9XT7r8pZZMK/wSsSN4nWwPqwKg+9iKOl8wwDokp/T0mzzZ5ksxv14OY0vg+6UqIXjDyqtNjONUKrNPOcUd1NPqtJRxLDQl5kW/Hh5bO9ljUU6hZOQrYj+FG1h1W5ZlSuuOasvF7APkoRaPdmrFKmctsJhrsNP/S50CqQP+gqdDxv8eytEXl+8O2UvjCKNC4F2MxmNQvKu2RUJZMQQjbnsReDOpYV47Sw+I5O0oY8thN2AueiGVLSeP7B2mdwry8f72++Zij3GO7EaC12sFl6b/1awcLEN88wsdWBnwY6qQFvWsbFfHTOrJRykshFs5p3Ijhyd9sZjnhyVdYq0gOlriHCyQdInJZuJSE+8xm3GYwHrMJebpQboB5emmVCbwrlbZM7EV+nOKrjXfSJKCj+QouQ2yBL4bLW70HuX7+32GlgcjeoTB9vs+EcJV7557ed+dCDxp5vVucq98iT59IAWqzQoX1DpvJE2TApJvjrlHTORIvEOyPKveMJFqmLl0JDhmJLz/Q6l9gT,#3.2 Sparck Jones Model,No Preview Available
pdf1727.pdf,https://watermark.silverchair.com/35-3-243.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsswggLHBgkqhkiG9w0BBwagggK4MIICtAIBADCCAq0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM3kAOpCtiB0ga4XiQAgEQgIICfs5NQIoV5ln1y9aWMpYhFXwCbMydjfhd49J4cvtNNWb9lYWv21Ay6vjP_2PQcfJqY2D9XUtqX2_uqm2eKzsZkzbT8g6dZpAq5YTPo6GzNAMCqFTwtA1lc1UhO7bkBloHtF7L4JHfJEuamAI_Lm2xx-IBEyx3JnNHi2v3D1AWSdFunwLvLuQ_hCELYDtaNnlJgaP72PXrcn7B-eYOMCimZ-0QH34XJK4_xVAgOlXfXBLkv_wUsnipxr7elPR66kPAlOT8zWTcfTSWcLbhv8q5gJ88gy-F83bau1x1FjYSOkpE8FT4jaxKYFPKfPGjS2hPuBpbNr1w82GDMtLzlFsxuCiEoOvhCEqBoVXsTwFN7Fin5MkXv2VRZrupKXDEz3UZOUFkw-RpgQDvqYZnD6YRR0g5KtSJfDNd-b1fczMRVbuLAagoLNkA9QASopT7kUK4vN-elvuOIJziQ-GfCnLjnOWJ40DiE_nkIGPocV4h6_cg5rVyTSJdevkwHbX7z-uF6xeKnPAMAWjVs7S561uIB9L5esDDNkQ_eFpFtEB_e66uSv6aZ3QyDZ9LI5aVP6WxF-HM4fUmqy-eueIw7VrbhYrLOSSetyexofASSiPMhUhW4sT6dnPjwYLZRnpjG0OCqrOlX-1GDxJKFqU_UN-yixpgdehKNuQkOo8B3-YP_y2FiQFPOnO0aHBXI4YUVqF5wxtqcmilnSDVMPqhYhOySkCYNaY7n1nJqWwlCUIS9lsnovMdJV3WjTpF6QWcPWePBf8FnLChCG8Y3809Xn-e9x6owrCvPqGd31YYaJ5GrfX7YhHkhF1UjG8RPT3UAirztAWMKrd_dDoft9G-Xow6,an introduction and survey over probabilistic information retrieval (I...,No Preview Available
pdf1728.pdf,https://watermark.silverchair.com/35-3-243.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsswggLHBgkqhkiG9w0BBwagggK4MIICtAIBADCCAq0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMsOHN0hzl3Pe2VAeyAgEQgIICfjIdNhHjLaidZwfsRP6sDcCvuVrbwKaqugaeghDbYPYHFHTX2GiHspNxPuUvI3D0CK-Bn8My0rwIAebj6EuAgsSH7ekTImiqjk9-rISwkdvkNHYfja3Hynimc-gn77-QeoDO5kT6ZAptk4DlFcwHv-mS8T5qtacjK0uj3HXKq6qgiKqWiZwyDw_lL2aM6PNVnDFrR4jaYWyOljY0AJF3lrn3g8YT2tr2qbE5wMoLMsQ6HM3o6vIiQyBEnBFUMAmvmUiLkN3HLlLuPk5ooqfePmFmrW8d7AgVVRuLWAzj42rR8wJb7lQQu_kCecxgTm0jTuf5t-ywxePnNwaNZkJ0lJ620qnP6qXXi3ZpR90BiFI8S-C_IBXxfUQZ_YqeY6uOe8rmkFeYLKqbNDAtAa91psoGa6HHxODmovnzZ3czXHAHSo8xL48kGdbZB9_z6iO8IRcEzabHaiaKkf84N0y1-gNp7oMJvZlH5g8RJW3rvfxx9xYTyW0iv92d-t2qaUgnWLsCFSCLvrCGRAnbzwK38hzaUyGcmt04w3u7mkVpADO0tW_TKuioa-p0_KOxK_5TmD92swsnDuw9RrJiNjaBFd_arCHW-7hyRzQvMBJHobB36DwZxpCE2bAaHISzlm_Mp9XKSh1pZvgdyZ0mbgZtg_1pcR-WYRBPfqFcunW4rkOKrFMa2BX2qX_ePMZ0DIT5b7h0yFrAMIauoXQHGP1s9OOFjWMHy1aeTQus29BoKsykz_vgRrhoRKSYbtOlLVfnvBkN_6c02qNwTqOMUBAUMIYl1l6bGxAxl7jgMoSk2cssS0_5UTgpi6dW2PQEr434w9PTrPuNW_Z_iDEvp6qJ,#3.1 Probabilistic models in information retrieval,No Preview Available
pdf1729.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,#L3.2 An introduction to RSJ Model,No Preview Available
pdf1730.pdf,https://watermark.silverchair.com/35-3-243.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsswggLHBgkqhkiG9w0BBwagggK4MIICtAIBADCCAq0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMREbE8DEmZtX4f-w8AgEQgIICfh2Oz7I-3rWr2dWQ35ANNMJ4YPLniEDwOb14kKJxU0SPFvmp-wgobKPi4-_F48_GnZVZPI2ZZfP-gR12tW0tYFJb-19r92UwScjaptjYa7L6QhCXnBdnH281OWOFyZk0on07qjLHKm7OJfxg_dJ7cyEvYdFw8n1EQwC0lSPBVQC3G9iGXgDxVNy0dcxKOguL516k7vdPn2VUYcmPk3ZioNXUMmNpUGJ2mR_Q7oukLrOf4KIp8KckL7VdvjxX7N-q6pjevuG-NbQVp8Jr8Xrd6mYi9ASNxa8Sb0sypOaruRJv5apgRW44H16PreS0H7vosMpLalk2aTqFb8G4HHwiVRJcxqkhQSPFflSrSD0efyDRPNVRxriKQx1z9G2EpgtZekSM-Nz5HAmyW_dGaPCgJxc_czrg73VTMGoxI2ME9HVzbo9Kk9Lv8vwLBQEwoBY2TDnjtKUN6iqWA9E37fgZOUtwpkab_f2pN2fWaguLEFMqZKIkFQgaOnpfogf8_NbA0UakcNbJblMoIwj72NM5vWS2qjIkD7V0uye1ZKSvR0clag04DiFVlT-J0368WTN1ldK_exKj0SUyzLweVOhgtwOow0dO8PH_IVGGMQ23dxXK5e3xBtV27JXWStIFyeepULgYyC-w7MlClQNYqsBxNPjg76G8K_FCRxjhDmh2QldttvNjV0QMLpmlsmKRSic-_5PlZN6w9WaCkKbfrhR3hxc9-gFxDw053UNBD5q8V6iijxFqBSKVVqy2-7DdcluLByyB0MEHO-KFNWm5K4_6rYREw_5bIC71e4sgRI6bGptz4TAkFYnD2gozgJhZg6wyfVKPObqNPjF-RvbyrKde,Introduction about Probabilistic Retrieval Models #L3.1,Probabilistic Models in Information Retrieval
pdf1731.pdf,https://medium.com/swlh/relevance-ranking-and-search-a98b35ebc7b3,Rankings and introductions on some traditional IR models,"1960s — researchers were testing web search engines on about 1.5 megabytes of text data. Fast forward to 2018, we now have billions of web pages and colossal data. Though one issue which still pers..."
pdf1732.pdf,https://medium.com/nlplanet/two-minutes-nlp-20-learning-resources-for-information-retrieval-16f8405f0892,A guide for learning Information Retrieval,"Here follows the first draft, curated by me, of the Information Retrieval learning resources of NLPlanet. Being a draft, this list will be improved using the feedback of the community.

This arti..."
pdf1733.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec09.pdf,Review of the Language Modeling Approach #L4.2,No Preview Available
pdf1734.pdf,https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-356.pdf,The original RSJ Paper,No Preview Available
pdf1735.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,Okapi BM25 #L4.1,"The BIM was originally designed for short catalog records and abstracts of fairly consistent length, and it works reasonably in these contexts, but for modern full-text search collections, it seems..."
pdf1736.pdf,https://en.wikipedia.org/wiki/Okapi_BM25,BM25#L3.2,No Preview Available
pdf1737.pdf,https://www.forbes.com/sites/garydrenik/2023/01/11/large-language-models-will-define-artificial-intelligence/?sh=dcd2a9db60f5,"Since lecture #L3.1 started with language models, I am sharing this re...",No Preview Available
pdf1738.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,RSJ Model,No Preview Available
pdf1739.pdf,https://github.com/caiyinqiong/Semantic-Retrieval-Models,"#L3.2 Lists of papers related to IR, including KL-divergence retrieval...","A curated list of awesome papers for Semantic Retrieval, including some early methods and recent neural models for information retrieval tasks (e.g., ad-hoc retrieval, open-domain QA, community-bas..."
pdf1740.pdf,https://www.researchgate.net/publication/327249698_Robertson-Sparck-Jones_Probabilistic_Model_Tutorial,A useful explanation of Robertson-Spark Model covered in Lecture #L3.2,Robertson-Spärck-Jones Probabilistic Model Tutorial
pdf1741.pdf,https://smithamilli.com/blog/kneser-ney/,"An article explaining Kneser-Ney Smoothing, which is based on the idea...","Kneser-Ney Smoothing
The solution is to “smooth” the language models to move some probability towards unknown n-grams. There are many ways to do this, but the method with the best performance is i..."
pdf1742.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,The author provided a comparison for scoring functions when applying R...,#3.1 intro to Robertson-Sparck Jones Model
pdf1743.pdf,https://medium.com/analytics-vidhya/nlp-lecture-series-from-basic-to-advance-level-additional-content-1c1e51c9f936,Explains Probabilistic Retrieval in basic words #L3.2,Probabilistic Retrieval
pdf1744.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/language-modeling-versus-other-approaches-in-ir-1.html,A comparison between language model and other IR approaches,Language modeling versus other approaches in IR#L3.2
pdf1745.pdf,http://www.cs.cornell.edu/courses/cs630/2006sp/guides/lec8.hp.pdf,introduction to language model for lecture 3.1,Probabilistic Retrieval Model#L3.1
pdf1746.pdf,https://medium.com/analytics-vidhya/nlp-lecture-series-from-basic-to-advance-level-additional-content-1c1e51c9f936,A useful explanation of Probabilistic Models covered in Lecture #L3.2,Probabilistic Retrieval
pdf1747.pdf,https://en.wikipedia.org/wiki/Probabilistic_relevance_model,A useful explanation of Probabilistic Models covered in Lecture #L3.1,Probabilistic relevance model
pdf1748.pdf,https://aspoerri.comminfo.rutgers.edu/InfoCrystal/Ch_2.html,A useful explanation of Probabilistic Models covered in Lecture #L3.1,Probabilistic Model
pdf1749.pdf,http://boston.lti.cs.cmu.edu/callan/Workshops/lmir01/WorkshopProcs/Papers/lafferty.pdf,Document generation and query generation #L3.2,No Preview Available
pdf1750.pdf,https://nlp.stanford.edu/IR-book/pdf/11prob.pdf,Probabilistic information retrival#L3.1,No Preview Available
pdf1751.pdf,https://www.semanticscholar.org/paper/IDF-revisited:-a-simple-new-derivation-within-the-Lee/69bf203c39fd734a96e6891748d3ada683940dd6,Paper making a separate derivation of the Robertson Sparck model #L3.2,There have been a number of prior attempts to theoretically justify the effectiveness of the inverse document frequency (IDF). Those that take as their starting point Robertson and Sparck Jones's p...
pdf1752.pdf,https://en.wikipedia.org/wiki/Lagrange_multiplier,The wiki help to further explain Lagrange multiplier used in lecture,No Preview Available
pdf1753.pdf,https://citeseerx.ist.psu.edu/document?repid=rep1,#L3.2 A paper related to LM estimation,"Despite the availability of better performing techniques, most
language models are trained using popular toolkits that do not
support perplexity optimization. In this work, we present an efficient ..."
pdf1754.pdf,http://ciir.cs.umass.edu/pubfiles/ir-171.pdf,A General LM for IR that is simple and intuitive,"A language model for information retrieval, which is based on a range of data smoothing techniques"
pdf1755.pdf,https://en.wikipedia.org/wiki/Okapi_BM25,#L3.1 Description of BM25,"In information retrieval, Okapi BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is base..."
pdf1756.pdf,https://www.pnas.org/doi/10.1073/pnas.1303108110,This explains principles of parameter estimation #L3.1,Principles of parametric estimation in modeling language
pdf1757.pdf,https://en.wikipedia.org/wiki/Generative_model,Explains about Generative model #L3.1,Generative model
pdf1758.pdf,https://www.analyticsvidhya.com/blog/2021/07/deep-understanding-of-discriminative-and-generative-models-in-machine-learning/,Explains difference between Generative and Discriminative models #L3.1,Deep Understanding of Discriminative and Generative Models in Machine Learning
pdf1759.pdf,https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf,This paper introduce the concept of Probabilistic Relevance Framework ...,"Probabilistic Relevance Framework (PRF) is a formal framework
for document retrieval, grounded in work done in the 1970–1980s, which
led to the development of one of the most successful text-retr..."
pdf1760.pdf,https://www.geeksforgeeks.org/types-of-queries-in-ir-systems/,A blog article about types of queries. #L4.1,"The Information Retrieval (IR) system finds the relevant documents from a large data set according to the user query. Queries submitted by users to search engines might be ambiguous, concise and t..."
pdf1761.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,"#L3.2
A comprehensive explanation of Okapi BM25","The BIM was originally designed for short catalog records and abstracts of fairly consistent length, and it works reasonably in these contexts, but for modern full-text search collections, it seems..."
pdf1762.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,#L3.2 Introduction to RSJ model,No Preview Available
pdf1763.pdf,https://academic.oup.com/comjnl/article/35/3/243/525633,#L3.1 Probabilistic IR models,No Preview Available
pdf1764.pdf,https://www.youtube.com/watch?v=3OypMyJcec8,Document and Query relevance #L3.2,"MSCI 541 : Query Estimation, Relevance models, RM3, Dirichlet Mixtures"
pdf1765.pdf,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5553725/,"#L3.1
Intro to Layer-wise relevance propagation(LRP), an alternative ...","Layer-wise relevance propagation (LRP) [13, 36] is a recently introduced technique for estimating which elements of a classifier input are important to achieve a certain classification decision. It..."
pdf1766.pdf,https://resources.mpi-inf.mpg.de/departments/d5/teaching/ws13_14/irdm/slides/irdm-3-3.pdf,This explains about the Probabilistic retrieval models explained in #L...,Probabilistic Retrieval models
pdf1767.pdf,https://towardsdatascience.com/understanding-term-based-retrieval-methods-in-information-retrieval-2be5eb3dde9f,Overview of information retrieval methods and it's relevance in the de...,Understanding Term-Based Retrieval Methods in Information Retrieval
pdf1768.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,This explains about Robertson Sparck Jones #L3.2,Introduction to (Robertson/Sparck Jones)
pdf1769.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,pretty much understand the lecture. This is just a page showcase diffe...,No Preview Available
pdf1770.pdf,https://paperswithcode.com/paper/query-generation-with-external-knowledge-for,Query Generation with External Knowledge for Dense Retrieval #L4.1,"In this work, we propose Query Generation with External Knowledge (QGEK), a novel method for generating queries with external information related to the corresponding document."
pdf1771.pdf,https://en.wikipedia.org/wiki/Maximum_likelihood_estimation,The definition and detailed concepts of Maximum likelihood estimation....,Maximum likelihood estimation
pdf1772.pdf,https://aip.scitation.org/doi/10.1063/1.4765524,An interesting read on empirical divergence minimization #L4.2,No Preview Available
pdf1773.pdf,https://aip.scitation.org/doi/10.1063/1.4765524,An interesting read on empirical divergence minimization,No Preview Available
pdf1774.pdf,https://online.stat.psu.edu/stat510/lesson/5/5.2,Some practical smoothing applications #L4.1,No Preview Available
pdf1775.pdf,https://www.youtube.com/watch?v=XFIKE34HafY,Video that explains about Okapi BM25 #L3.2,Okapi BM25
pdf1776.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec09.pdf,#L3.1 Why use query likelihood and relevance models,No Preview Available
pdf1777.pdf,https://en.wikipedia.org/wiki/Relevance_feedback,"Wikipedia page of relevance feedback, useful for Lecture #4.2","Relevance feedback is a feature of some information retrieval systems. The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedb..."
pdf1778.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html,Language models for information retrieval L3.2,No Preview Available
pdf1779.pdf,http://ciir.cs.umass.edu/pubfiles/ir-347.pdf,Cluster-Based Retrieval Using Language Models L3.1,No Preview Available
pdf1780.pdf,https://www.youtube.com/watch?v=Xwt4aw5tZrE,#L4.2 Divergence minimization,Maximum Likelihood as Minimizing KL Divergence
pdf1781.pdf,https://www.youtube.com/watch?v=vZAXpvHhQow,#L4.1 TF-IDF,Calculate TF-IDF in NLP (Simple Example)
pdf1782.pdf,https://www.youtube.com/watch?v=dfCrWzyUq1U,#L3.2 Document generation,Document Preprocessing in Information Retrieval
pdf1783.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html,#L3.1 Language Models for Information Retrieval,Language models for information retrieval
pdf1784.pdf,https://www.youtube.com/watch?v=6P2z9PDRWTw,#L2.1 Statistical Language Models,Lecture 20 — Statistical Language Models | UIUC
pdf1785.pdf,https://www.youtube.com/watch?v=GiyMGBuu45w,#L2.1 N-gram language models,NLP: Understanding the N-gram language models
pdf1786.pdf,https://towardsdatascience.com/how-to-build-a-smart-search-engine-a86fca0d0795,#L3.2 Implementation of BM25,No Preview Available
pdf1787.pdf,https://nlp.stanford.edu/IR-book/pdf/11prob.pdf,#L3.1 Probabilistic Information Retrieval,No Preview Available
pdf1788.pdf,https://colinraffel.com/blog/gans-and-divergence-minimization.html,#4.2 GANs and Divergence Minimization,No Preview Available
pdf1789.pdf,https://ils.unc.edu/courses/2020_fall/inls509_001/lectures/07-QueryLikelihoodModel.pdf,#4.1 Query-Likelihood Model with some sample questions,No Preview Available
pdf1790.pdf,http://www.cs.cmu.edu/~czhai/paper/cikm2001-fb.pdf,The paper that introduced the two language model-based feedback techni...,No Preview Available
pdf1791.pdf,http://www.cs.cmu.edu/~czhai/paper/lmir2001-dualrole.pdf,The paper that introduced two-stage smoothing discussed in #L4.1,No Preview Available
pdf1792.pdf,https://www.cl.cam.ac.uk/teaching/1617/InfoRtrv/lecture7-relevance-feedback.pdf,#L4.2 Slides talking about Relevance feedback with illustrations,"Relevance Feedback and Query
Expansion"
pdf1793.pdf,https://lintool.github.io/robust04-analysis-papers/Lv-Zhai2015_Article_NegativeQueryGenerationBridgin.pdf,#L4.1 A good study on query likelihood retrieval function extension by...,Negative query generation
pdf1794.pdf,https://machinelearningmastery.com/divergence-between-probability-distributions/,The examples of Divergence Minimization that covered in lecture #L4.2,How to Calculate the KL Divergence for Machine Learning
pdf1795.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,More description of query likelihood language models that covered in l...,Using query likelihood language models in IR
pdf1796.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html,A great website explaining query generation probability(discussed in #...,"But as people have come to understand the LM approach better, it has become apparent that the role of smoothing in this model is not only to avoid zero probabilities. The smoothing of terms actuall..."
pdf1797.pdf,https://www.youtube.com/watch?v=d9alWZRzBWk,#L1.3 information theory,Journey into information theory
pdf1798.pdf,https://www.youtube.com/watch?v=KzfWUEJjG18,#L1.2 Basic Probability,Math Antics - Basic Probability
pdf1799.pdf,https://www.youtube.com/watch?v=BeDeHntF68M,#L1.1 Text mining,What is Text Mining?
pdf1800.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/extended-language-modeling-approaches-1.html,A great website about probabilistic distance model(discussed in #L4.2)...,"Rather than directly generating in either direction, we can make a language model from both the document and query, and then ask how different these two language models are from each other. Laffert..."
pdf1801.pdf,https://analyticsindiamag.com/7-types-of-generative-models-for-your-next-machine-learning-project/,A useful page to learn more about different generative models #L4.2,No Preview Available
pdf1802.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,#L4.1 Usage of query likelihood language models in IR,No Preview Available
pdf1803.pdf,https://www.ccs.neu.edu/home/jaa/CSG339.06F/Lectures/language.pdf,"More slides about query likelihood, document likelihood and smoothing ...",No Preview Available
pdf1804.pdf,https://www.robots.ox.ac.uk/~parg/_projects/ica/riz/Thesis/thesis041.html,A short but detailed explanation of the theta f estimation method I: G...,The Generative Mixture Model
pdf1805.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html,An extension taking about how to estimate the query generation probabi...,Estimating the query generation probability
pdf1806.pdf,https://citeseerx.ist.psu.edu/document?repid=rep1,"Notes on the KL-divergence retrieval formula and
Dirichlet prior smoo...",No Preview Available
pdf1807.pdf,https://dl.acm.org/doi/10.1145/3038912.3052710,Query Expansion Based on a Feedback Concept Model for Microblog Retrie...,Query Expansion Based on a Feedback Concept Model for Microblog Retrieval
pdf1808.pdf,https://notesonai.com/Query Likelihood Model,Query Likelihood Model #L4.1,Query Likelihood Model
pdf1809.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html,Language models for information retrieval #3.1,Language models for information retrieval
pdf1810.pdf,https://citeseerx.ist.psu.edu/doc/10.1.1.9.9735,This article explains how the EM algorithm can be interpreted as lower...,No Preview Available
pdf1811.pdf,https://arxiv.org/pdf/1908.07162.pdf,#L5.2 Another example of topic mining that uses more specific categori...,"Discriminative Topic Mining via Category-Name Guided Text
Embedding"
pdf1812.pdf,https://www.upgrad.com/blog/cluster-analysis-data-mining/,#L5.1 Similarities between clustering and data mining. Neither has def...,"Cluster Analysis in Data Mining: Applications, Methods & Requirements"
pdf1813.pdf,https://en.wikipedia.org/wiki/Mixture_model,"The general concept of mixture model in statistics, related to #L5.1",No Preview Available
pdf1814.pdf,https://d1wqtxts1xzle7.cloudfront.net/30739140/cikm09-evalFb-libre.pdf?1391874050=,A great explanation of language model for IR #L4.2,"We systematically compare five representative state-of-theart methods for estimating query language models with pseudo
feedback in ad hoc information retrieval, including two variants of the relev..."
pdf1815.pdf,https://dl.acm.org/doi/pdf/10.1145/319950.320022,A great explanantion of Language Models for IR: Query Generation #4.1,"Our solution is to propose a new language model for information
retrieval based on a range of data smoothing techniques."
pdf1816.pdf,https://dl.acm.org/doi/abs/10.1145/564376.564387,#L4.1,Two-stage language models for information retrieval
pdf1817.pdf,https://dl.acm.org/doi/abs/10.1145/564376.564387,@L4.1,Two-stage language models for information retrieval
pdf1818.pdf,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-16-S13-S8,#L5.1: This describes a technique called Rate of Perplexity Change tha...,the rate of perplexity change (RPC) as a function of numbers of topics is proposed as a suitable selector
pdf1819.pdf,https://courses.cs.washington.edu/courses/cse312/11wi/slides/12em.pdf,#L5.2: These slides describe the EM algorithm in more depth. Specifica...,EM as Egg vs Chicken
pdf1820.pdf,https://en.wikipedia.org/wiki/Generative_model,This article is about generative models in the context of statistical ...,This article is about generative models in the context of statistical classification.
pdf1821.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html,Estimating the query generation probability #L4.2,Estimating the query generation probability
pdf1822.pdf,http://www.cs.cmu.edu/~czhai/paper/sigir2002-twostage.pdf,Paper on two-stage smoothing referenced in #L4.1,No Preview Available
pdf1823.pdf,https://www.semanticscholar.org/paper/Revisiting-the-Divergence-Minimization-Feedback-Lv-Zhai/409b889d0d2991a4e9b7bf48bd649ca014752a7d,"#L4.2: A paper that can better explain the ""Divergence Minimization Fe...",Revisiting the Divergence Minimization Feedback Model
pdf1824.pdf,http://www.cs.cornell.edu/courses/cs4300/2013fa/lectures/language-models-2-4-pp.pdf,Has a nice examples of feedback. #L4.2,No Preview Available
pdf1825.pdf,https://people.cs.georgetown.edu/~nazli/classes/ir-Slides/LanguageModel-13.pdf,Has a nice summary of the query likelihood model. #L4.1,No Preview Available
pdf1826.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,An useful website which briefly explain what is topic modeling(discuss...,There are many techniques that are used to obtain topic models. This post aims to explain the Latent Dirichlet Allocation (LDA): a widely used topic modelling technique and the TextRank process: a ...
pdf1827.pdf,https://www.ccs.neu.edu/home/vip/teach/IRcourse/1_retrieval_models/slides/language_models.pdf,"Query likelihood models, smoothing, and relationship to TF-IDF. Relate...",No Preview Available
pdf1828.pdf,https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43902.pdf,MIXTURE OF MIXTURE N-GRAM LANGUAGE MODELS,No Preview Available
pdf1829.pdf,https://www.researchgate.net/publication/2848669_Two-Stage_Language_Models_for_Information_Retrieval,"#L4.1: The introduction paper of the topic ""Two-Stage Smoothing"" cover...",Two-Stage Language Models for Information Retrieval
pdf1830.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/extended-language-modeling-approaches-1.html,#L4.2 Some extended language modeling methods including KL divergence,No Preview Available
pdf1831.pdf,https://link.springer.com/article/10.1007/s11222-020-09989-9,#5.2 Deep mixtures of unigrams for uncovering topics in textual data,Deep mixtures of unigrams for uncovering topics in textual data
pdf1832.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,#5.1 Introduction to mixture model with some code,No Preview Available
pdf1833.pdf,https://ieeexplore.ieee.org/abstract/document/6478772?casa_token=xfGIDIZZNmcAAAAA:w0ty8P9xfwEUQDSkLcm57vTo_8MPoUE5peUgDdtejq0v4ecD-Lv3qX89iH5tq5VxJd-TZ-Pgi8U,#L4.2 Multi_Bernoulli,In Bayesian multi-target filtering knowledge of parameters such as clutter intensity and detection probability profile are of critical importance. Significant mismatches in clutter and detection mo...
pdf1834.pdf,https://link.springer.com/article/10.1007/s10791-015-9257-z,#L4.1 Generation Model,"The language modeling approach to information retrieval has recently attracted much attention. In the language modeling retrieval models, we can score and rank documents based on the query likeliho..."
pdf1835.pdf,https://www.nowpublishers.com/article/Details/INR-019,#4.2 A systematic review of the probabilistic relevance framework,"This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its applica..."
pdf1836.pdf,http://evaluatir.org/research/teaching/comp90042/2014s1/lect/l13.pdf,#4.2 Cross-lingual Information Retrieval,This slides explain how to exploit mixture LMs in different language to handle cross-lingual information retrieval
pdf1837.pdf,https://www.researchgate.net/publication/220883992_Automated_SQL_query_generation_for_systematic_testing_of_database_engines,An insightful paper about query generation for database test engine #L...,a novel approach for generating syntactically and semantically correct SQL queries as inputs for testing relational databases
pdf1838.pdf,https://www.researchgate.net/publication/220883992_Automated_SQL_query_generation_for_systematic_testing_of_database_engines,An insightful paper about query generation for systematic testing of d...,a novel approach for generating syntactically and semantically correct SQL queries as inputs for testing relational databases
pdf1839.pdf,http://www.cs.cmu.edu/~czhai/paper/cikm2001-fb.pdf,A paper on Model-based Feedback #L4.2,No Preview Available
pdf1840.pdf,https://dl.acm.org/doi/pdf/10.1145/383952.383970?casa_token=YUBnVqt1Ix8AAAAA:BqyKpEbjrjsSSxBWdI2z_89brtEomst1itVVB_rSL9S2Ryf1s-fWtuYzhq-5vW2rJJr_MIwKjAJo4A,a useful explanation of Query Model Estimation covered in Lecture #L4....,"Document Language Models, Query Models, and Risk Minimization for Information Retrieval"
pdf1841.pdf,https://ekamperi.github.io/mathematics/2021/07/03/expectation-maximization-part1.html,#L6.1 describes the intuition behind the iterative expectation maximiz...,"#L6.1 The first step is the expectation step, where we form a function for the expectation of the log-likelihood, using the current best estimates of the model’s parameters. Whereas, in the maximiz..."
pdf1842.pdf,http://simpledatamining.blogspot.com/2015/04/probabilistic-retrieval-model-basics.html,#L4.1: Derivation and Example of JM Smoothing for Query Likelihood Mod...,No Preview Available
pdf1843.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,#L5.2 useful resource to understand expectation-maximization algorithm...,The expectation-maximization algorithm is an approach for performing maximum likelihood estimation in the presence of latent variables
pdf1844.pdf,https://en.wikipedia.org/wiki/Mixture_model,#L5.1 An overview of what mixture models are and how the are used for ...,#L5.1 Topics in a document
pdf1845.pdf,https://www.statlect.com/fundamentals-of-statistics/EM-algorithm,introduce convergence of EM algorithm #L6.2,"Maximum likelihood problem
The maximum likelihood estimator (MLE) of the parameter  is the vector  that solves the maximization problem

This problem does not usually have an analytical solution..."
pdf1846.pdf,https://statweb.stanford.edu/~jtaylo/courses/stats306b/restricted/notebooks/EM_algorithm.pdf,a detailed introduction on EM algorithm #L6.1,The EM algorithm
pdf1847.pdf,https://arxiv.org/pdf/2202.04582.pdf,A cutting edge topic mining paper involving EM algorithm,Topic Discovery via Latent Space Clustering of Pretrained Language Model Representation
pdf1848.pdf,https://arxiv.org/pdf/2007.09536.pdf,A cutting edge paper on topic mining #L5.1,Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding
pdf1849.pdf,https://par.nsf.gov/servlets/purl/10175982,A Reinforcement Learning Framework for Relevance Feedback. Related to ...,A Reinforcement Learning Framework for Relevance Feedback
pdf1850.pdf,https://www.cl.cam.ac.uk/teaching/1617/InfoRtrv/lecture7-relevance-feedback.pdf,"Relevance Feedback and Query
Expansion","Relevance Feedback and Query
Expansion"
pdf1851.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/pseudo-relevance-feedback-1.html,Pseudo relevance feedback,Pseudo relevance feedback
pdf1852.pdf,https://en.wikipedia.org/wiki/Relevance_feedback,Relevance feedback #L4.2,Relevance feedback
pdf1853.pdf,https://en.wikipedia.org/wiki/Relevance_feedback,Relevance feedback,Relevance feedback
pdf1854.pdf,https://link.springer.com/chapter/10.1007/0-306-47019-5_3,Language Models for IR: Feedback #L4.2,Language Models for Relevance Feedback
pdf1855.pdf,https://en.wikipedia.org/wiki/Query_likelihood_model,Query likelihood model #L4.1,Query likelihood model
pdf1856.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec09.pdf,Language Models: More on Query Likelihood #L4.1,Language Models: More on Query Likelihood
pdf1857.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,Using query likelihood language models in IR #L4.1,Using query likelihood language models in IR
pdf1858.pdf,https://www.cs.cornell.edu/courses/cs4300/2013fa/lectures/language-models-2-4-pp.pdf,Slides from Cornell's IR course that go over LMs for text retrieval ta...,"Language Models for Text Retrieval (L#4.1, 4.2)"
pdf1859.pdf,https://dl.acm.org/doi/pdf/10.1145/383952.383970?casa_token=YUBnVqt1Ix8AAAAA:BqyKpEbjrjsSSxBWdI2z_89brtEomst1itVVB_rSL9S2Ryf1s-fWtuYzhq-5vW2rJJr_MIwKjAJo4A,a useful explanation of Feedback as Model Interpolation  covered in Le...,One way to accomplish this is through a pseudo-feedback mechanism.
pdf1860.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/extended-language-modeling-approaches-1.html,Brief introduction to incorporating relevance feedback in language mod...,Extended language modeling approaches
pdf1861.pdf,http://www.cs.cmu.edu/~czhai/paper/cikm2001-fb.pdf,Paper that talks about the two approaches to updating a query Language...,"Model-based Feedback in the
Language Modeling Approach to Information Retrieval"
pdf1862.pdf,https://citeseerx.ist.psu.edu/document?repid=rep1,The paper is mentioned in the slide 39 and proposed the generative mod...,No Preview Available
pdf1863.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec09.pdf,Some interesting exercises for query likelihood #4.1,No Preview Available
pdf1864.pdf,https://ils.unc.edu/courses/2020_fall/inls509_001/lectures/07-QueryLikelihoodModel.pdf,An introduction of query likelihood model with concrete examples,No Preview Available
pdf1865.pdf,https://nlp.stanford.edu/IR-book/pdf/12lmodel.pdf,"Going again from Stanford, I found a great read that talks about infor...",No Preview Available
pdf1866.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html,"Going from my last weeks submission of the Stanford page, I found a se...","Estimating the query generation probability
In this section we describe how to estimate . The probability of producing the query given the LM  of document  using maximum likelihood estimation ( ML..."
pdf1867.pdf,https://www.lri.fr/~sebag/COURS/EM_algorithm.pdf,#L6.2 a useful resource to understand EM algorithm. It given an introd...,A tutorial on EM algorithm #L6.2
pdf1868.pdf,https://nanonets.com/blog/topic-modeling-with-lsa-plsa-lda-lda2vec/,#L6.3 goes over pLSA and compares it against other techniques for topi...,"LSA, pLSA, LDA, and the newer, deep learning-based lda2vec #L6.3"
pdf1869.pdf,https://www.hongliangjie.com/notes/lm.pdf,Brief explanation of Query Likelihood Language Models: Multinomial and...,The basic idea behind Query Likelihood Language Models (QLLM) is that a query is a sample drawn from a language model.
pdf1870.pdf,http://www-personal.umich.edu/~qmei/pub/sigir07-poisson.pdf,#L4.1 Query Generation Model,No Preview Available
pdf1871.pdf,https://en.wikipedia.org/wiki/Kullback–Leibler_divergence,The introduction of Kullback–Leibler divergence by Wikipedia #L1.3,Kullback–Leibler divergence
pdf1872.pdf,https://en.wikipedia.org/wiki/Kullback–Leibler_divergence,Inroduction of Kullback–Leibler divergence by Wikipedia,Kullback–Leibler divergence
pdf1873.pdf,http://jedlik.phy.bme.hu/~gerjanos/HMM/node6.html,#L7.2 Useful resource to understanding the three problems of HMM - eva...,Three basic problems of HMMs
pdf1874.pdf,https://web.stanford.edu/~jurafsky/slp3/A.pdf,"#L7.3 Goes into the detail of viterbi algorithm for HMM, giving the in...",Decoding: Viterbi Algorithm #L7.3
pdf1875.pdf,https://dl.acm.org/doi/abs/10.1145/1165774.1165775,#L7.1 Paper on extracting relevant passages using HMMs. Goes into deta...,Extraction of coherent relevant passages using hidden Markov models
pdf1876.pdf,https://experts.illinois.edu/en/publications/revisiting-the-divergence-minimization-feedback-model,Paper that talks about the Divergence minimization feedback model refe...,"Pseudo-relevance feedback (PRF) has proven to be an effective strategy for improving retrieval accuracy. In this paper, we revisit a PRF method based on statistical language models, namely the dive..."
pdf1877.pdf,https://dl.acm.org/doi/10.1145/383952.383970,It's for #4.2,"Document language models, query models, and risk minimization for information retrieval"
pdf1878.pdf,https://www.techtarget.com/searchenterpriseai/definition/language-modeling,It's for #4.1,language modeling
pdf1879.pdf,https://tirth1272.medium.com/ultimate-guide-to-query-generation-model-gpt-3-4e463016a2d5,Introduction to some applications,Query Generation Model
pdf1880.pdf,https://www.robots.ox.ac.uk/~parg/_projects/ica/riz/Thesis/thesis041.html,Explains the Generative Mixture Model in #L4.2,The Generative Mixture Model
pdf1881.pdf,https://arxiv.org/ftp/arxiv/papers/2104/2104.12592.pdf,#L6.2: This paper talks about ways we can improve the convergence of t...,"most improvements focus on initial parameters, such as improvements in the Deterministic
Annealing EM (DAEM) algorithm proposed by Ueda and Nakano [10], the Split and Merge EM
(SMEM) algorithm pr..."
pdf1882.pdf,https://dl.acm.org/doi/10.1145/2661829.2661900,A good research paper that talks about the Divergence Minimization Fee...,Revisiting the Divergence Minimization Feedback Model
pdf1883.pdf,http://faculty.washington.edu/fxia/courses/LING572/EM_collins97.pdf,#L6.1: This resource talks about convergence of the EM algorithm (part...,"Theorem 2 guarantees convergence to a stationary value, but this stationary value could be a saddle point."
pdf1884.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html,Gives a brief overview and the formulas used for retrieval ranking for...,Estimating the query generation probability
pdf1885.pdf,https://arxiv.org/pdf/2106.13618.pdf,The paper proposes a new paradigm of neural ranking models for informa...,A modern Perspective on query likelihood with deep generative retrieval models.
pdf1886.pdf,https://www.youtube.com/watch?v=9LsB0Mbr-4M,Covers query likelihood retrieval function in #L4.1,Lecture 21 — Query Likelihood Retrieval Function | UIUC
pdf1887.pdf,https://blogs.sas.com/content/iml/2020/05/28/minimize-kullback-leibler-divergence.html,#L4.2 - Empirical Divergence Minimization on KL divergence with exampl...,No Preview Available
pdf1889.pdf,http://www.iro.umontreal.ca/~nie/IFT6255/zhai-cimk-01.pdf,"#L4.2Model-based Feedback in the
Language Modeling Approach to Inform...",No Preview Available
pdf1890.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_em.html,An useful introduction to EM algorithm #L4.1,No Preview Available
pdf1891.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,#L4.1 Using query likelihood language models in IR,No Preview Available
pdf1892.pdf,https://apps.dtic.mil/sti/pdfs/AD1144748.pdf,#L4.1 Document Representaion and query expansion mdoels,"Blog relevance
ranking differs from traditional document ranking in ad-hoc
information retrieval in several ways"
pdf1893.pdf,https://dl.acm.org/doi/abs/10.1145/1099554.1099725?casa_token=tc7h36KhORQAAAAA:QBYJn3tuOHT4gfYF9Fs95DJaaMNce7Iphihj5NAFE2gaOoJgbTm4h_9dXajUR_rPk7TF0I24yyW_Ow,#L4.2,Query expansion using term relationships in language models for information retrieval
pdf1894.pdf,https://www.tandfonline.com/doi/abs/10.1080/03610918.2020.1722834,Several two-component mixture distributions for count data,No Preview Available
pdf1895.pdf,https://d1wqtxts1xzle7.cloudfront.net/30739222/p403-zhai-libre.pdf?1392089624=,The authors propose and evaluate two different methods for updating qu...,Model-based Feedback in the Language Modeling Approach to Information Retrival
pdf1896.pdf,https://course.ccs.neu.edu/cs6200sp15/slides/m03.s06 - query likelihood retrieval.pdf,L4.2 Query Likelihood Retrieval,No Preview Available
pdf1897.pdf,http://www.xuanhui.me/pub/negfb-cikm07.pdf,L4.2 Improve Retrieval Accuracy using Negative Feedback,No Preview Available
pdf1898.pdf,https://link.springer.com/content/pdf/10.1007/978-94-017-0171-6_1.pdf,Probabilistic Relevance Models Based on Document and Query Generation,No Preview Available
pdf1899.pdf,https://stats.stackexchange.com/questions/20720/plsa-probabilistic-latent-semantic-analysis-how-to-choose-topic-number,#L6.3: A question I had during the lecture was how do we choose the nu...,"The number of topics / latent classes can be considered as a ""meta"" parameter"
pdf1900.pdf,https://arxiv.org/pdf/1908.07162.pdf,Discriminative Topic Mining via Category-Name Guided Text Embedding #L...,"We propose a new task, discriminative topic mining, which leverages a set of user-provided category names to mine discriminative topics from text corpora."
pdf1901.pdf,https://en.wikipedia.org/wiki/Relevance_feedback,IR Feedback for #L4.2,Relevance feedback
pdf1902.pdf,https://en.wikipedia.org/wiki/Relevance_feedback,IR Feedback for #L4.2,No Preview Available
pdf1903.pdf,https://link.springer.com/article/10.1007/s11042-020-10305-w,Dirichlet Smoothing for #L4.1,No Preview Available
pdf1904.pdf,https://sigir.org/wp-content/uploads/2017/06/p268.pdf,IR Smoothing for #L4.1,No Preview Available
pdf1905.pdf,https://www.cl.cam.ac.uk/teaching/1617/InfoRtrv/lecture7-relevance-feedback.pdf,Another lecture slides related to Relevance Feedback and Query Expansi...,No Preview Available
pdf1906.pdf,https://arxiv.org/pdf/2204.14146.pdf,a usefule description of feedback covered in #L4.2,"Here, we propose to learn
from natural language feedback, which conveys more information per human evaluation.
We learn from language feedback on model
outputs using a three-step learning algori..."
pdf1907.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec09.pdf,a useful description of query likelihood covered in #L4.1,"As a point of comparison, we consider the alternative
approach of inducing a language model from a query and considering the document
as the object being generated.2 This approach to scoring docu..."
pdf1908.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A gentle introduction to Expectation-Maximization Algorithm(discussed ...,The expectation-maximization algorithm is an approach for performing maximum likelihood estimation in the presence of latent variables. It does this by first estimating the values for the latent va...
pdf1909.pdf,https://ethanperez.net/feedback.pdf,An introduction of new method for training language model with Natural...,No Preview Available
pdf1910.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,Query likelihood language models #4.1,No Preview Available
pdf1911.pdf,https://en.wikipedia.org/wiki/Jensen's_inequality,"The Wikipedia entry on Jensen's inequality, which explains where the l...",No Preview Available
pdf1912.pdf,https://en.wikipedia.org/wiki/Jensen's_inequality,"The Wikipedia entry on Jensen's inequality, which explains where the l...",No Preview Available
pdf1913.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,eM algorithm defination in wiki #5.2,No Preview Available
pdf1914.pdf,https://monkeylearn.com/topic-analysis/,a example of topic mining named monkey learn #5.1,No Preview Available
pdf1915.pdf,https://www.cs.cmu.edu/~callan/Papers/trec08-lezhao.pdf,#L4.2 Extending relevance model for relevance feedback,"Relevance feedback is the retrieval task where the system is given not only an information
need, but also some relevance judgement information, usually from users’ feedback for an
initial result ..."
pdf1916.pdf,https://www.youtube.com/watch?v=2y3sy2UcTmY,#L4.2: Types of relevant feedback,No Preview Available
pdf1917.pdf,https://www.youtube.com/watch?v=A3g7B7g0zUE,#L4.2: Examples of relevance feedback (text based example from 6:14),No Preview Available
pdf1918.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec09.pdf,#L4.2,No Preview Available
pdf1919.pdf,https://course.ccs.neu.edu/cs6200sp15/slides/m03.s06 - query likelihood retrieval.pdf,#L4.1,No Preview Available
pdf1920.pdf,https://arxiv.org/pdf/2105.06587.pdf,#L4.2,Empirical Divergence Minimization
pdf1921.pdf,https://dl.acm.org/doi/pdf/10.1145/383952.383970,#4.1 Query models and risk minimization,No Preview Available
pdf1922.pdf,https://dl.acm.org/doi/pdf/10.1145/1277741.1277794,#L4.2,No Preview Available
pdf1923.pdf,https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec05.pdf,Introduction to Robertson-Sparck Jones Model in lecture #L4.1,Robertson-Sparck Jones Model
pdf1924.pdf,https://www.dcs.gla.ac.uk/Keith/Chapter.6/Ch.6.html,#L4.1,No Preview Available
pdf1925.pdf,https://dl.acm.org/doi/pdf/10.1145/1645953.1646259,#4.2 Psuedo feedback model,No Preview Available
pdf1926.pdf,http://www.iro.umontreal.ca/~nie/IFT6255/zhai-cimk-01.pdf,Paper on Feedback #L4.2,"Model-based Feedback in the
Language Modeling Approach to Information Retrieval"
pdf1927.pdf,https://d1wqtxts1xzle7.cloudfront.net/30739222/p403-zhai-libre.pdf?1392089624=,The authors propose and evaluate two different methods for updating qu...,Model-based feedback in the Language Modeling Approach to Information Retrieval
pdf1928.pdf,http://www.iro.umontreal.ca/~nie/IFT6255/zhai-cimk-01.pdf,Empirical Divergence #4.2,No Preview Available
pdf1929.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/the-query-likelihood-model-1.html,Concise explanations on query generation models #L4.1,The query likelihood model
pdf1930.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,Okapi BM25 of #L4.1,Okapi BM25
pdf1931.pdf,https://dl.acm.org/doi/pdf/10.5555/2856151.2856174,IR: Document generation 1,No Preview Available
pdf1932.pdf,https://aclanthology.org/2022.deelio-1.3.pdf,Related paper for query generation.,No Preview Available
pdf1933.pdf,https://tirth1272.medium.com/ultimate-guide-to-query-generation-model-gpt-3-4e463016a2d5,Brief introduction on the mechanism of transformer language model used...,#L4.1 Query Generation Language Model
pdf1934.pdf,http://boston.lti.cs.cmu.edu/callan/Workshops/lmir01/WorkshopProcs/Papers/lafferty.pdf,Comparison about document generation and query generation of #L4.1,Document generation vs. Query Generation
pdf1935.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,A succinct proof of convergence of EM algorithm #L6.2,No Preview Available
pdf1936.pdf,https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained,Understanding LK divergence in simple terms. #L4.2,No Preview Available
pdf1937.pdf,https://www.ccs.neu.edu/home/vip/teach/IRcourse/1_retrieval_models/other_notes/zhai-lafferty.pdf,This is the research paper referenced in #L4.1. This covers detailed s...,No Preview Available
pdf1938.pdf,https://link.springer.com/chapter/10.1007/978-3-031-02130-5_3,#L4.1,"Statistical Language Models for Information Retrieval

Simple Query Likelihood Retrieval Model"
pdf1939.pdf,https://arxiv.org/abs/2204.14146,A paper related to training language models with feedback in #L4.2,Training Language Models with Language Feedback
pdf1940.pdf,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4685438/,#L4.2 Query Expansion Model,No Preview Available
pdf1941.pdf,https://kmwllc.com/index.php/2020/03/20/understanding-tf-idf-and-bm-25/,#L4.1 Understanding TF-IDF,No Preview Available
pdf1942.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html,#L4.1 A more detailed explanation of estimation the query generation p...,No Preview Available
pdf1943.pdf,https://nlp.stanford.edu/IR-book/pdf/11prob.pdf,Detailed introduction to IR related to Lecture 4.2,No Preview Available
pdf1944.pdf,http://www.cs.cmu.edu/~czhai/paper/lmir2001-dualrole.pdf,This explains dual role of smoothing #L4.1,Dual Role of Smoothing
pdf1945.pdf,https://maroo.cs.umass.edu/getpdf.php?id=495,This explains about Formal Multiple-Bernoulli Models for Language Mode...,Formal Multiple-Bernoulli Models for Language Modeling
pdf1946.pdf,https://vitalflux.com/quick-introduction-smoothing-techniques-language-models/,Different smoothing methods mentioned in Lecture 4.1,No Preview Available
pdf1947.pdf,https://ils.unc.edu/courses/2020_fall/inls509_001/lectures/07-QueryLikelihoodModel.pdf,This explains about Query-Likelihood Retrieval Model #L4.1,Query-Likelihood Retrieval Model
pdf1948.pdf,https://aclanthology.org/2022.deelio-1.3/,IR: Query Generation,No Preview Available
pdf1949.pdf,http://boston.lti.cs.cmu.edu/callan/Workshops/lmir01/WorkshopProcs/Papers/lafferty.pdf,"Document generation and Query generation - The  Difference, Importance...",Probabilistic IR Models Based on Document and Query Generation
pdf1950.pdf,https://dl.acm.org/doi/10.1145/2661829.2661900,#L4.2 Divergence Minimization,No Preview Available
pdf1951.pdf,https://www.statlect.com/probability-distributions/multinoulli-distribution,#L4.1 This article briefly illustrates the concept of Multi-Bernoulli ...,"Multinoulli distribution
by Marco Taboga, PhD

The Multinoulli distribution (sometimes also called categorical distribution) is a multivariate discrete distribution that generalizes the Bernoull..."
pdf1952.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,A general explanation and derivation of expectation maximization algor...,No Preview Available
pdf1953.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,#L4.1 Query Likelihood LM,No Preview Available
pdf1954.pdf,https://dl.acm.org/doi/10.1145/2661829.2661900,#4.2 A research paper about Divergence Minimization Feedback Model,Revisiting the Divergence Minimization Feedback Model
pdf1955.pdf,https://en.wikipedia.org/wiki/Query_likelihood_model,#L4.1 A wiki page about Query likelihood model,Query likelihood model
pdf1956.pdf,http://evaluatir.org/research/teaching/comp90042/2014s1/lect/l13.pdf,Comprehensive overview of relevance models #L4.2,Relevance Models
pdf1957.pdf,https://djoerdhiemstra.com/wp-content/uploads/sigir04.pdf,"As discussed with the learned topic models in #L4.2, this is the origi...","Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing. As such, they need fewer (non-zero) parameters to describe the ..."
pdf1958.pdf,https://djoerdhiemstra.com/wp-content/uploads/sigir04.pdf,This paper explains more about Parsimonious Language Models for Inform...,Parsimonious Language Models for Information Retrieval
pdf1959.pdf,https://en.wikipedia.org/wiki/Mixture_model,Introduction of Mixture Model in #L4.2,Mixture model
pdf1960.pdf,https://text-machine-lab.github.io/blog/2020/generative-models/,The generative model for text mining#L5.1,"If we wish to classify whether that picture on your phone is a cat or a dog, we ask the question: which outcome is the most probable? The straightforward way to go about this is to estimate the pro..."
pdf1961.pdf,https://djoerdhiemstra.com/wp-content/uploads/sigir04.pdf,This explains about the Parsimonious Language Model discussed in topic...,Parsimonious Language Model
pdf1962.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,A useful explanation of different smoothing models covered in Lecture ...,Smoothing for Language Models
pdf1963.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/relevance-feedback-and-pseudo-relevance-feedback-1.html,Relevance feedback and pseudo relevance feedback,Relevance feedback #L4.2
pdf1964.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,Usage of query likelihood retrieval model in IR,query likelihood retrieval model #L4.1
pdf1965.pdf,https://github.com/riya-joshi-401/Query-Likelihood-Retrieval-Model,This explains about the Jelinek-Mercer Smoothing covered in lecture #L...,Query-Likelihood-Retrieval-Model
pdf1966.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,Query likelihood retrival#L4.1,No Preview Available
pdf1967.pdf,http://www.iro.umontreal.ca/~nie/IFT6255/zhai-cimk-01.pdf,"This explains about Model-based Feedback in the
Language Model #L4.2","Model-based Feedback in the
Language Modeling Approach to Information Retrieval"
pdf1968.pdf,https://colinraffel.com/blog/gans-and-divergence-minimization.html,#L4.2 This article is parallel with our discussion of the divergence m...,"In generative modeling, our goal is to produce a model qθ(x)
���
���
(
���
)
 of some “true” underlying probability distribution p(x)
���
(
���
)
. For the moment, let's consider modelin..."
pdf1969.pdf,https://en.wikipedia.org/wiki/Markov_decision_process,"Wiki page on markov decision process, which is similar to the mixture ...",No Preview Available
pdf1970.pdf,https://djoerdhiemstra.com/wp-content/uploads/sigir04.pdf,This paper has explanation about Parsimonious Language Model covered i...,Parsimonious Language Model
pdf1971.pdf,https://immersinn.github.io/mm01_tulm.html,Topic mining models,No Preview Available
pdf1972.pdf,https://maroo.cs.umass.edu/getpdf.php?id=294,"#L4.2
More results on Model-based feedback of different statistical q...","An ideal experimental set up to test the performance of relevant
query feedback would be to collect queries and relevance judgments from users for a long period of time and then evaluate the
pe..."
pdf1973.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/estimating-the-query-generation-probability-1.html,"#L4.1
Some more about query generation and smoothing","Thus, we need to smooth probabilities in our document language models: to discount non-zero probabilities and to give some probability mass to unseen words. There's a wide space of approaches to sm..."
pdf1974.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/relevance-feedback-and-pseudo-relevance-feedback-1.html,Relevance feedback references#L4.2,No Preview Available
pdf1975.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,A detailed explanation for Expectation Maximization Algorithm  #L5.2,Expectation Maximization (EM) is a classic algorithm developed in the 60s and 70s with diverse applications. It can be used as an unsupervised clustering algorithm and extends to NLP applications l...
pdf1976.pdf,https://en.wikipedia.org/wiki/Query_likelihood_model,This describes the query likelihood model #4.2,The query likelihood model is a language model[1][2] used in information retrieval. A language model is constructed for each document in the collection. It is then possible to rank each document by...
pdf1977.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,A detailed explanation for Expectation Maximization # L5.2,No Preview Available
pdf1978.pdf,https://en.wikipedia.org/wiki/Okapi_BM25,Describes the BM25 score of a document given a query. #4.1,"In information retrieval, Okapi BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is base..."
pdf1979.pdf,https://en.wikipedia.org/wiki/Okapi_BM25,"Describes how given a query Q, what the bm25 score of the document is.","In information retrieval, Okapi BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is base..."
pdf1980.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html,#L4.1 Okapi BM25: a non-binary model,Okapi BM25: a non-binary model
pdf1981.pdf,https://www.frontiersin.org/articles/10.3389/frai.2020.00042/full,A research paper on modeling topics for short text. Lecture #5.2,"This paper investigates the topic modeling subject and its common application areas, methods, and tools. Also, we examine and compare five frequently used topic modeling methods, as applied to shor..."
pdf1982.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,"Blog article on topic mining, related to Lecture #5.1","Topic modelling provides us with methods to organize, understand and summarize large collections of textual information."
pdf1983.pdf,https://www.cl.cam.ac.uk/teaching/0910/ArtIntII/notes4-10-small.pdf,#L7.2: These slides talk about learning an HMM using reinforcement lea...,The agent can perform actions in order to change the world’s state.
pdf1984.pdf,https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86,#L.3: this article describes the function of the alpha and beta functi...,"[Alpha and beta] are both recursive functions, which means that we could reuse previous answer as the input for the next answer."
pdf1985.pdf,https://vitalflux.com/hidden-markov-models-concepts-explained-with-examples/,#L7.1: This article illustrates what a real HMM could look like. In he...,A Markov model is made up of two components: the state transition and hidden random variables that are conditioned on each other
pdf1986.pdf,https://aclanthology.org/2022.deelio-1.3/,A research on Query Generation,No Preview Available
pdf1987.pdf,https://www.tandfonline.com/doi/abs/10.1198/01622145030000001007,An Alternative to Two-Component Mixture Models,No Preview Available
pdf1988.pdf,https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/984321.984322,A research paper on smoothing methods for language models,No Preview Available
pdf1989.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,query likelihood language models in IR,No Preview Available
pdf1990.pdf,https://en.wikipedia.org/wiki/Relevance_feedback,Relevance feedback Wikipedia #L4.2,Relevance feedback
pdf1991.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A Gentle Introduction to Expectation-Maximization (EM Algorithm) #L5.2...,A Gentle Introduction to Expectation-Maximization (EM Algorithm)
pdf1992.pdf,https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch20.pdf,Mixture Models #L5.1,Mixture Models
pdf1993.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,Introduction to Mixture Models #L5.1,Introduction to Mixture Models
pdf1994.pdf,https://nlp.stanford.edu/IR-book/html/htmledition/using-query-likelihood-language-models-in-ir-1.html,Using query likelihood language models in IR #L4.1,Using query likelihood language models in IR
pdf1995.pdf,https://en.wikipedia.org/wiki/Query_likelihood_model,Query likelihood model wikipidia #L4.1,Query likelihood model
pdf1996.pdf,http://boston.lti.cs.cmu.edu/callan/Workshops/lmir01/WorkshopProcs/Papers/lafferty.pdf,Probabilistic IR Models Based on Document and Query Generation L4.2,No Preview Available
pdf1997.pdf,http://ciir.cs.umass.edu/pubfiles/ir-171.pdf,A General Language Model for Information Retrieval L4.1,No Preview Available
pdf1998.pdf,https://dl.acm.org/doi/10.1145/2623330.2623622,Mining topics in documents with big data #L5.1,Mining topics in documents: standing on the shoulders of big data
pdf1999.pdf,https://en.wikipedia.org/wiki/Relevance_feedback,#L4.2 Relevance feedback,Relevance feedback
pdf2000.pdf,https://machinelearningmastery.com/expectation-maximization-em-algorithm/,A supplementary explanation of EM algorithm with examples and code. Re...,A Gentle Introduction to Expectation-Maximization (EM Algorithm)
pdf2001.pdf,https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43902.pdf,A detailed introduction about mixture language model for #L5.1,MIXTURE OF MIXTURE N-GRAM LANGUAGE MODEL
pdf2002.pdf,https://en.wikipedia.org/wiki/Memex,On the Influence of the Memex Concept,"The concept of the memex influenced the development of early hypertext systems, eventually leading to the creation of the World Wide Web, and personal knowledge base software.[2] The hypothetical i..."
pdf2003.pdf,https://web.stanford.edu/~jurafsky/slp3/8.pdf,"A book chapter about POS tagging using HMM and the Viterbi algorithm, ...",No Preview Available
pdf2004.pdf,https://publications.idiap.ch/attachments/papers/2004/monay-acm-1568937089.pdf,Another paper about PLSA-based image annotation and retrieval #L6.3,No Preview Available
pdf2005.pdf,https://disco.ethz.ch/courses/fs16/ti2/lecture/chapter11.pdf,"Apart from HMM, PageRank is another application of Markov chains. Rela...",No Preview Available
pdf2006.pdf,https://www.analyticsvidhya.com/blog/2021/06/part-17-step-by-step-guide-to-master-nlp-topic-modelling-using-plsa/,More information about the Probabilistic Latent Semantic Analysis (PLS...,Step by Step Guide to Master NLP – Topic Modelling using pLSA
pdf2007.pdf,https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1888739,The recent development of Two-Component Mixture Model that covered in ...,Two-Component Mixture Model in the Presence of Covariates
pdf2008.pdf,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=,An interesting application of PLSA to image annotation and retrieval #...,No Preview Available
pdf2009.pdf,http://www.hcbravo.org/dscert-algo/homeworks/topic_modeling.html,Probabilistic Latent Semantic Analysis with the EM Algorithm #L5.2,Probabilistic Latent Semantic Analysis with the EM Algorithm
pdf2010.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,EM algorithm #L6.1,EM algorithm #L6.1
pdf2011.pdf,https://orangedatamining.com/blog/2021/2021-01-27-word-distribution/,word distribution #L5.2,word distribution #L5.2
pdf2012.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,Topic mining #L5.1,Topic mining #L5.1
pdf2013.pdf,https://www.toptal.com/python/topic-modeling-python,Implementing topic modeling using Python #L5.1,"Topic modeling lets developers implement helpful features like detecting breaking news on social media, recommending personalized messages, detecting fake users, and characterizing information flow..."
pdf2014.pdf,https://dibyaghosh.com/blog/probability/kldivergence.html,#4.2 KL Divergence in ML and its Minimization,No Preview Available
pdf2015.pdf,http://mlwiki.org/index.php/Smoothing_for_Language_Models,#L4.1 All abouth Smoothing for LMs in one place,No Preview Available
pdf2016.pdf,https://aip.scitation.org/doi/10.1063/1.4903681,Two-component mixture model: Application to palm oil and exchange rate...,the present paper investigates the relationship between exchange rate and palm oil price in Malaysia by using Maximum Likelihood Estimation via Newton-Raphson algorithm to fit a two components mixt...
pdf2017.pdf,https://courses.grainger.illinois.edu/ECE417/fa2021/lectures/lec15.pdf,A succinct introduction to the Baum-Welch algorithm discussed in #L7.3,No Preview Available
pdf2018.pdf,https://colinraffel.com/blog/gans-and-divergence-minimization.html,Generative Adversarial Networks and Minimize Divergence,No Preview Available
pdf2019.pdf,https://ils.unc.edu/courses/2020_fall/inls509_001/lectures/07-QueryLikelihoodModel.pdf,More examples of Query-likelihood retrieval (page34 -),No Preview Available
pdf2020.pdf,https://courses.cs.washington.edu/courses/cse446/18wi/slides/em.pdf,An example of the Expectation-Maximization (EM) Algorithm for topic mo...,(An example of) The Expectation-Maximization (EM) Algorithm
pdf2021.pdf,https://blog.mlreview.com/topic-modeling-with-scikit-learn-e80d33668730,Topic Modeling with Scikit Learn. Related to lecture #L5.1,Topic Modeling with Scikit Learn
pdf2022.pdf,https://arxiv.org/pdf/1301.6705.pdf,Original Paper that proposed Probabilistic Latent Semantic Analysis #L...,Probabilistic Latent Semantic Analysis
pdf2023.pdf,https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf,#L5.1 A Survey of Topic Modeling in Text Mining,A nice and comprehensive survey of topic modeling in addition to the introduction in L5.1
pdf2024.pdf,https://www.tandfonline.com/doi/abs/10.1198/01622145030000001007?journalCode=uasa20,A great explanation of Mixture Language Models: Two-Component Mixture ...,"(1) It allows for a natural way to extend the proportional hazards regression model, leading to a wide class of extended hazard regression models. (2) In some settings the model can be interpreted ..."
pdf2025.pdf,https://ieeexplore.ieee.org/abstract/document/9508773,A great explanation of Mixture Language Models: Overview & Topic Minin...,Sentiment classification is a form of data analytics where people’s feelings and attitudes toward a topic are mined from data.
pdf2026.pdf,https://en.wikipedia.org/wiki/Baum–Welch_algorithm,Baum-Welch Algorithm #L7.3,Baum-Welch Algorithm #L7.3
pdf2027.pdf,https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/em-hmm.pdf,EM and HMM #L7.2,EM and HMM #L7.2
pdf2028.pdf,https://en.wikipedia.org/wiki/Hidden_Markov_model,Hidden Markov model #L7.1,Hidden Markov model #L7.1
pdf2029.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,EM algorithm #L6.2,EM algorithm #L6.2
pdf2030.pdf,https://www.eecis.udel.edu/~shatkay/Course/papers/UIntrotoTopicModelsBlei2011-5.pdf,This paper provides an overview of Probabilistic Topic models #L5.1,Introduction to Probabilistic Topic Models
pdf2031.pdf,https://www.youtube.com/watch?v=REypj2sy_5U,Video about EM algorithm #L5.2,EM algorithm: how it works
pdf2032.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,Expectation–maximization algorithm Wikipidia #L5.2,Expectation–maximization algorithm
pdf2033.pdf,https://monkeylearn.com/blog/introduction-to-topic-modeling/,This provides explanation about topic mining and analysis #L5.1,Topic Modeling: An Introduction
pdf2034.pdf,https://towardsdatascience.com/expectation-maximization-explained-c82f5ed438e5,Useful page to read more about the EM algorithm and applications in NL...,No Preview Available
pdf2035.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,#L5.1 Introduction to Mixture Models (General definition),No Preview Available
pdf2036.pdf,https://en.wikipedia.org/wiki/Expectation–maximization_algorithm,#L5.2 Expectation-maximization algorithm wiki page,No Preview Available
pdf2037.pdf,https://arxiv.org/pdf/2007.09536.pdf,#5.1 A Hierarchical topic mining approach via joint spherical tree and...,"We develop a novel joint tree and text embedding method along
with a principled optimization procedure that allows simultaneous
modeling of the category tree structure and the corpus generative
..."
pdf2038.pdf,https://openreview.net/pdf?id=tGZu6DlbreV,#5.2 RNNLogic: A novel knowledge graph reasoning approach applying EM ...,"We develop an EM-based algorithm for optimization. In each
iteration, the reasoning predictor is first updated to explore some generated logic
rules for reasoning. Then in the E-step, we select a..."
pdf2039.pdf,http://www.stat.columbia.edu/~bodhi/Talks/2ComponentMixModel.pdf,Two-component Mixture Model. Related to Lecture #L5.2,Two-component Mixture Model
pdf2040.pdf,https://www.youtube.com/watch?v=uwd5t6mJahQ,#L5.1: Maximum marginal relevance - explanation and applications,No Preview Available
pdf2041.pdf,https://medium.com/analytics-vidhya/probabilistic-topic-models-394edb21d499,##L5.2 A mixture of Unigram Language Models,Mixture of Unigram Language Models.
pdf2042.pdf,https://www.nature.com/articles/nbt1406,Explanation of the expectation maximization algorithm and it's applica...,"The expectation maximization algorithm arises in many computational biology applications that involve probabilistic models. What is it good for, and how does it work?"
pdf2043.pdf,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4122269/,Article which reviews probabilistic topic models and application of to...,"In this article, we review probabilistic topic models: graphical models that can be used to summarize a large collection of documents with a smaller number of distributions over words."
pdf2044.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,L5.1 A brief introduction about topics in text mining,No Preview Available
pdf2045.pdf,https://stats.stackexchange.com/questions/83387/why-is-the-expectation-maximization-algorithm-guaranteed-to-converge-to-a-local,This explains why EM algorithm converges to local optimum as hill clim...,Why is the Expectation Maximization algorithm guaranteed to converge to a local optimum?
pdf2046.pdf,https://immersinn.github.io/mm01_tulm.html,This provides a detailed explanation and implementation of mixture of ...,Mixture of Two Unigram Language Models
pdf2047.pdf,http://www.columbia.edu/~mh2078/MachineLearningORFE/EM_Algorithm.pdf,This provides an explanation about EM algorithm #L5.2,The EM Algorithm
pdf2048.pdf,https://orangedatamining.com/widget-catalog/text-mining/topicmodelling-widget/,Description and Example of Topic Modelling #L5.1,Topic Modelling
pdf2049.pdf,https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html,"Text Mining, Topic Modeling #L5.1",What is Topic Modeling? Why do we need it?
pdf2050.pdf,https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html,#L5.1 Introduction to Mixture models,"We often make simplifying modeling assumptions when analyzing a data set such as assuming each observation comes from one specific distribution (say, a Gaussian distribution)."
pdf2051.pdf,https://arxiv.org/pdf/1702.03307.pdf,#L5.2 Generative mixture of Networks,A generative model based on training deep architectures is proposed.
pdf2052.pdf,https://medium.com/analytics-vidhya/probabilistic-topic-models-394edb21d499,#L5.1Probabilistic topic models using mixture models,Topic modeling algorithms are statistical methods that analyze the words of the original texts to discover the themes.
pdf2053.pdf,https://arxiv.org/pdf/math/0607812.pdf,SEMIPARAMETRIC ESTIMATION OF A TWO COMPONENT MIXTURE MODEL #L5.2,No Preview Available
pdf2054.pdf,https://monkeylearn.com/topic-analysis/,An useful introduction of topic analysis with useful examples #L5.1,No Preview Available
pdf2055.pdf,https://monkeylearn.com/topic-analysis/,#L5.1 Topic Analysis,"What Is Topic Analysis?
Topic analysis (also called topic detection, topic modeling, or topic extraction) is a machine learning technique that organizes and understands large collections of text d..."
pdf2056.pdf,http://www.stat.columbia.edu/~bodhi/Talks/2ComponentMixModel.pdf,Lecture slides from Columbia University about two component mix model ...,No Preview Available
pdf2057.pdf,https://github.com/sohaib730/Gaussian-Mixture-Model,#L5.2 an python implementation of Gaussian-Mixture-Model,Gaussian-Mixture-Model
pdf2058.pdf,https://www.cs.toronto.edu/~rgrosse/csc321/mixture_models.pdf,"L5.1, 5.2: U of T resource that goes over Mixture Models, EM algorithm...",Mixture Models
pdf2059.pdf,https://www.eecis.udel.edu/~shatkay/Course/papers/UIntrotoTopicModelsBlei2011-5.pdf,"#L5.1 Introduction to Probabilistic Topic Models
Basically includes e...",No Preview Available
